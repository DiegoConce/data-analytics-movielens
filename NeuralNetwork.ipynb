{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-06-22T14:38:08.703049829Z",
     "start_time": "2023-06-22T14:38:08.651435413Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import copy\n",
    "import os\n",
    "from torch import nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import product\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "if not os.path.exists('results'):\n",
    "    os.mkdir('results')\n",
    "\n",
    "if not os.path.exists('results/pca'):\n",
    "    os.mkdir('results/pca')\n",
    "\n",
    "if not os.path.exists('results/no_pca'):\n",
    "    os.mkdir('results/no_pca')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "is_pca = False\n",
    "\n",
    "if is_pca:\n",
    "    path = 'runs/nn/nn_pca/'\n",
    "    results_path = 'results/pca/'\n",
    "else:\n",
    "    path = 'runs/nn/nn_no_pca/'\n",
    "    results_path = 'results/no_pca/'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-22T14:38:08.799981144Z",
     "start_time": "2023-06-22T14:38:08.671130727Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-22T14:38:11.730857186Z",
     "start_time": "2023-06-22T14:38:08.681890335Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "   rating_mean      007  007 (series)  18th century    1920s    1930s  \\\n0     3.893708  0.02875       0.02375       0.06250  0.07575  0.14075   \n1     3.251527  0.04125       0.04050       0.06275  0.08275  0.09100   \n2     3.142028  0.04675       0.05550       0.02925  0.08700  0.04750   \n3     2.853547  0.03425       0.03800       0.04050  0.03100  0.06500   \n4     3.058434  0.04300       0.05325       0.03800  0.04100  0.05400   \n\n     1950s    1960s    1970s   1980s  ...  Film-Noir  Horror  IMAX  Musical  \\\n0  0.14675  0.06350  0.20375  0.2020  ...          0       0     0        0   \n1  0.06125  0.06925  0.09600  0.0765  ...          0       0     0        0   \n2  0.04775  0.04600  0.14275  0.0285  ...          0       0     0        0   \n3  0.03575  0.02900  0.08650  0.0320  ...          0       0     0        0   \n4  0.06725  0.02775  0.07650  0.0215  ...          0       0     0        0   \n\n   Mystery  Romance  Sci-Fi  Thriller  War  Western  \n0        0        0       0         0    0        0  \n1        0        0       0         0    0        0  \n2        0        1       0         0    0        0  \n3        0        1       0         0    0        0  \n4        0        0       0         0    0        0  \n\n[5 rows x 1149 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>rating_mean</th>\n      <th>007</th>\n      <th>007 (series)</th>\n      <th>18th century</th>\n      <th>1920s</th>\n      <th>1930s</th>\n      <th>1950s</th>\n      <th>1960s</th>\n      <th>1970s</th>\n      <th>1980s</th>\n      <th>...</th>\n      <th>Film-Noir</th>\n      <th>Horror</th>\n      <th>IMAX</th>\n      <th>Musical</th>\n      <th>Mystery</th>\n      <th>Romance</th>\n      <th>Sci-Fi</th>\n      <th>Thriller</th>\n      <th>War</th>\n      <th>Western</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>3.893708</td>\n      <td>0.02875</td>\n      <td>0.02375</td>\n      <td>0.06250</td>\n      <td>0.07575</td>\n      <td>0.14075</td>\n      <td>0.14675</td>\n      <td>0.06350</td>\n      <td>0.20375</td>\n      <td>0.2020</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>3.251527</td>\n      <td>0.04125</td>\n      <td>0.04050</td>\n      <td>0.06275</td>\n      <td>0.08275</td>\n      <td>0.09100</td>\n      <td>0.06125</td>\n      <td>0.06925</td>\n      <td>0.09600</td>\n      <td>0.0765</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3.142028</td>\n      <td>0.04675</td>\n      <td>0.05550</td>\n      <td>0.02925</td>\n      <td>0.08700</td>\n      <td>0.04750</td>\n      <td>0.04775</td>\n      <td>0.04600</td>\n      <td>0.14275</td>\n      <td>0.0285</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2.853547</td>\n      <td>0.03425</td>\n      <td>0.03800</td>\n      <td>0.04050</td>\n      <td>0.03100</td>\n      <td>0.06500</td>\n      <td>0.03575</td>\n      <td>0.02900</td>\n      <td>0.08650</td>\n      <td>0.0320</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>3.058434</td>\n      <td>0.04300</td>\n      <td>0.05325</td>\n      <td>0.03800</td>\n      <td>0.04100</td>\n      <td>0.05400</td>\n      <td>0.06725</td>\n      <td>0.02775</td>\n      <td>0.07650</td>\n      <td>0.0215</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 1149 columns</p>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 13798\n"
     ]
    }
   ],
   "source": [
    "\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "\n",
    "df = pd.read_csv('dataset.csv')\n",
    "display(df.head())\n",
    "print(f'Number of samples: {df.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-22T14:38:11.731327930Z",
     "start_time": "2023-06-22T14:38:11.704113293Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "# PyTorch Device\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "print(\"Using {} device\".format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-22T14:38:12.051697972Z",
     "start_time": "2023-06-22T14:38:11.710793602Z"
    }
   },
   "outputs": [],
   "source": [
    "X = df.drop('rating_mean', axis=1)\n",
    "y = df['rating_mean']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=seed)\n",
    "\n",
    "# Convert to numpy arrays\n",
    "X_train = np.array(X_train, dtype=np.float32)\n",
    "X_val = np.array(X_val, dtype=np.float32)\n",
    "X_test = np.array(X_test, dtype=np.float32)\n",
    "\n",
    "y_train = np.array(y_train, dtype=np.float32)\n",
    "y_val = np.array(y_val, dtype=np.float32)\n",
    "y_test = np.array(y_test, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA is not applied\n"
     ]
    }
   ],
   "source": [
    "if is_pca:\n",
    "    print(\"Applying PCA...\")\n",
    "    pca = PCA(n_components=0.95)\n",
    "    pca.fit(X_train)\n",
    "    X_train = pca.transform(X_train)\n",
    "    X_val = pca.transform(X_val)\n",
    "    X_test = pca.transform(X_test)\n",
    "    print(f'Number of features after PCA: {X_train.shape[1]}')\n",
    "else:\n",
    "    print(\"PCA is not applied\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-22T14:38:12.078132375Z",
     "start_time": "2023-06-22T14:38:11.922243646Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-22T14:38:12.078805909Z",
     "start_time": "2023-06-22T14:38:11.969586944Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 9934\n",
      "Number of validation samples: 1104\n",
      "Number of test samples: 2760\n"
     ]
    }
   ],
   "source": [
    "print('Number of training samples:', X_train.shape[0])\n",
    "print('Number of validation samples:', X_val.shape[0])\n",
    "print('Number of test samples:', X_test.shape[0])\n",
    "\n",
    "b_size = 32\n",
    "\n",
    "train_set = torch.utils.data.DataLoader(\n",
    "    torch.utils.data.TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train)), batch_size=b_size,\n",
    "    shuffle=True)\n",
    "\n",
    "val_set = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(torch.from_numpy(X_val), torch.from_numpy(y_val)),\n",
    "                                      batch_size=b_size, shuffle=True)\n",
    "\n",
    "test_set = torch.utils.data.DataLoader(\n",
    "    torch.utils.data.TensorDataset(torch.from_numpy(X_test), torch.from_numpy(y_test)), batch_size=b_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the neural network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-22T14:38:12.079161097Z",
     "start_time": "2023-06-22T14:38:11.969900926Z"
    }
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, dropout_p, depth):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "\n",
    "        model = [\n",
    "            nn.Linear(input_size, hidden_sizes),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_p)\n",
    "        ]\n",
    "\n",
    "        for i in range(depth):\n",
    "            model.append(nn.Linear(hidden_sizes, hidden_sizes))\n",
    "            model.append(nn.ReLU())\n",
    "            model.append(nn.Dropout(dropout_p))\n",
    "\n",
    "        model.append(nn.Linear(hidden_sizes, 1))  # output layer\n",
    "\n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x.squeeze()  # Used to remove single-dimensional entries from the shape of an arra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-22T14:38:12.079904834Z",
     "start_time": "2023-06-22T14:38:11.970126186Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of combinations: 72\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "input_size = X_train.shape[1]\n",
    "\n",
    "hidden_size = [128, 256, 512]  # 128\n",
    "batch_size = [32, 64]  # 32\n",
    "dropout_p = [0.2, 0.3]  # 0.2\n",
    "depth = [3, 4, 5]\n",
    "learning_rate = [0.001, 0.1]\n",
    "num_epochs = 200\n",
    "\n",
    "hyperparams = product(hidden_size, batch_size, dropout_p, depth, learning_rate)\n",
    "print('Number of combinations:', len(list(hyperparams)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-22T14:38:12.080313714Z",
     "start_time": "2023-06-22T14:38:11.970419060Z"
    }
   },
   "outputs": [],
   "source": [
    "# Training the model\n",
    "def train_model(model, num_epochs, train_set, val_set, learning_rate, writer):\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "\n",
    "    # Early stopping\n",
    "    best_model = None\n",
    "    best_loss = np.inf\n",
    "    patience = 10\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_train_loss = 0.0\n",
    "\n",
    "        for inputs, targets in train_set:\n",
    "            # Reset gradients\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs.squeeze(), targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_train_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        train_loss.append(epoch_train_loss / len(train_set.dataset))\n",
    "\n",
    "        # Validation loop\n",
    "        model.eval()\n",
    "        epoch_val_loss = 0.0\n",
    "        for inputs, targets in val_set:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs.squeeze(), targets)\n",
    "            epoch_val_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        val_loss.append(epoch_val_loss / len(val_set.dataset))\n",
    "\n",
    "        writer.add_scalar('Loss/train', train_loss[-1], epoch)\n",
    "        writer.add_scalar('Loss/val', val_loss[-1], epoch)\n",
    "\n",
    "        print(f'Epoch: {epoch + 1}/{num_epochs} | Train loss: {train_loss[-1]:.4f} | Val loss: {val_loss[-1]:.4f}')\n",
    "\n",
    "        # Early stopping\n",
    "        if val_loss[-1] < best_loss:\n",
    "            best_loss = val_loss[-1]\n",
    "            epochs_no_improve = 0\n",
    "            best_model = copy.deepcopy(model)\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve == patience:\n",
    "                print('Early stopping!')\n",
    "                break\n",
    "\n",
    "    print(f\"Trained in {epoch + 1} epochs with best val loss: {best_loss:.4f}\")\n",
    "\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-22T14:38:12.081095461Z",
     "start_time": "2023-06-22T14:38:11.970823873Z"
    }
   },
   "outputs": [],
   "source": [
    "# Testing model function\n",
    "def test_model(model, data_loader):\n",
    "    criterion = nn.MSELoss()\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    # No need to track the gradients\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in data_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            test_loss += loss.item() * inputs.size(0)\n",
    "            y_true.extend(targets.tolist())\n",
    "            y_pred.extend(outputs.tolist())\n",
    "\n",
    "        test_loss /= len(data_loader.dataset)\n",
    "\n",
    "    return test_loss, y_true, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---Combination 1---\n",
      "-Hidden size: 128\n",
      "-Batch size: 32\n",
      "-Dropout: 0.2\n",
      "-Depth: 3\n",
      "-Learning rate: 0.001\n",
      "\n",
      "Epoch: 1/200 | Train loss: 0.6690 | Val loss: 0.0446\n",
      "Epoch: 2/200 | Train loss: 0.1280 | Val loss: 0.0179\n",
      "Epoch: 3/200 | Train loss: 0.0961 | Val loss: 0.0146\n",
      "Epoch: 4/200 | Train loss: 0.0820 | Val loss: 0.0139\n",
      "Epoch: 5/200 | Train loss: 0.0761 | Val loss: 0.0158\n",
      "Epoch: 6/200 | Train loss: 0.0740 | Val loss: 0.0156\n",
      "Epoch: 7/200 | Train loss: 0.0663 | Val loss: 0.0139\n",
      "Epoch: 8/200 | Train loss: 0.0626 | Val loss: 0.0193\n",
      "Epoch: 9/200 | Train loss: 0.0602 | Val loss: 0.0099\n",
      "Epoch: 10/200 | Train loss: 0.0554 | Val loss: 0.0152\n",
      "Epoch: 11/200 | Train loss: 0.0570 | Val loss: 0.0104\n",
      "Epoch: 12/200 | Train loss: 0.0519 | Val loss: 0.0279\n",
      "Epoch: 13/200 | Train loss: 0.0494 | Val loss: 0.0231\n",
      "Epoch: 14/200 | Train loss: 0.0470 | Val loss: 0.0119\n",
      "Epoch: 15/200 | Train loss: 0.0475 | Val loss: 0.0137\n",
      "Epoch: 16/200 | Train loss: 0.0442 | Val loss: 0.0098\n",
      "Epoch: 17/200 | Train loss: 0.0441 | Val loss: 0.0226\n",
      "Epoch: 18/200 | Train loss: 0.0440 | Val loss: 0.0093\n",
      "Epoch: 19/200 | Train loss: 0.0410 | Val loss: 0.0095\n",
      "Epoch: 20/200 | Train loss: 0.0379 | Val loss: 0.0103\n",
      "Epoch: 21/200 | Train loss: 0.0394 | Val loss: 0.0127\n",
      "Epoch: 22/200 | Train loss: 0.0366 | Val loss: 0.0178\n",
      "Epoch: 23/200 | Train loss: 0.0351 | Val loss: 0.0072\n",
      "Epoch: 24/200 | Train loss: 0.0341 | Val loss: 0.0133\n",
      "Epoch: 25/200 | Train loss: 0.0326 | Val loss: 0.0131\n",
      "Epoch: 26/200 | Train loss: 0.0319 | Val loss: 0.0122\n",
      "Epoch: 27/200 | Train loss: 0.0313 | Val loss: 0.0085\n",
      "Epoch: 28/200 | Train loss: 0.0308 | Val loss: 0.0105\n",
      "Epoch: 29/200 | Train loss: 0.0294 | Val loss: 0.0117\n",
      "Epoch: 30/200 | Train loss: 0.0280 | Val loss: 0.0129\n",
      "Epoch: 31/200 | Train loss: 0.0258 | Val loss: 0.0151\n",
      "Epoch: 32/200 | Train loss: 0.0273 | Val loss: 0.0086\n",
      "Epoch: 33/200 | Train loss: 0.0255 | Val loss: 0.0120\n",
      "Early stopping!\n",
      "Trained in 33 epochs with best val loss: 0.0072\n",
      "Test loss: 0.01248063280692567\n",
      "\n",
      "\n",
      "---Combination 2---\n",
      "-Hidden size: 128\n",
      "-Batch size: 32\n",
      "-Dropout: 0.2\n",
      "-Depth: 3\n",
      "-Learning rate: 0.1\n",
      "\n",
      "Epoch: 1/200 | Train loss: 109370.3677 | Val loss: 0.3154\n",
      "Epoch: 2/200 | Train loss: 1.0478 | Val loss: 0.2378\n",
      "Epoch: 3/200 | Train loss: 0.9040 | Val loss: 0.2807\n",
      "Epoch: 4/200 | Train loss: 0.9380 | Val loss: 0.3594\n",
      "Epoch: 5/200 | Train loss: 0.7865 | Val loss: 0.3468\n",
      "Epoch: 6/200 | Train loss: 0.7660 | Val loss: 0.2268\n",
      "Epoch: 7/200 | Train loss: 0.7425 | Val loss: 0.2259\n",
      "Epoch: 8/200 | Train loss: 0.7258 | Val loss: 0.2425\n",
      "Epoch: 9/200 | Train loss: 0.7157 | Val loss: 0.2786\n",
      "Epoch: 10/200 | Train loss: 0.6896 | Val loss: 0.2445\n",
      "Epoch: 11/200 | Train loss: 0.6633 | Val loss: 0.2261\n",
      "Epoch: 12/200 | Train loss: 0.6330 | Val loss: 0.2273\n",
      "Epoch: 13/200 | Train loss: 0.5965 | Val loss: 0.2307\n",
      "Epoch: 14/200 | Train loss: 0.5609 | Val loss: 0.2324\n",
      "Epoch: 15/200 | Train loss: 0.5174 | Val loss: 0.2267\n",
      "Epoch: 16/200 | Train loss: 0.4879 | Val loss: 0.2441\n",
      "Epoch: 17/200 | Train loss: 0.4225 | Val loss: 0.2316\n",
      "Early stopping!\n",
      "Trained in 17 epochs with best val loss: 0.2259\n",
      "Test loss: 0.23007100390783256\n",
      "\n",
      "\n",
      "---Combination 3---\n",
      "-Hidden size: 128\n",
      "-Batch size: 32\n",
      "-Dropout: 0.2\n",
      "-Depth: 4\n",
      "-Learning rate: 0.001\n",
      "\n",
      "Epoch: 1/200 | Train loss: 0.7023 | Val loss: 0.0330\n",
      "Epoch: 2/200 | Train loss: 0.1314 | Val loss: 0.0409\n",
      "Epoch: 3/200 | Train loss: 0.1034 | Val loss: 0.0852\n",
      "Epoch: 4/200 | Train loss: 0.0881 | Val loss: 0.0280\n",
      "Epoch: 5/200 | Train loss: 0.0783 | Val loss: 0.0238\n",
      "Epoch: 6/200 | Train loss: 0.0732 | Val loss: 0.0287\n",
      "Epoch: 7/200 | Train loss: 0.0705 | Val loss: 0.0167\n",
      "Epoch: 8/200 | Train loss: 0.0654 | Val loss: 0.0111\n",
      "Epoch: 9/200 | Train loss: 0.0621 | Val loss: 0.0105\n",
      "Epoch: 10/200 | Train loss: 0.0595 | Val loss: 0.0115\n",
      "Epoch: 11/200 | Train loss: 0.0527 | Val loss: 0.0207\n",
      "Epoch: 12/200 | Train loss: 0.0539 | Val loss: 0.0132\n",
      "Epoch: 13/200 | Train loss: 0.0510 | Val loss: 0.0247\n",
      "Epoch: 14/200 | Train loss: 0.0493 | Val loss: 0.0102\n",
      "Epoch: 15/200 | Train loss: 0.0475 | Val loss: 0.0113\n",
      "Epoch: 16/200 | Train loss: 0.0448 | Val loss: 0.0111\n",
      "Epoch: 17/200 | Train loss: 0.0433 | Val loss: 0.0217\n",
      "Epoch: 18/200 | Train loss: 0.0412 | Val loss: 0.0159\n",
      "Epoch: 19/200 | Train loss: 0.0420 | Val loss: 0.0093\n",
      "Epoch: 20/200 | Train loss: 0.0405 | Val loss: 0.0081\n",
      "Epoch: 21/200 | Train loss: 0.0391 | Val loss: 0.0147\n",
      "Epoch: 22/200 | Train loss: 0.0369 | Val loss: 0.0103\n",
      "Epoch: 23/200 | Train loss: 0.0355 | Val loss: 0.0099\n",
      "Epoch: 24/200 | Train loss: 0.0346 | Val loss: 0.0088\n",
      "Epoch: 25/200 | Train loss: 0.0329 | Val loss: 0.0138\n",
      "Epoch: 26/200 | Train loss: 0.0318 | Val loss: 0.0192\n",
      "Epoch: 27/200 | Train loss: 0.0311 | Val loss: 0.0184\n",
      "Epoch: 28/200 | Train loss: 0.0294 | Val loss: 0.0112\n",
      "Epoch: 29/200 | Train loss: 0.0302 | Val loss: 0.0157\n",
      "Epoch: 30/200 | Train loss: 0.0279 | Val loss: 0.0213\n",
      "Early stopping!\n",
      "Trained in 30 epochs with best val loss: 0.0081\n",
      "Test loss: 0.02124661286563977\n",
      "\n",
      "\n",
      "---Combination 4---\n",
      "-Hidden size: 128\n",
      "-Batch size: 32\n",
      "-Dropout: 0.2\n",
      "-Depth: 4\n",
      "-Learning rate: 0.1\n",
      "\n",
      "Epoch: 1/200 | Train loss: 1149966.1508 | Val loss: 1.2763\n",
      "Epoch: 2/200 | Train loss: 1.8207 | Val loss: 1.2441\n",
      "Epoch: 3/200 | Train loss: 1.4687 | Val loss: 0.7505\n",
      "Epoch: 4/200 | Train loss: 1.3529 | Val loss: 0.4715\n",
      "Epoch: 5/200 | Train loss: 1.1210 | Val loss: 0.4029\n",
      "Epoch: 6/200 | Train loss: 1.0485 | Val loss: 0.3022\n",
      "Epoch: 7/200 | Train loss: 0.9565 | Val loss: 0.2425\n",
      "Epoch: 8/200 | Train loss: 0.9736 | Val loss: 0.3230\n",
      "Epoch: 9/200 | Train loss: 0.9367 | Val loss: 0.3856\n",
      "Epoch: 10/200 | Train loss: 0.8427 | Val loss: 0.4458\n",
      "Epoch: 11/200 | Train loss: 0.8564 | Val loss: 0.2351\n",
      "Epoch: 12/200 | Train loss: 0.7663 | Val loss: 0.2955\n",
      "Epoch: 13/200 | Train loss: 0.7223 | Val loss: 0.2587\n",
      "Epoch: 14/200 | Train loss: 0.6820 | Val loss: 0.2335\n",
      "Epoch: 15/200 | Train loss: 0.6804 | Val loss: 0.2258\n",
      "Epoch: 16/200 | Train loss: 0.6619 | Val loss: 0.2783\n",
      "Epoch: 17/200 | Train loss: 0.6364 | Val loss: 0.2520\n",
      "Epoch: 18/200 | Train loss: 0.6047 | Val loss: 0.3813\n",
      "Epoch: 19/200 | Train loss: 0.5955 | Val loss: 0.2306\n",
      "Epoch: 20/200 | Train loss: 0.5637 | Val loss: 0.2378\n",
      "Epoch: 21/200 | Train loss: 0.5226 | Val loss: 0.2766\n",
      "Epoch: 22/200 | Train loss: 0.5047 | Val loss: 0.2834\n",
      "Epoch: 23/200 | Train loss: 0.4495 | Val loss: 0.2260\n",
      "Epoch: 24/200 | Train loss: 0.4366 | Val loss: 0.2300\n",
      "Epoch: 25/200 | Train loss: 0.3988 | Val loss: 0.2329\n",
      "Early stopping!\n",
      "Trained in 25 epochs with best val loss: 0.2258\n",
      "Test loss: 0.23454863567283188\n",
      "\n",
      "\n",
      "---Combination 5---\n",
      "-Hidden size: 128\n",
      "-Batch size: 32\n",
      "-Dropout: 0.2\n",
      "-Depth: 5\n",
      "-Learning rate: 0.001\n",
      "\n",
      "Epoch: 1/200 | Train loss: 0.8163 | Val loss: 0.0585\n",
      "Epoch: 2/200 | Train loss: 0.1360 | Val loss: 0.0178\n",
      "Epoch: 3/200 | Train loss: 0.1047 | Val loss: 0.0440\n",
      "Epoch: 4/200 | Train loss: 0.0901 | Val loss: 0.0190\n",
      "Epoch: 5/200 | Train loss: 0.0819 | Val loss: 0.0163\n",
      "Epoch: 6/200 | Train loss: 0.0750 | Val loss: 0.0296\n",
      "Epoch: 7/200 | Train loss: 0.0698 | Val loss: 0.0192\n",
      "Epoch: 8/200 | Train loss: 0.0651 | Val loss: 0.0120\n",
      "Epoch: 9/200 | Train loss: 0.0617 | Val loss: 0.0123\n",
      "Epoch: 10/200 | Train loss: 0.0581 | Val loss: 0.0133\n",
      "Epoch: 11/200 | Train loss: 0.0553 | Val loss: 0.0077\n",
      "Epoch: 12/200 | Train loss: 0.0534 | Val loss: 0.0097\n",
      "Epoch: 13/200 | Train loss: 0.0487 | Val loss: 0.0105\n",
      "Epoch: 14/200 | Train loss: 0.0476 | Val loss: 0.0090\n",
      "Epoch: 15/200 | Train loss: 0.0472 | Val loss: 0.0162\n",
      "Epoch: 16/200 | Train loss: 0.0431 | Val loss: 0.0150\n",
      "Epoch: 17/200 | Train loss: 0.0407 | Val loss: 0.0137\n",
      "Epoch: 18/200 | Train loss: 0.0406 | Val loss: 0.0136\n",
      "Epoch: 19/200 | Train loss: 0.0390 | Val loss: 0.0176\n",
      "Epoch: 20/200 | Train loss: 0.0363 | Val loss: 0.0171\n",
      "Epoch: 21/200 | Train loss: 0.0354 | Val loss: 0.0124\n",
      "Early stopping!\n",
      "Trained in 21 epochs with best val loss: 0.0077\n",
      "Test loss: 0.012332908752495828\n",
      "\n",
      "\n",
      "---Combination 6---\n",
      "-Hidden size: 128\n",
      "-Batch size: 32\n",
      "-Dropout: 0.2\n",
      "-Depth: 5\n",
      "-Learning rate: 0.1\n",
      "\n",
      "Epoch: 1/200 | Train loss: 1861725.6813 | Val loss: 0.2258\n",
      "Epoch: 2/200 | Train loss: 0.3920 | Val loss: 0.2261\n",
      "Epoch: 3/200 | Train loss: 0.3894 | Val loss: 0.2267\n",
      "Epoch: 4/200 | Train loss: 0.3761 | Val loss: 0.2281\n",
      "Epoch: 5/200 | Train loss: 0.3645 | Val loss: 0.2294\n",
      "Epoch: 6/200 | Train loss: 0.3579 | Val loss: 0.2256\n",
      "Epoch: 7/200 | Train loss: 0.3538 | Val loss: 0.2259\n",
      "Epoch: 8/200 | Train loss: 7.5207 | Val loss: 0.2277\n",
      "Epoch: 9/200 | Train loss: 0.3558 | Val loss: 0.2354\n",
      "Epoch: 10/200 | Train loss: 0.3456 | Val loss: 0.2273\n",
      "Epoch: 11/200 | Train loss: 0.3439 | Val loss: 0.2260\n",
      "Epoch: 12/200 | Train loss: 796.0185 | Val loss: 0.2352\n",
      "Epoch: 13/200 | Train loss: 12.5062 | Val loss: 0.2384\n",
      "Epoch: 14/200 | Train loss: 0.7472 | Val loss: 0.2293\n",
      "Epoch: 15/200 | Train loss: 0.7263 | Val loss: 0.2305\n",
      "Epoch: 16/200 | Train loss: 0.7094 | Val loss: 0.2668\n",
      "Early stopping!\n",
      "Trained in 16 epochs with best val loss: 0.2256\n",
      "Test loss: 0.27059317257093346\n",
      "\n",
      "\n",
      "---Combination 7---\n",
      "-Hidden size: 128\n",
      "-Batch size: 32\n",
      "-Dropout: 0.3\n",
      "-Depth: 3\n",
      "-Learning rate: 0.001\n",
      "\n",
      "Epoch: 1/200 | Train loss: 0.8796 | Val loss: 0.0584\n",
      "Epoch: 2/200 | Train loss: 0.1912 | Val loss: 0.0311\n",
      "Epoch: 3/200 | Train loss: 0.1473 | Val loss: 0.0376\n",
      "Epoch: 4/200 | Train loss: 0.1265 | Val loss: 0.0309\n",
      "Epoch: 5/200 | Train loss: 0.1170 | Val loss: 0.0356\n",
      "Epoch: 6/200 | Train loss: 0.1054 | Val loss: 0.0276\n",
      "Epoch: 7/200 | Train loss: 0.0986 | Val loss: 0.0188\n",
      "Epoch: 8/200 | Train loss: 0.0941 | Val loss: 0.0185\n",
      "Epoch: 9/200 | Train loss: 0.0898 | Val loss: 0.0134\n",
      "Epoch: 10/200 | Train loss: 0.0812 | Val loss: 0.0242\n",
      "Epoch: 11/200 | Train loss: 0.0796 | Val loss: 0.0528\n",
      "Epoch: 12/200 | Train loss: 0.0754 | Val loss: 0.0252\n",
      "Epoch: 13/200 | Train loss: 0.0744 | Val loss: 0.0183\n",
      "Epoch: 14/200 | Train loss: 0.0720 | Val loss: 0.0368\n",
      "Epoch: 15/200 | Train loss: 0.0700 | Val loss: 0.0197\n",
      "Epoch: 16/200 | Train loss: 0.0672 | Val loss: 0.0191\n",
      "Epoch: 17/200 | Train loss: 0.0646 | Val loss: 0.0108\n",
      "Epoch: 18/200 | Train loss: 0.0640 | Val loss: 0.0145\n",
      "Epoch: 19/200 | Train loss: 0.0602 | Val loss: 0.0162\n",
      "Epoch: 20/200 | Train loss: 0.0577 | Val loss: 0.0190\n",
      "Epoch: 21/200 | Train loss: 0.0549 | Val loss: 0.0266\n",
      "Epoch: 22/200 | Train loss: 0.0517 | Val loss: 0.0289\n",
      "Epoch: 23/200 | Train loss: 0.0500 | Val loss: 0.0140\n",
      "Epoch: 24/200 | Train loss: 0.0481 | Val loss: 0.0263\n",
      "Epoch: 25/200 | Train loss: 0.0457 | Val loss: 0.0220\n",
      "Epoch: 26/200 | Train loss: 0.0435 | Val loss: 0.0133\n",
      "Epoch: 27/200 | Train loss: 0.0417 | Val loss: 0.0116\n",
      "Early stopping!\n",
      "Trained in 27 epochs with best val loss: 0.0108\n",
      "Test loss: 0.011699437344635742\n",
      "\n",
      "\n",
      "---Combination 8---\n",
      "-Hidden size: 128\n",
      "-Batch size: 32\n",
      "-Dropout: 0.3\n",
      "-Depth: 3\n",
      "-Learning rate: 0.1\n",
      "\n",
      "Epoch: 1/200 | Train loss: 120552.1060 | Val loss: 84.5694\n",
      "Epoch: 2/200 | Train loss: 1900.4863 | Val loss: 12.3581\n",
      "Epoch: 3/200 | Train loss: 11.4590 | Val loss: 1.3082\n",
      "Epoch: 4/200 | Train loss: 3.7508 | Val loss: 1.1618\n",
      "Epoch: 5/200 | Train loss: 13.8983 | Val loss: 1.0259\n",
      "Epoch: 6/200 | Train loss: 2.6860 | Val loss: 0.8711\n",
      "Epoch: 7/200 | Train loss: 2.1373 | Val loss: 0.6756\n",
      "Epoch: 8/200 | Train loss: 1.6594 | Val loss: 0.5467\n",
      "Epoch: 9/200 | Train loss: 1.2171 | Val loss: 0.4385\n",
      "Epoch: 10/200 | Train loss: 1.0964 | Val loss: 0.3689\n",
      "Epoch: 11/200 | Train loss: 0.7591 | Val loss: 0.3180\n",
      "Epoch: 12/200 | Train loss: 0.4801 | Val loss: 0.2785\n",
      "Epoch: 13/200 | Train loss: 0.3697 | Val loss: 0.2513\n",
      "Epoch: 14/200 | Train loss: 0.2995 | Val loss: 0.2355\n",
      "Epoch: 15/200 | Train loss: 0.3105 | Val loss: 0.2283\n",
      "Epoch: 16/200 | Train loss: 0.2470 | Val loss: 0.2257\n",
      "Epoch: 17/200 | Train loss: 0.2420 | Val loss: 0.2259\n",
      "Epoch: 18/200 | Train loss: 0.2381 | Val loss: 0.2256\n",
      "Epoch: 19/200 | Train loss: 0.2402 | Val loss: 0.2256\n",
      "Epoch: 20/200 | Train loss: 0.2375 | Val loss: 0.2256\n",
      "Epoch: 21/200 | Train loss: 0.2375 | Val loss: 0.2257\n",
      "Epoch: 22/200 | Train loss: 0.2394 | Val loss: 0.2256\n",
      "Epoch: 23/200 | Train loss: 0.2375 | Val loss: 0.2256\n",
      "Epoch: 24/200 | Train loss: 0.2375 | Val loss: 0.2259\n",
      "Epoch: 25/200 | Train loss: 0.2376 | Val loss: 0.2259\n",
      "Epoch: 26/200 | Train loss: 0.2375 | Val loss: 0.2256\n",
      "Epoch: 27/200 | Train loss: 0.2376 | Val loss: 0.2256\n",
      "Epoch: 28/200 | Train loss: 0.2376 | Val loss: 0.2257\n",
      "Epoch: 29/200 | Train loss: 0.2377 | Val loss: 0.2256\n",
      "Epoch: 30/200 | Train loss: 0.2376 | Val loss: 0.2259\n",
      "Epoch: 31/200 | Train loss: 0.2377 | Val loss: 0.2256\n",
      "Epoch: 32/200 | Train loss: 0.2377 | Val loss: 0.2261\n",
      "Epoch: 33/200 | Train loss: 0.2378 | Val loss: 0.2260\n",
      "Epoch: 34/200 | Train loss: 0.2382 | Val loss: 0.2261\n",
      "Epoch: 35/200 | Train loss: 0.2380 | Val loss: 0.2256\n",
      "Epoch: 36/200 | Train loss: 0.2382 | Val loss: 0.2274\n",
      "Epoch: 37/200 | Train loss: 0.2380 | Val loss: 0.2279\n",
      "Epoch: 38/200 | Train loss: 0.2410 | Val loss: 0.2267\n",
      "Epoch: 39/200 | Train loss: 0.2385 | Val loss: 0.2262\n",
      "Early stopping!\n",
      "Trained in 39 epochs with best val loss: 0.2256\n",
      "Test loss: 0.22678653157275655\n",
      "\n",
      "\n",
      "---Combination 9---\n",
      "-Hidden size: 128\n",
      "-Batch size: 32\n",
      "-Dropout: 0.3\n",
      "-Depth: 4\n",
      "-Learning rate: 0.001\n",
      "\n",
      "Epoch: 1/200 | Train loss: 0.7986 | Val loss: 0.1608\n",
      "Epoch: 2/200 | Train loss: 0.1823 | Val loss: 0.0474\n",
      "Epoch: 3/200 | Train loss: 0.1418 | Val loss: 0.0769\n",
      "Epoch: 4/200 | Train loss: 0.1270 | Val loss: 0.0336\n",
      "Epoch: 5/200 | Train loss: 0.1162 | Val loss: 0.0347\n",
      "Epoch: 6/200 | Train loss: 0.1074 | Val loss: 0.0524\n",
      "Epoch: 7/200 | Train loss: 0.0964 | Val loss: 0.0214\n",
      "Epoch: 8/200 | Train loss: 0.0929 | Val loss: 0.0207\n",
      "Epoch: 9/200 | Train loss: 0.0860 | Val loss: 0.0320\n",
      "Epoch: 10/200 | Train loss: 0.0813 | Val loss: 0.0253\n",
      "Epoch: 11/200 | Train loss: 0.0770 | Val loss: 0.0442\n",
      "Epoch: 12/200 | Train loss: 0.0754 | Val loss: 0.0154\n",
      "Epoch: 13/200 | Train loss: 0.0715 | Val loss: 0.0190\n",
      "Epoch: 14/200 | Train loss: 0.0690 | Val loss: 0.0229\n",
      "Epoch: 15/200 | Train loss: 0.0647 | Val loss: 0.0202\n",
      "Epoch: 16/200 | Train loss: 0.0637 | Val loss: 0.0282\n",
      "Epoch: 17/200 | Train loss: 0.0595 | Val loss: 0.0197\n",
      "Epoch: 18/200 | Train loss: 0.0577 | Val loss: 0.0184\n",
      "Epoch: 19/200 | Train loss: 0.0570 | Val loss: 0.0168\n",
      "Epoch: 20/200 | Train loss: 0.0530 | Val loss: 0.0297\n",
      "Epoch: 21/200 | Train loss: 0.0520 | Val loss: 0.0226\n",
      "Epoch: 22/200 | Train loss: 0.0488 | Val loss: 0.0283\n",
      "Early stopping!\n",
      "Trained in 22 epochs with best val loss: 0.0154\n",
      "Test loss: 0.028192609888703927\n",
      "\n",
      "\n",
      "---Combination 10---\n",
      "-Hidden size: 128\n",
      "-Batch size: 32\n",
      "-Dropout: 0.3\n",
      "-Depth: 4\n",
      "-Learning rate: 0.1\n",
      "\n",
      "Epoch: 1/200 | Train loss: 134900.8522 | Val loss: 12.8574\n",
      "Epoch: 2/200 | Train loss: 11.8050 | Val loss: 10.0968\n",
      "Epoch: 3/200 | Train loss: 8.8175 | Val loss: 7.2052\n",
      "Epoch: 4/200 | Train loss: 5.9697 | Val loss: 4.6747\n",
      "Epoch: 5/200 | Train loss: 3.7083 | Val loss: 2.7626\n",
      "Epoch: 6/200 | Train loss: 2.1552 | Val loss: 1.5024\n",
      "Epoch: 7/200 | Train loss: 1.1481 | Val loss: 0.7821\n",
      "Epoch: 8/200 | Train loss: 0.6133 | Val loss: 0.4309\n",
      "Epoch: 9/200 | Train loss: 0.4032 | Val loss: 0.2872\n",
      "Epoch: 10/200 | Train loss: 0.2739 | Val loss: 0.2402\n",
      "Epoch: 11/200 | Train loss: 0.2575 | Val loss: 0.2279\n",
      "Epoch: 12/200 | Train loss: 0.2396 | Val loss: 0.2258\n",
      "Epoch: 13/200 | Train loss: 0.2378 | Val loss: 0.2256\n",
      "Epoch: 14/200 | Train loss: 0.2376 | Val loss: 0.2257\n",
      "Epoch: 15/200 | Train loss: 2.8976 | Val loss: 0.2261\n",
      "Epoch: 16/200 | Train loss: 7.1324 | Val loss: 0.2269\n",
      "Epoch: 17/200 | Train loss: 1.4394 | Val loss: 0.2258\n",
      "Epoch: 18/200 | Train loss: 0.2375 | Val loss: 0.2257\n",
      "Epoch: 19/200 | Train loss: 0.2375 | Val loss: 0.2257\n",
      "Epoch: 20/200 | Train loss: 0.2394 | Val loss: 0.2256\n",
      "Epoch: 21/200 | Train loss: 0.2375 | Val loss: 0.2256\n",
      "Epoch: 22/200 | Train loss: 0.2375 | Val loss: 0.2256\n",
      "Epoch: 23/200 | Train loss: 0.2380 | Val loss: 0.2256\n",
      "Early stopping!\n",
      "Trained in 23 epochs with best val loss: 0.2256\n",
      "Test loss: 0.22557911302732384\n",
      "\n",
      "\n",
      "---Combination 11---\n",
      "-Hidden size: 128\n",
      "-Batch size: 32\n",
      "-Dropout: 0.3\n",
      "-Depth: 5\n",
      "-Learning rate: 0.001\n",
      "\n",
      "Epoch: 1/200 | Train loss: 0.8313 | Val loss: 0.1981\n",
      "Epoch: 2/200 | Train loss: 0.1877 | Val loss: 0.0587\n",
      "Epoch: 3/200 | Train loss: 0.1488 | Val loss: 0.0346\n",
      "Epoch: 4/200 | Train loss: 0.1377 | Val loss: 0.0454\n",
      "Epoch: 5/200 | Train loss: 0.1227 | Val loss: 0.0240\n",
      "Epoch: 6/200 | Train loss: 0.1113 | Val loss: 0.0254\n",
      "Epoch: 7/200 | Train loss: 0.1007 | Val loss: 0.0279\n",
      "Epoch: 8/200 | Train loss: 0.0989 | Val loss: 0.0328\n",
      "Epoch: 9/200 | Train loss: 0.0911 | Val loss: 0.0162\n",
      "Epoch: 10/200 | Train loss: 0.0863 | Val loss: 0.0158\n",
      "Epoch: 11/200 | Train loss: 0.0783 | Val loss: 0.0420\n",
      "Epoch: 12/200 | Train loss: 0.0765 | Val loss: 0.0333\n",
      "Epoch: 13/200 | Train loss: 0.0755 | Val loss: 0.0263\n",
      "Epoch: 14/200 | Train loss: 0.0726 | Val loss: 0.0250\n",
      "Epoch: 15/200 | Train loss: 0.0679 | Val loss: 0.0261\n",
      "Epoch: 16/200 | Train loss: 0.0651 | Val loss: 0.0454\n",
      "Epoch: 17/200 | Train loss: 0.0640 | Val loss: 0.0188\n",
      "Epoch: 18/200 | Train loss: 0.0602 | Val loss: 0.0223\n",
      "Epoch: 19/200 | Train loss: 0.0596 | Val loss: 0.0220\n",
      "Epoch: 20/200 | Train loss: 0.0561 | Val loss: 0.0201\n",
      "Early stopping!\n",
      "Trained in 20 epochs with best val loss: 0.0158\n",
      "Test loss: 0.01972293979015903\n",
      "\n",
      "\n",
      "---Combination 12---\n",
      "-Hidden size: 128\n",
      "-Batch size: 32\n",
      "-Dropout: 0.3\n",
      "-Depth: 5\n",
      "-Learning rate: 0.1\n",
      "\n",
      "Epoch: 1/200 | Train loss: 3726031.4887 | Val loss: 2.2159\n",
      "Epoch: 2/200 | Train loss: 7946.7911 | Val loss: 0.6971\n",
      "Epoch: 3/200 | Train loss: 187.6341 | Val loss: 1.4519\n",
      "Epoch: 4/200 | Train loss: 4.5234 | Val loss: 1.4729\n",
      "Epoch: 5/200 | Train loss: 4.7856 | Val loss: 1.3390\n",
      "Epoch: 6/200 | Train loss: 4.0415 | Val loss: 1.3295\n",
      "Epoch: 7/200 | Train loss: 3.8507 | Val loss: 1.3025\n",
      "Epoch: 8/200 | Train loss: 3.6977 | Val loss: 1.2284\n",
      "Epoch: 9/200 | Train loss: 3.5213 | Val loss: 1.2097\n",
      "Epoch: 10/200 | Train loss: 3.2028 | Val loss: 1.0820\n",
      "Epoch: 11/200 | Train loss: 2.9850 | Val loss: 1.0059\n",
      "Epoch: 12/200 | Train loss: 18.1372 | Val loss: 0.8923\n",
      "Early stopping!\n",
      "Trained in 12 epochs with best val loss: 0.6971\n",
      "Test loss: 0.9074321532594984\n",
      "\n",
      "\n",
      "---Combination 13---\n",
      "-Hidden size: 128\n",
      "-Batch size: 64\n",
      "-Dropout: 0.2\n",
      "-Depth: 3\n",
      "-Learning rate: 0.001\n",
      "\n",
      "Epoch: 1/200 | Train loss: 0.9366 | Val loss: 0.0964\n",
      "Epoch: 2/200 | Train loss: 0.1798 | Val loss: 0.0577\n",
      "Epoch: 3/200 | Train loss: 0.1214 | Val loss: 0.0211\n",
      "Epoch: 4/200 | Train loss: 0.0988 | Val loss: 0.0284\n",
      "Epoch: 5/200 | Train loss: 0.0874 | Val loss: 0.0328\n",
      "Epoch: 6/200 | Train loss: 0.0800 | Val loss: 0.0148\n",
      "Epoch: 7/200 | Train loss: 0.0744 | Val loss: 0.0128\n",
      "Epoch: 8/200 | Train loss: 0.0689 | Val loss: 0.0101\n",
      "Epoch: 9/200 | Train loss: 0.0657 | Val loss: 0.0126\n",
      "Epoch: 10/200 | Train loss: 0.0636 | Val loss: 0.0119\n",
      "Epoch: 11/200 | Train loss: 0.0642 | Val loss: 0.0107\n",
      "Epoch: 12/200 | Train loss: 0.0598 | Val loss: 0.0098\n",
      "Epoch: 13/200 | Train loss: 0.0579 | Val loss: 0.0113\n",
      "Epoch: 14/200 | Train loss: 0.0545 | Val loss: 0.0078\n",
      "Epoch: 15/200 | Train loss: 0.0545 | Val loss: 0.0139\n",
      "Epoch: 16/200 | Train loss: 0.0532 | Val loss: 0.0122\n",
      "Epoch: 17/200 | Train loss: 0.0516 | Val loss: 0.0108\n",
      "Epoch: 18/200 | Train loss: 0.0503 | Val loss: 0.0082\n",
      "Epoch: 19/200 | Train loss: 0.0481 | Val loss: 0.0088\n",
      "Epoch: 20/200 | Train loss: 0.0471 | Val loss: 0.0106\n",
      "Epoch: 21/200 | Train loss: 0.0488 | Val loss: 0.0082\n",
      "Epoch: 22/200 | Train loss: 0.0449 | Val loss: 0.0138\n",
      "Epoch: 23/200 | Train loss: 0.0430 | Val loss: 0.0101\n",
      "Epoch: 24/200 | Train loss: 0.0431 | Val loss: 0.0082\n",
      "Early stopping!\n",
      "Trained in 24 epochs with best val loss: 0.0078\n",
      "Test loss: 0.008303511639436085\n",
      "\n",
      "\n",
      "---Combination 14---\n",
      "-Hidden size: 128\n",
      "-Batch size: 64\n",
      "-Dropout: 0.2\n",
      "-Depth: 3\n",
      "-Learning rate: 0.1\n",
      "\n",
      "Epoch: 1/200 | Train loss: 741132.8599 | Val loss: 0.2275\n",
      "Epoch: 2/200 | Train loss: 0.8606 | Val loss: 0.2329\n",
      "Epoch: 3/200 | Train loss: 0.7121 | Val loss: 0.2275\n",
      "Epoch: 4/200 | Train loss: 0.6246 | Val loss: 0.2366\n",
      "Epoch: 5/200 | Train loss: 0.5871 | Val loss: 0.2289\n",
      "Epoch: 6/200 | Train loss: 0.5244 | Val loss: 0.2392\n",
      "Epoch: 7/200 | Train loss: 0.4915 | Val loss: 0.2316\n",
      "Epoch: 8/200 | Train loss: 0.4521 | Val loss: 0.2263\n",
      "Epoch: 9/200 | Train loss: 0.4479 | Val loss: 0.2318\n",
      "Epoch: 10/200 | Train loss: 0.4103 | Val loss: 0.2278\n",
      "Epoch: 11/200 | Train loss: 0.4031 | Val loss: 0.2284\n",
      "Epoch: 12/200 | Train loss: 0.3908 | Val loss: 0.2316\n",
      "Epoch: 13/200 | Train loss: 0.3983 | Val loss: 0.2316\n",
      "Epoch: 14/200 | Train loss: 0.4046 | Val loss: 0.2265\n",
      "Epoch: 15/200 | Train loss: 0.3975 | Val loss: 0.2523\n",
      "Epoch: 16/200 | Train loss: 0.3980 | Val loss: 0.2491\n",
      "Epoch: 17/200 | Train loss: 0.4046 | Val loss: 0.2279\n",
      "Epoch: 18/200 | Train loss: 0.4007 | Val loss: 0.2261\n",
      "Epoch: 19/200 | Train loss: 0.3876 | Val loss: 0.2269\n",
      "Epoch: 20/200 | Train loss: 0.4036 | Val loss: 0.2266\n",
      "Epoch: 21/200 | Train loss: 0.3956 | Val loss: 0.2327\n",
      "Epoch: 22/200 | Train loss: 0.4081 | Val loss: 0.2296\n",
      "Epoch: 23/200 | Train loss: 0.4056 | Val loss: 0.2536\n",
      "Epoch: 24/200 | Train loss: 0.3956 | Val loss: 0.2259\n",
      "Epoch: 25/200 | Train loss: 0.3928 | Val loss: 0.2532\n",
      "Epoch: 26/200 | Train loss: 0.4110 | Val loss: 0.2256\n",
      "Epoch: 27/200 | Train loss: 0.3932 | Val loss: 0.2547\n",
      "Epoch: 28/200 | Train loss: 0.3870 | Val loss: 0.2339\n",
      "Epoch: 29/200 | Train loss: 0.3925 | Val loss: 0.2323\n",
      "Epoch: 30/200 | Train loss: 0.3821 | Val loss: 0.2836\n",
      "Epoch: 31/200 | Train loss: 0.3945 | Val loss: 0.2272\n",
      "Epoch: 32/200 | Train loss: 0.3889 | Val loss: 0.2310\n",
      "Epoch: 33/200 | Train loss: 0.4011 | Val loss: 0.2273\n",
      "Epoch: 34/200 | Train loss: 0.3871 | Val loss: 0.2447\n",
      "Epoch: 35/200 | Train loss: 0.3788 | Val loss: 0.2697\n",
      "Epoch: 36/200 | Train loss: 0.3787 | Val loss: 0.2263\n",
      "Early stopping!\n",
      "Trained in 36 epochs with best val loss: 0.2256\n",
      "Test loss: 0.22681891097538714\n",
      "\n",
      "\n",
      "---Combination 15---\n",
      "-Hidden size: 128\n",
      "-Batch size: 64\n",
      "-Dropout: 0.2\n",
      "-Depth: 4\n",
      "-Learning rate: 0.001\n",
      "\n",
      "Epoch: 1/200 | Train loss: 1.2095 | Val loss: 0.1196\n",
      "Epoch: 2/200 | Train loss: 0.2280 | Val loss: 0.0417\n",
      "Epoch: 3/200 | Train loss: 0.1395 | Val loss: 0.0284\n",
      "Epoch: 4/200 | Train loss: 0.1129 | Val loss: 0.0644\n",
      "Epoch: 5/200 | Train loss: 0.1026 | Val loss: 0.0141\n",
      "Epoch: 6/200 | Train loss: 0.0890 | Val loss: 0.0152\n",
      "Epoch: 7/200 | Train loss: 0.0843 | Val loss: 0.0203\n",
      "Epoch: 8/200 | Train loss: 0.0832 | Val loss: 0.0755\n",
      "Epoch: 9/200 | Train loss: 0.0831 | Val loss: 0.0182\n",
      "Epoch: 10/200 | Train loss: 0.0750 | Val loss: 0.0324\n",
      "Epoch: 11/200 | Train loss: 0.0747 | Val loss: 0.0311\n",
      "Epoch: 12/200 | Train loss: 0.0697 | Val loss: 0.0213\n",
      "Epoch: 13/200 | Train loss: 0.0693 | Val loss: 0.0143\n",
      "Epoch: 14/200 | Train loss: 0.0662 | Val loss: 0.0096\n",
      "Epoch: 15/200 | Train loss: 0.0655 | Val loss: 0.0503\n",
      "Epoch: 16/200 | Train loss: 0.0639 | Val loss: 0.0147\n",
      "Epoch: 17/200 | Train loss: 0.0595 | Val loss: 0.0446\n",
      "Epoch: 18/200 | Train loss: 0.0590 | Val loss: 0.0130\n",
      "Epoch: 19/200 | Train loss: 0.0534 | Val loss: 0.0096\n",
      "Epoch: 20/200 | Train loss: 0.0528 | Val loss: 0.0086\n",
      "Epoch: 21/200 | Train loss: 0.0511 | Val loss: 0.0090\n",
      "Epoch: 22/200 | Train loss: 0.0504 | Val loss: 0.0117\n",
      "Epoch: 23/200 | Train loss: 0.0485 | Val loss: 0.0107\n",
      "Epoch: 24/200 | Train loss: 0.0476 | Val loss: 0.0211\n",
      "Epoch: 25/200 | Train loss: 0.0481 | Val loss: 0.0085\n",
      "Epoch: 26/200 | Train loss: 0.0461 | Val loss: 0.0118\n",
      "Epoch: 27/200 | Train loss: 0.0453 | Val loss: 0.0135\n",
      "Epoch: 28/200 | Train loss: 0.0447 | Val loss: 0.0102\n",
      "Epoch: 29/200 | Train loss: 0.0436 | Val loss: 0.0166\n",
      "Epoch: 30/200 | Train loss: 0.0426 | Val loss: 0.0148\n",
      "Epoch: 31/200 | Train loss: 0.0422 | Val loss: 0.0110\n",
      "Epoch: 32/200 | Train loss: 0.0402 | Val loss: 0.0106\n",
      "Epoch: 33/200 | Train loss: 0.0406 | Val loss: 0.0137\n",
      "Epoch: 34/200 | Train loss: 0.0390 | Val loss: 0.0115\n",
      "Epoch: 35/200 | Train loss: 0.0397 | Val loss: 0.0127\n",
      "Early stopping!\n",
      "Trained in 35 epochs with best val loss: 0.0085\n",
      "Test loss: 0.012619814433265424\n",
      "\n",
      "\n",
      "---Combination 16---\n",
      "-Hidden size: 128\n",
      "-Batch size: 64\n",
      "-Dropout: 0.2\n",
      "-Depth: 4\n",
      "-Learning rate: 0.1\n",
      "\n",
      "Epoch: 1/200 | Train loss: 9721298.4345 | Val loss: 14.3579\n",
      "Epoch: 2/200 | Train loss: 20.0982 | Val loss: 14.1789\n",
      "Epoch: 3/200 | Train loss: 14.0487 | Val loss: 13.9621\n",
      "Epoch: 4/200 | Train loss: 13.3587 | Val loss: 13.6869\n",
      "Epoch: 5/200 | Train loss: 14.0812 | Val loss: 13.3428\n",
      "Epoch: 6/200 | Train loss: 12.7634 | Val loss: 12.9599\n",
      "Epoch: 7/200 | Train loss: 12.3496 | Val loss: 12.5359\n",
      "Epoch: 8/200 | Train loss: 12.0100 | Val loss: 12.0772\n",
      "Epoch: 9/200 | Train loss: 11.7908 | Val loss: 11.5847\n",
      "Epoch: 10/200 | Train loss: 12.1906 | Val loss: 11.0634\n",
      "Epoch: 11/200 | Train loss: 10.6799 | Val loss: 10.5219\n",
      "Epoch: 12/200 | Train loss: 9.6219 | Val loss: 9.9609\n",
      "Epoch: 13/200 | Train loss: 8.5351 | Val loss: 9.4409\n",
      "Epoch: 14/200 | Train loss: 7.7704 | Val loss: 8.9073\n",
      "Epoch: 15/200 | Train loss: 7.2983 | Val loss: 8.3652\n",
      "Epoch: 16/200 | Train loss: 6.8368 | Val loss: 7.8059\n",
      "Epoch: 17/200 | Train loss: 6.4768 | Val loss: 7.2454\n",
      "Epoch: 18/200 | Train loss: 5.8535 | Val loss: 6.6806\n",
      "Epoch: 19/200 | Train loss: 5.4998 | Val loss: 6.1114\n",
      "Epoch: 20/200 | Train loss: 5.5062 | Val loss: 5.4810\n",
      "Epoch: 21/200 | Train loss: 4.9254 | Val loss: 4.8767\n",
      "Epoch: 22/200 | Train loss: 4.3533 | Val loss: 4.2956\n",
      "Epoch: 23/200 | Train loss: 3.8450 | Val loss: 3.7470\n",
      "Epoch: 24/200 | Train loss: 3.3167 | Val loss: 3.2378\n",
      "Epoch: 25/200 | Train loss: 2.9633 | Val loss: 2.7665\n",
      "Epoch: 26/200 | Train loss: 2.4454 | Val loss: 2.3393\n",
      "Epoch: 27/200 | Train loss: 2.0882 | Val loss: 1.9575\n",
      "Epoch: 28/200 | Train loss: 1.7337 | Val loss: 1.6195\n",
      "Epoch: 29/200 | Train loss: 1.4268 | Val loss: 1.3286\n",
      "Epoch: 30/200 | Train loss: 1.1644 | Val loss: 1.0824\n",
      "Epoch: 31/200 | Train loss: 0.9753 | Val loss: 0.8753\n",
      "Epoch: 32/200 | Train loss: 0.7882 | Val loss: 0.7084\n",
      "Epoch: 33/200 | Train loss: 0.6388 | Val loss: 0.5749\n",
      "Epoch: 34/200 | Train loss: 0.5301 | Val loss: 0.4712\n",
      "Epoch: 35/200 | Train loss: 0.4385 | Val loss: 0.3934\n",
      "Epoch: 36/200 | Train loss: 0.3768 | Val loss: 0.3363\n",
      "Epoch: 37/200 | Train loss: 0.3296 | Val loss: 0.2961\n",
      "Epoch: 38/200 | Train loss: 0.2970 | Val loss: 0.2689\n",
      "Epoch: 39/200 | Train loss: 0.2792 | Val loss: 0.2507\n",
      "Epoch: 40/200 | Train loss: 0.2613 | Val loss: 0.2393\n",
      "Epoch: 41/200 | Train loss: 0.2655 | Val loss: 0.2327\n",
      "Epoch: 42/200 | Train loss: 0.2463 | Val loss: 0.2290\n",
      "Epoch: 43/200 | Train loss: 0.2426 | Val loss: 0.2271\n",
      "Epoch: 44/200 | Train loss: 0.2454 | Val loss: 0.2261\n",
      "Epoch: 45/200 | Train loss: 0.2401 | Val loss: 0.2257\n",
      "Epoch: 46/200 | Train loss: 0.2400 | Val loss: 0.2256\n",
      "Epoch: 47/200 | Train loss: 0.2391 | Val loss: 0.2256\n",
      "Epoch: 48/200 | Train loss: 0.2384 | Val loss: 0.2256\n",
      "Epoch: 49/200 | Train loss: 0.2387 | Val loss: 0.2256\n",
      "Epoch: 50/200 | Train loss: 0.2390 | Val loss: 0.2256\n",
      "Epoch: 51/200 | Train loss: 0.2378 | Val loss: 0.2256\n",
      "Epoch: 52/200 | Train loss: 0.2382 | Val loss: 0.2257\n",
      "Epoch: 53/200 | Train loss: 0.2383 | Val loss: 0.2257\n",
      "Epoch: 54/200 | Train loss: 0.2378 | Val loss: 0.2257\n",
      "Epoch: 55/200 | Train loss: 0.2377 | Val loss: 0.2256\n",
      "Epoch: 56/200 | Train loss: 0.2376 | Val loss: 0.2256\n",
      "Epoch: 57/200 | Train loss: 0.2376 | Val loss: 0.2257\n",
      "Early stopping!\n",
      "Trained in 57 epochs with best val loss: 0.2256\n",
      "Test loss: 0.22555960520454074\n",
      "\n",
      "\n",
      "---Combination 17---\n",
      "-Hidden size: 128\n",
      "-Batch size: 64\n",
      "-Dropout: 0.2\n",
      "-Depth: 5\n",
      "-Learning rate: 0.001\n",
      "\n",
      "Epoch: 1/200 | Train loss: 1.1594 | Val loss: 0.2181\n",
      "Epoch: 2/200 | Train loss: 0.1750 | Val loss: 0.0297\n",
      "Epoch: 3/200 | Train loss: 0.1177 | Val loss: 0.0706\n",
      "Epoch: 4/200 | Train loss: 0.0956 | Val loss: 0.0499\n",
      "Epoch: 5/200 | Train loss: 0.0856 | Val loss: 0.0166\n",
      "Epoch: 6/200 | Train loss: 0.0785 | Val loss: 0.0282\n",
      "Epoch: 7/200 | Train loss: 0.0756 | Val loss: 0.0237\n",
      "Epoch: 8/200 | Train loss: 0.0693 | Val loss: 0.0179\n",
      "Epoch: 9/200 | Train loss: 0.0687 | Val loss: 0.0338\n",
      "Epoch: 10/200 | Train loss: 0.0631 | Val loss: 0.0215\n",
      "Epoch: 11/200 | Train loss: 0.0608 | Val loss: 0.0107\n",
      "Epoch: 12/200 | Train loss: 0.0579 | Val loss: 0.0228\n",
      "Epoch: 13/200 | Train loss: 0.0557 | Val loss: 0.0100\n",
      "Epoch: 14/200 | Train loss: 0.0549 | Val loss: 0.0128\n",
      "Epoch: 15/200 | Train loss: 0.0520 | Val loss: 0.0107\n",
      "Epoch: 16/200 | Train loss: 0.0515 | Val loss: 0.0097\n",
      "Epoch: 17/200 | Train loss: 0.0487 | Val loss: 0.0158\n",
      "Epoch: 18/200 | Train loss: 0.0479 | Val loss: 0.0107\n",
      "Epoch: 19/200 | Train loss: 0.0449 | Val loss: 0.0113\n",
      "Epoch: 20/200 | Train loss: 0.0456 | Val loss: 0.0093\n",
      "Epoch: 21/200 | Train loss: 0.0444 | Val loss: 0.0102\n",
      "Epoch: 22/200 | Train loss: 0.0433 | Val loss: 0.0131\n",
      "Epoch: 23/200 | Train loss: 0.0413 | Val loss: 0.0086\n",
      "Epoch: 24/200 | Train loss: 0.0424 | Val loss: 0.0159\n",
      "Epoch: 25/200 | Train loss: 0.0407 | Val loss: 0.0099\n",
      "Epoch: 26/200 | Train loss: 0.0385 | Val loss: 0.0122\n",
      "Epoch: 27/200 | Train loss: 0.0387 | Val loss: 0.0103\n",
      "Epoch: 28/200 | Train loss: 0.0378 | Val loss: 0.0097\n",
      "Epoch: 29/200 | Train loss: 0.0391 | Val loss: 0.0127\n",
      "Epoch: 30/200 | Train loss: 0.0364 | Val loss: 0.0110\n",
      "Epoch: 31/200 | Train loss: 0.0373 | Val loss: 0.0073\n",
      "Epoch: 32/200 | Train loss: 0.0368 | Val loss: 0.0109\n",
      "Epoch: 33/200 | Train loss: 0.0350 | Val loss: 0.0102\n",
      "Epoch: 34/200 | Train loss: 0.0358 | Val loss: 0.0091\n",
      "Epoch: 35/200 | Train loss: 0.0332 | Val loss: 0.0232\n",
      "Epoch: 36/200 | Train loss: 0.0324 | Val loss: 0.0124\n",
      "Epoch: 37/200 | Train loss: 0.0335 | Val loss: 0.0222\n",
      "Epoch: 38/200 | Train loss: 0.0327 | Val loss: 0.0093\n",
      "Epoch: 39/200 | Train loss: 0.0318 | Val loss: 0.0095\n",
      "Epoch: 40/200 | Train loss: 0.0299 | Val loss: 0.0132\n",
      "Epoch: 41/200 | Train loss: 0.0305 | Val loss: 0.0088\n",
      "Early stopping!\n",
      "Trained in 41 epochs with best val loss: 0.0073\n",
      "Test loss: 0.008366564742248991\n",
      "\n",
      "\n",
      "---Combination 18---\n",
      "-Hidden size: 128\n",
      "-Batch size: 64\n",
      "-Dropout: 0.2\n",
      "-Depth: 5\n",
      "-Learning rate: 0.1\n",
      "\n",
      "Epoch: 1/200 | Train loss: 18840387.6100 | Val loss: 13.8871\n",
      "Epoch: 2/200 | Train loss: 14.6105 | Val loss: 13.7537\n",
      "Epoch: 3/200 | Train loss: 14.6040 | Val loss: 13.5755\n",
      "Epoch: 4/200 | Train loss: 13.7061 | Val loss: 13.3591\n",
      "Epoch: 5/200 | Train loss: 2867155.3925 | Val loss: 14.0373\n",
      "Epoch: 6/200 | Train loss: 26.8816 | Val loss: 13.7397\n",
      "Epoch: 7/200 | Train loss: 21.6779 | Val loss: 13.3998\n",
      "Epoch: 8/200 | Train loss: 112.0467 | Val loss: 13.0408\n",
      "Epoch: 9/200 | Train loss: 12.9133 | Val loss: 12.6406\n",
      "Epoch: 10/200 | Train loss: 12.5308 | Val loss: 12.2111\n",
      "Epoch: 11/200 | Train loss: 12.0533 | Val loss: 11.7541\n",
      "Epoch: 12/200 | Train loss: 11.5824 | Val loss: 11.2715\n",
      "Epoch: 13/200 | Train loss: 11.0872 | Val loss: 10.7658\n",
      "Epoch: 14/200 | Train loss: 10.5692 | Val loss: 10.2386\n",
      "Epoch: 15/200 | Train loss: 10.0312 | Val loss: 9.6927\n",
      "Epoch: 16/200 | Train loss: 9.4762 | Val loss: 9.1315\n",
      "Epoch: 17/200 | Train loss: 8.9073 | Val loss: 8.5583\n",
      "Epoch: 18/200 | Train loss: 8.3278 | Val loss: 7.9764\n",
      "Epoch: 19/200 | Train loss: 8.0148 | Val loss: 7.3887\n",
      "Epoch: 20/200 | Train loss: 7.1521 | Val loss: 6.8026\n",
      "Epoch: 21/200 | Train loss: 32.5957 | Val loss: 6.2151\n",
      "Epoch: 22/200 | Train loss: 5.9786 | Val loss: 5.6397\n",
      "Epoch: 23/200 | Train loss: 5.4098 | Val loss: 5.0804\n",
      "Epoch: 24/200 | Train loss: 4.8568 | Val loss: 4.5391\n",
      "Epoch: 25/200 | Train loss: 4.3241 | Val loss: 4.0200\n",
      "Epoch: 26/200 | Train loss: 3.8160 | Val loss: 3.5280\n",
      "Epoch: 27/200 | Train loss: 3.3367 | Val loss: 3.0664\n",
      "Epoch: 28/200 | Train loss: 2.8898 | Val loss: 2.6393\n",
      "Epoch: 29/200 | Train loss: 2.4784 | Val loss: 2.2487\n",
      "Epoch: 30/200 | Train loss: 2.1046 | Val loss: 1.8967\n",
      "Epoch: 31/200 | Train loss: 1.7700 | Val loss: 1.5837\n",
      "Epoch: 32/200 | Train loss: 1.4750 | Val loss: 1.3110\n",
      "Epoch: 33/200 | Train loss: 225.3808 | Val loss: 1.1091\n",
      "Epoch: 34/200 | Train loss: 27.4000 | Val loss: 0.8947\n",
      "Epoch: 35/200 | Train loss: 0.8337 | Val loss: 0.7286\n",
      "Epoch: 36/200 | Train loss: 0.6822 | Val loss: 0.5947\n",
      "Epoch: 37/200 | Train loss: 0.5610 | Val loss: 0.4893\n",
      "Epoch: 38/200 | Train loss: 0.4670 | Val loss: 0.4089\n",
      "Epoch: 39/200 | Train loss: 1.0592 | Val loss: 0.3482\n",
      "Epoch: 40/200 | Train loss: 0.3425 | Val loss: 0.3052\n",
      "Epoch: 41/200 | Train loss: 0.3055 | Val loss: 0.2755\n",
      "Epoch: 42/200 | Train loss: 0.2799 | Val loss: 0.2556\n",
      "Epoch: 43/200 | Train loss: 0.2629 | Val loss: 0.2426\n",
      "Epoch: 44/200 | Train loss: 0.2521 | Val loss: 0.2348\n",
      "Epoch: 45/200 | Train loss: 0.2455 | Val loss: 0.2302\n",
      "Epoch: 46/200 | Train loss: 1.6150 | Val loss: 0.2274\n",
      "Epoch: 47/200 | Train loss: 0.2393 | Val loss: 0.2263\n",
      "Epoch: 48/200 | Train loss: 0.2383 | Val loss: 0.2258\n",
      "Epoch: 49/200 | Train loss: 0.2378 | Val loss: 0.2256\n",
      "Epoch: 50/200 | Train loss: 0.2376 | Val loss: 0.2256\n",
      "Epoch: 51/200 | Train loss: 0.2440 | Val loss: 0.2256\n",
      "Epoch: 52/200 | Train loss: 0.2375 | Val loss: 0.2256\n",
      "Epoch: 53/200 | Train loss: 0.2374 | Val loss: 0.2256\n",
      "Epoch: 54/200 | Train loss: 0.2374 | Val loss: 0.2256\n",
      "Epoch: 55/200 | Train loss: 0.2374 | Val loss: 0.2256\n",
      "Epoch: 56/200 | Train loss: 0.2374 | Val loss: 0.2256\n",
      "Epoch: 57/200 | Train loss: 0.2374 | Val loss: 0.2257\n",
      "Epoch: 58/200 | Train loss: 0.2374 | Val loss: 0.2257\n",
      "Epoch: 59/200 | Train loss: 0.2374 | Val loss: 0.2256\n",
      "Epoch: 60/200 | Train loss: 0.2374 | Val loss: 0.2257\n",
      "Early stopping!\n",
      "Trained in 60 epochs with best val loss: 0.2256\n",
      "Test loss: 0.22555960978286854\n",
      "\n",
      "\n",
      "---Combination 19---\n",
      "-Hidden size: 128\n",
      "-Batch size: 64\n",
      "-Dropout: 0.3\n",
      "-Depth: 3\n",
      "-Learning rate: 0.001\n",
      "\n",
      "Epoch: 1/200 | Train loss: 1.2645 | Val loss: 0.1880\n",
      "Epoch: 2/200 | Train loss: 0.2791 | Val loss: 0.0690\n",
      "Epoch: 3/200 | Train loss: 0.1839 | Val loss: 0.0484\n",
      "Epoch: 4/200 | Train loss: 0.1447 | Val loss: 0.0622\n",
      "Epoch: 5/200 | Train loss: 0.1323 | Val loss: 0.0246\n",
      "Epoch: 6/200 | Train loss: 0.1254 | Val loss: 0.0228\n",
      "Epoch: 7/200 | Train loss: 0.1185 | Val loss: 0.0308\n",
      "Epoch: 8/200 | Train loss: 0.1121 | Val loss: 0.0258\n",
      "Epoch: 9/200 | Train loss: 0.1057 | Val loss: 0.0249\n",
      "Epoch: 10/200 | Train loss: 0.0996 | Val loss: 0.0174\n",
      "Epoch: 11/200 | Train loss: 0.0959 | Val loss: 0.0446\n",
      "Epoch: 12/200 | Train loss: 0.0906 | Val loss: 0.0344\n",
      "Epoch: 13/200 | Train loss: 0.0877 | Val loss: 0.0260\n",
      "Epoch: 14/200 | Train loss: 0.0865 | Val loss: 0.0406\n",
      "Epoch: 15/200 | Train loss: 0.0795 | Val loss: 0.0171\n",
      "Epoch: 16/200 | Train loss: 0.0834 | Val loss: 0.0222\n",
      "Epoch: 17/200 | Train loss: 0.0759 | Val loss: 0.0115\n",
      "Epoch: 18/200 | Train loss: 0.0761 | Val loss: 0.0271\n",
      "Epoch: 19/200 | Train loss: 0.0722 | Val loss: 0.0273\n",
      "Epoch: 20/200 | Train loss: 0.0698 | Val loss: 0.0429\n",
      "Epoch: 21/200 | Train loss: 0.0695 | Val loss: 0.0210\n",
      "Epoch: 22/200 | Train loss: 0.0671 | Val loss: 0.0330\n",
      "Epoch: 23/200 | Train loss: 0.0684 | Val loss: 0.0214\n",
      "Epoch: 24/200 | Train loss: 0.0673 | Val loss: 0.0136\n",
      "Epoch: 25/200 | Train loss: 0.0634 | Val loss: 0.0241\n",
      "Epoch: 26/200 | Train loss: 0.0621 | Val loss: 0.0241\n",
      "Epoch: 27/200 | Train loss: 0.0635 | Val loss: 0.0339\n",
      "Early stopping!\n",
      "Trained in 27 epochs with best val loss: 0.0115\n",
      "Test loss: 0.033396153670290245\n",
      "\n",
      "\n",
      "---Combination 20---\n",
      "-Hidden size: 128\n",
      "-Batch size: 64\n",
      "-Dropout: 0.3\n",
      "-Depth: 3\n",
      "-Learning rate: 0.1\n",
      "\n",
      "Epoch: 1/200 | Train loss: 553771.5548 | Val loss: 2.1898\n",
      "Epoch: 2/200 | Train loss: 5.8194 | Val loss: 1.0382\n",
      "Epoch: 3/200 | Train loss: 1.8261 | Val loss: 0.9052\n",
      "Epoch: 4/200 | Train loss: 1.3708 | Val loss: 0.5849\n",
      "Epoch: 5/200 | Train loss: 1.3020 | Val loss: 0.2800\n",
      "Epoch: 6/200 | Train loss: 1.1547 | Val loss: 0.4112\n",
      "Epoch: 7/200 | Train loss: 1.1153 | Val loss: 0.2475\n",
      "Epoch: 8/200 | Train loss: 1.1410 | Val loss: 0.5531\n",
      "Epoch: 9/200 | Train loss: 1.0058 | Val loss: 0.2261\n",
      "Epoch: 10/200 | Train loss: 0.9296 | Val loss: 0.2317\n",
      "Epoch: 11/200 | Train loss: 0.9267 | Val loss: 0.4913\n",
      "Epoch: 12/200 | Train loss: 0.8997 | Val loss: 0.3453\n",
      "Epoch: 13/200 | Train loss: 0.8909 | Val loss: 0.5533\n",
      "Epoch: 14/200 | Train loss: 0.9044 | Val loss: 0.2377\n",
      "Epoch: 15/200 | Train loss: 0.8676 | Val loss: 0.2322\n",
      "Epoch: 16/200 | Train loss: 0.8646 | Val loss: 0.2335\n",
      "Epoch: 17/200 | Train loss: 0.8738 | Val loss: 0.7465\n",
      "Epoch: 18/200 | Train loss: 0.8361 | Val loss: 0.2475\n",
      "Epoch: 19/200 | Train loss: 0.7656 | Val loss: 0.2273\n",
      "Early stopping!\n",
      "Trained in 19 epochs with best val loss: 0.2261\n",
      "Test loss: 0.2281624997439592\n",
      "\n",
      "\n",
      "---Combination 21---\n",
      "-Hidden size: 128\n",
      "-Batch size: 64\n",
      "-Dropout: 0.3\n",
      "-Depth: 4\n",
      "-Learning rate: 0.001\n",
      "\n",
      "Epoch: 1/200 | Train loss: 1.2076 | Val loss: 1.0021\n",
      "Epoch: 2/200 | Train loss: 0.2438 | Val loss: 0.0670\n",
      "Epoch: 3/200 | Train loss: 0.1576 | Val loss: 0.0811\n",
      "Epoch: 4/200 | Train loss: 0.1402 | Val loss: 0.0578\n",
      "Epoch: 5/200 | Train loss: 0.1263 | Val loss: 0.0311\n",
      "Epoch: 6/200 | Train loss: 0.1167 | Val loss: 0.0285\n",
      "Epoch: 7/200 | Train loss: 0.1115 | Val loss: 0.0269\n",
      "Epoch: 8/200 | Train loss: 0.1036 | Val loss: 0.0268\n",
      "Epoch: 9/200 | Train loss: 0.1000 | Val loss: 0.0302\n",
      "Epoch: 10/200 | Train loss: 0.0986 | Val loss: 0.0525\n",
      "Epoch: 11/200 | Train loss: 0.0929 | Val loss: 0.0280\n",
      "Epoch: 12/200 | Train loss: 0.0894 | Val loss: 0.0453\n",
      "Epoch: 13/200 | Train loss: 0.0844 | Val loss: 0.0285\n",
      "Epoch: 14/200 | Train loss: 0.0802 | Val loss: 0.0311\n",
      "Epoch: 15/200 | Train loss: 0.0812 | Val loss: 0.0321\n",
      "Epoch: 16/200 | Train loss: 0.0776 | Val loss: 0.0403\n",
      "Epoch: 17/200 | Train loss: 0.0749 | Val loss: 0.0128\n",
      "Epoch: 18/200 | Train loss: 0.0719 | Val loss: 0.0236\n",
      "Epoch: 19/200 | Train loss: 0.0713 | Val loss: 0.0275\n",
      "Epoch: 20/200 | Train loss: 0.0723 | Val loss: 0.0231\n",
      "Epoch: 21/200 | Train loss: 0.0700 | Val loss: 0.0260\n",
      "Epoch: 22/200 | Train loss: 0.0676 | Val loss: 0.0218\n",
      "Epoch: 23/200 | Train loss: 0.0635 | Val loss: 0.0168\n",
      "Epoch: 24/200 | Train loss: 0.0666 | Val loss: 0.0344\n",
      "Epoch: 25/200 | Train loss: 0.0621 | Val loss: 0.0275\n",
      "Epoch: 26/200 | Train loss: 0.0632 | Val loss: 0.0178\n",
      "Epoch: 27/200 | Train loss: 0.0609 | Val loss: 0.0279\n",
      "Early stopping!\n",
      "Trained in 27 epochs with best val loss: 0.0128\n",
      "Test loss: 0.028980270433037178\n",
      "\n",
      "\n",
      "---Combination 22---\n",
      "-Hidden size: 128\n",
      "-Batch size: 64\n",
      "-Dropout: 0.3\n",
      "-Depth: 4\n",
      "-Learning rate: 0.1\n",
      "\n",
      "Epoch: 1/200 | Train loss: 1399347.5740 | Val loss: 10.4128\n",
      "Epoch: 2/200 | Train loss: 6.5705 | Val loss: 7.3405\n",
      "Epoch: 3/200 | Train loss: 4.7744 | Val loss: 6.4206\n",
      "Epoch: 4/200 | Train loss: 4.0628 | Val loss: 4.0524\n",
      "Epoch: 5/200 | Train loss: 3.3931 | Val loss: 1.7440\n",
      "Epoch: 6/200 | Train loss: 3.0009 | Val loss: 2.2483\n",
      "Epoch: 7/200 | Train loss: 2.7487 | Val loss: 1.3335\n",
      "Epoch: 8/200 | Train loss: 2.3534 | Val loss: 0.7141\n",
      "Epoch: 9/200 | Train loss: 1.9535 | Val loss: 0.7829\n",
      "Epoch: 10/200 | Train loss: 1.9173 | Val loss: 0.8151\n",
      "Epoch: 11/200 | Train loss: 1.8444 | Val loss: 0.8867\n",
      "Epoch: 12/200 | Train loss: 1.7219 | Val loss: 0.6590\n",
      "Epoch: 13/200 | Train loss: 1.7025 | Val loss: 0.7482\n",
      "Epoch: 14/200 | Train loss: 1.7047 | Val loss: 0.7317\n",
      "Epoch: 15/200 | Train loss: 1.6610 | Val loss: 0.5947\n",
      "Epoch: 16/200 | Train loss: 1.5937 | Val loss: 0.5694\n",
      "Epoch: 17/200 | Train loss: 1.3790 | Val loss: 0.3879\n",
      "Epoch: 18/200 | Train loss: 1.2404 | Val loss: 0.2761\n",
      "Epoch: 19/200 | Train loss: 1.2480 | Val loss: 0.2435\n",
      "Epoch: 20/200 | Train loss: 1.1993 | Val loss: 0.2770\n",
      "Epoch: 21/200 | Train loss: 1.0794 | Val loss: 0.2619\n",
      "Epoch: 22/200 | Train loss: 1.0211 | Val loss: 0.2632\n",
      "Epoch: 23/200 | Train loss: 0.9908 | Val loss: 0.3940\n",
      "Epoch: 24/200 | Train loss: 0.9510 | Val loss: 0.2619\n",
      "Epoch: 25/200 | Train loss: 0.9456 | Val loss: 0.2359\n",
      "Epoch: 26/200 | Train loss: 0.8933 | Val loss: 0.2908\n",
      "Epoch: 27/200 | Train loss: 0.8852 | Val loss: 0.2569\n",
      "Epoch: 28/200 | Train loss: 0.8383 | Val loss: 0.2525\n",
      "Epoch: 29/200 | Train loss: 0.8019 | Val loss: 0.2554\n",
      "Epoch: 30/200 | Train loss: 0.7906 | Val loss: 0.2456\n",
      "Epoch: 31/200 | Train loss: 0.7544 | Val loss: 0.2849\n",
      "Epoch: 32/200 | Train loss: 0.7663 | Val loss: 0.2659\n",
      "Epoch: 33/200 | Train loss: 0.7110 | Val loss: 0.2558\n",
      "Epoch: 34/200 | Train loss: 0.6508 | Val loss: 0.2310\n",
      "Epoch: 35/200 | Train loss: 0.6475 | Val loss: 0.2918\n",
      "Epoch: 36/200 | Train loss: 0.5966 | Val loss: 0.3069\n",
      "Epoch: 37/200 | Train loss: 0.5549 | Val loss: 0.2449\n",
      "Epoch: 38/200 | Train loss: 0.5200 | Val loss: 0.2292\n",
      "Epoch: 39/200 | Train loss: 0.4948 | Val loss: 0.2256\n",
      "Epoch: 40/200 | Train loss: 0.4580 | Val loss: 0.2484\n",
      "Epoch: 41/200 | Train loss: 0.4408 | Val loss: 0.2532\n",
      "Epoch: 42/200 | Train loss: 0.4134 | Val loss: 0.2407\n",
      "Epoch: 43/200 | Train loss: 0.3901 | Val loss: 0.2303\n",
      "Epoch: 44/200 | Train loss: 0.3612 | Val loss: 0.2387\n",
      "Epoch: 45/200 | Train loss: 0.3540 | Val loss: 0.2370\n",
      "Epoch: 46/200 | Train loss: 0.3245 | Val loss: 0.2273\n",
      "Epoch: 47/200 | Train loss: 0.3090 | Val loss: 0.2496\n",
      "Epoch: 48/200 | Train loss: 0.2962 | Val loss: 0.2291\n",
      "Epoch: 49/200 | Train loss: 0.2860 | Val loss: 0.2260\n",
      "Early stopping!\n",
      "Trained in 49 epochs with best val loss: 0.2256\n",
      "Test loss: 0.22646181204396745\n",
      "\n",
      "\n",
      "---Combination 23---\n",
      "-Hidden size: 128\n",
      "-Batch size: 64\n",
      "-Dropout: 0.3\n",
      "-Depth: 5\n",
      "-Learning rate: 0.001\n",
      "\n",
      "Epoch: 1/200 | Train loss: 1.2420 | Val loss: 0.3894\n",
      "Epoch: 2/200 | Train loss: 0.2421 | Val loss: 0.0500\n",
      "Epoch: 3/200 | Train loss: 0.1741 | Val loss: 0.0624\n",
      "Epoch: 4/200 | Train loss: 0.1518 | Val loss: 0.0801\n",
      "Epoch: 5/200 | Train loss: 0.1399 | Val loss: 0.0271\n",
      "Epoch: 6/200 | Train loss: 0.1321 | Val loss: 0.0353\n",
      "Epoch: 7/200 | Train loss: 0.1260 | Val loss: 0.0410\n",
      "Epoch: 8/200 | Train loss: 0.1162 | Val loss: 0.0603\n",
      "Epoch: 9/200 | Train loss: 0.1154 | Val loss: 0.0405\n",
      "Epoch: 10/200 | Train loss: 0.1086 | Val loss: 0.0483\n",
      "Epoch: 11/200 | Train loss: 0.1014 | Val loss: 0.0195\n",
      "Epoch: 12/200 | Train loss: 0.1014 | Val loss: 0.0682\n",
      "Epoch: 13/200 | Train loss: 0.0979 | Val loss: 0.0205\n",
      "Epoch: 14/200 | Train loss: 0.0916 | Val loss: 0.0141\n",
      "Epoch: 15/200 | Train loss: 0.0903 | Val loss: 0.0436\n",
      "Epoch: 16/200 | Train loss: 0.0843 | Val loss: 0.0179\n",
      "Epoch: 17/200 | Train loss: 0.0852 | Val loss: 0.0289\n",
      "Epoch: 18/200 | Train loss: 0.0791 | Val loss: 0.0269\n",
      "Epoch: 19/200 | Train loss: 0.0745 | Val loss: 0.0329\n",
      "Epoch: 20/200 | Train loss: 0.0755 | Val loss: 0.0269\n",
      "Epoch: 21/200 | Train loss: 0.0757 | Val loss: 0.0526\n",
      "Epoch: 22/200 | Train loss: 0.0728 | Val loss: 0.0191\n",
      "Epoch: 23/200 | Train loss: 0.0710 | Val loss: 0.0216\n",
      "Epoch: 24/200 | Train loss: 0.0694 | Val loss: 0.0447\n",
      "Early stopping!\n",
      "Trained in 24 epochs with best val loss: 0.0141\n",
      "Test loss: 0.04570431836705277\n",
      "\n",
      "\n",
      "---Combination 24---\n",
      "-Hidden size: 128\n",
      "-Batch size: 64\n",
      "-Dropout: 0.3\n",
      "-Depth: 5\n",
      "-Learning rate: 0.1\n",
      "\n",
      "Epoch: 1/200 | Train loss: 27232571.4020 | Val loss: 14.6187\n",
      "Epoch: 2/200 | Train loss: 3218.6098 | Val loss: 14.4858\n",
      "Epoch: 3/200 | Train loss: 109.0901 | Val loss: 14.3279\n",
      "Epoch: 4/200 | Train loss: 16.2334 | Val loss: 14.1344\n",
      "Epoch: 5/200 | Train loss: 14.8900 | Val loss: 13.9100\n",
      "Epoch: 6/200 | Train loss: 15.0386 | Val loss: 13.6552\n",
      "Epoch: 7/200 | Train loss: 15.3087 | Val loss: 13.3690\n",
      "Epoch: 8/200 | Train loss: 13.3263 | Val loss: 13.0563\n",
      "Epoch: 9/200 | Train loss: 13.0590 | Val loss: 12.7158\n",
      "Epoch: 10/200 | Train loss: 37.5053 | Val loss: 12.3557\n",
      "Epoch: 11/200 | Train loss: 21.0932 | Val loss: 11.9653\n",
      "Epoch: 12/200 | Train loss: 11.7817 | Val loss: 11.5492\n",
      "Epoch: 13/200 | Train loss: 11.3579 | Val loss: 11.1097\n",
      "Epoch: 14/200 | Train loss: 11.2577 | Val loss: 10.6494\n",
      "Epoch: 15/200 | Train loss: 10.5954 | Val loss: 10.1671\n",
      "Epoch: 16/200 | Train loss: 9.9354 | Val loss: 9.6676\n",
      "Epoch: 17/200 | Train loss: 34.2469 | Val loss: 9.1584\n",
      "Epoch: 18/200 | Train loss: 12.4453 | Val loss: 8.6229\n",
      "Epoch: 19/200 | Train loss: 9.4595 | Val loss: 8.0810\n",
      "Epoch: 20/200 | Train loss: 7.8686 | Val loss: 7.5355\n",
      "Epoch: 21/200 | Train loss: 7.2908 | Val loss: 6.9851\n",
      "Epoch: 22/200 | Train loss: 6.7470 | Val loss: 6.4346\n",
      "Epoch: 23/200 | Train loss: 6.1900 | Val loss: 5.8889\n",
      "Epoch: 24/200 | Train loss: 5.6497 | Val loss: 5.3504\n",
      "Epoch: 25/200 | Train loss: 5.1193 | Val loss: 4.8251\n",
      "Epoch: 26/200 | Train loss: 4.7680 | Val loss: 4.3160\n",
      "Epoch: 27/200 | Train loss: 4.1416 | Val loss: 3.8287\n",
      "Epoch: 28/200 | Train loss: 3.6262 | Val loss: 3.3661\n",
      "Epoch: 29/200 | Train loss: 3.1760 | Val loss: 2.9323\n",
      "Epoch: 30/200 | Train loss: 2.7599 | Val loss: 2.5291\n",
      "Epoch: 31/200 | Train loss: 2.3713 | Val loss: 2.1608\n",
      "Epoch: 32/200 | Train loss: 2.0239 | Val loss: 1.8266\n",
      "Epoch: 33/200 | Train loss: 1.7052 | Val loss: 1.5308\n",
      "Epoch: 34/200 | Train loss: 1.4266 | Val loss: 1.2706\n",
      "Epoch: 35/200 | Train loss: 1.1821 | Val loss: 1.0469\n",
      "Epoch: 36/200 | Train loss: 3.1633 | Val loss: 0.8618\n",
      "Epoch: 37/200 | Train loss: 0.8052 | Val loss: 0.7050\n",
      "Epoch: 38/200 | Train loss: 0.8337 | Val loss: 0.5791\n",
      "Epoch: 39/200 | Train loss: 0.5493 | Val loss: 0.4789\n",
      "Epoch: 40/200 | Train loss: 0.4622 | Val loss: 0.4020\n",
      "Epoch: 41/200 | Train loss: 0.3918 | Val loss: 0.3448\n",
      "Epoch: 42/200 | Train loss: 0.3411 | Val loss: 0.3035\n",
      "Epoch: 43/200 | Train loss: 0.3056 | Val loss: 0.2744\n",
      "Epoch: 44/200 | Train loss: 0.2813 | Val loss: 0.2550\n",
      "Epoch: 45/200 | Train loss: 0.2641 | Val loss: 0.2424\n",
      "Epoch: 46/200 | Train loss: 0.2533 | Val loss: 0.2347\n",
      "Epoch: 47/200 | Train loss: 0.2468 | Val loss: 0.2301\n",
      "Epoch: 48/200 | Train loss: 0.2421 | Val loss: 0.2277\n",
      "Epoch: 49/200 | Train loss: 60.8314 | Val loss: 0.2287\n",
      "Epoch: 50/200 | Train loss: 200.2590 | Val loss: 0.2272\n",
      "Epoch: 51/200 | Train loss: 35.8419 | Val loss: 0.2286\n",
      "Epoch: 52/200 | Train loss: 2.3867 | Val loss: 0.2274\n",
      "Epoch: 53/200 | Train loss: 0.3429 | Val loss: 0.2264\n",
      "Epoch: 54/200 | Train loss: 0.2382 | Val loss: 0.2260\n",
      "Epoch: 55/200 | Train loss: 0.2381 | Val loss: 0.2258\n",
      "Epoch: 56/200 | Train loss: 0.2372 | Val loss: 0.2257\n",
      "Epoch: 57/200 | Train loss: 6.4353 | Val loss: 0.2260\n",
      "Epoch: 58/200 | Train loss: 0.4035 | Val loss: 0.2257\n",
      "Epoch: 59/200 | Train loss: 7.5787 | Val loss: 0.2256\n",
      "Epoch: 60/200 | Train loss: 0.2650 | Val loss: 0.2256\n",
      "Epoch: 61/200 | Train loss: 0.2434 | Val loss: 0.2256\n",
      "Epoch: 62/200 | Train loss: 0.2384 | Val loss: 0.2256\n",
      "Epoch: 63/200 | Train loss: 0.2385 | Val loss: 0.2256\n",
      "Epoch: 64/200 | Train loss: 0.2380 | Val loss: 0.2256\n",
      "Epoch: 65/200 | Train loss: 0.2380 | Val loss: 0.2257\n",
      "Epoch: 66/200 | Train loss: 0.2378 | Val loss: 0.2257\n",
      "Epoch: 67/200 | Train loss: 0.2378 | Val loss: 0.2257\n",
      "Epoch: 68/200 | Train loss: 2.5548 | Val loss: 0.2256\n",
      "Epoch: 69/200 | Train loss: 3.2542 | Val loss: 0.2262\n",
      "Early stopping!\n",
      "Trained in 69 epochs with best val loss: 0.2256\n",
      "Test loss: 0.22578714420829993\n",
      "\n",
      "\n",
      "---Combination 25---\n",
      "-Hidden size: 256\n",
      "-Batch size: 32\n",
      "-Dropout: 0.2\n",
      "-Depth: 3\n",
      "-Learning rate: 0.001\n",
      "\n",
      "Epoch: 1/200 | Train loss: 0.4990 | Val loss: 0.0755\n",
      "Epoch: 2/200 | Train loss: 0.1114 | Val loss: 0.0251\n",
      "Epoch: 3/200 | Train loss: 0.0761 | Val loss: 0.0130\n",
      "Epoch: 4/200 | Train loss: 0.0605 | Val loss: 0.0131\n",
      "Epoch: 5/200 | Train loss: 0.0539 | Val loss: 0.0405\n",
      "Epoch: 6/200 | Train loss: 0.0534 | Val loss: 0.0101\n",
      "Epoch: 7/200 | Train loss: 0.0502 | Val loss: 0.0138\n",
      "Epoch: 8/200 | Train loss: 0.0457 | Val loss: 0.0175\n",
      "Epoch: 9/200 | Train loss: 0.0442 | Val loss: 0.0105\n",
      "Epoch: 10/200 | Train loss: 0.0415 | Val loss: 0.0098\n",
      "Epoch: 11/200 | Train loss: 0.0402 | Val loss: 0.0126\n",
      "Epoch: 12/200 | Train loss: 0.0361 | Val loss: 0.0095\n",
      "Epoch: 13/200 | Train loss: 0.0381 | Val loss: 0.0160\n",
      "Epoch: 14/200 | Train loss: 0.0377 | Val loss: 0.0145\n",
      "Epoch: 15/200 | Train loss: 0.0366 | Val loss: 0.0189\n",
      "Epoch: 16/200 | Train loss: 0.0333 | Val loss: 0.0102\n",
      "Epoch: 17/200 | Train loss: 0.0334 | Val loss: 0.0205\n",
      "Epoch: 18/200 | Train loss: 0.0341 | Val loss: 0.0128\n",
      "Epoch: 19/200 | Train loss: 0.0318 | Val loss: 0.0076\n",
      "Epoch: 20/200 | Train loss: 0.0301 | Val loss: 0.0186\n",
      "Epoch: 21/200 | Train loss: 0.0306 | Val loss: 0.0085\n",
      "Epoch: 22/200 | Train loss: 0.0296 | Val loss: 0.0125\n",
      "Epoch: 23/200 | Train loss: 0.0295 | Val loss: 0.0106\n",
      "Epoch: 24/200 | Train loss: 0.0281 | Val loss: 0.0076\n",
      "Epoch: 25/200 | Train loss: 0.0281 | Val loss: 0.0156\n",
      "Epoch: 26/200 | Train loss: 0.0272 | Val loss: 0.0114\n",
      "Epoch: 27/200 | Train loss: 0.0264 | Val loss: 0.0114\n",
      "Epoch: 28/200 | Train loss: 0.0261 | Val loss: 0.0138\n",
      "Epoch: 29/200 | Train loss: 0.0255 | Val loss: 0.0103\n",
      "Early stopping!\n",
      "Trained in 29 epochs with best val loss: 0.0076\n",
      "Test loss: 0.01006170936414729\n",
      "\n",
      "\n",
      "---Combination 26---\n",
      "-Hidden size: 256\n",
      "-Batch size: 32\n",
      "-Dropout: 0.2\n",
      "-Depth: 3\n",
      "-Learning rate: 0.1\n",
      "\n",
      "Epoch: 1/200 | Train loss: 7200577.3548 | Val loss: 13.5169\n",
      "Epoch: 2/200 | Train loss: 29.7141 | Val loss: 13.0871\n",
      "Epoch: 3/200 | Train loss: 12.9319 | Val loss: 12.5019\n",
      "Epoch: 4/200 | Train loss: 12.4592 | Val loss: 11.7822\n",
      "Epoch: 5/200 | Train loss: 11.4397 | Val loss: 10.9454\n",
      "Epoch: 6/200 | Train loss: 10.5487 | Val loss: 10.0071\n",
      "Epoch: 7/200 | Train loss: 9.5651 | Val loss: 8.9869\n",
      "Epoch: 8/200 | Train loss: 8.5108 | Val loss: 7.9077\n",
      "Epoch: 9/200 | Train loss: 7.4119 | Val loss: 6.7998\n",
      "Epoch: 10/200 | Train loss: 7.0714 | Val loss: 5.6985\n",
      "Epoch: 11/200 | Train loss: 5.2111 | Val loss: 4.6321\n",
      "Epoch: 12/200 | Train loss: 4.1930 | Val loss: 3.6413\n",
      "Epoch: 13/200 | Train loss: 3.2309 | Val loss: 2.7581\n",
      "Epoch: 14/200 | Train loss: 2.4084 | Val loss: 2.0083\n",
      "Epoch: 15/200 | Train loss: 1.7274 | Val loss: 1.4068\n",
      "Epoch: 16/200 | Train loss: 1.1977 | Val loss: 0.9533\n",
      "Epoch: 17/200 | Train loss: 0.8098 | Val loss: 0.6382\n",
      "Epoch: 18/200 | Train loss: 0.5509 | Val loss: 0.4378\n",
      "Epoch: 19/200 | Train loss: 0.3951 | Val loss: 0.3220\n",
      "Epoch: 20/200 | Train loss: 0.3054 | Val loss: 0.2637\n",
      "Epoch: 21/200 | Train loss: 103.9808 | Val loss: 0.2256\n",
      "Epoch: 22/200 | Train loss: 282.1953 | Val loss: 0.2378\n",
      "Epoch: 23/200 | Train loss: 0.2434 | Val loss: 0.2290\n",
      "Epoch: 24/200 | Train loss: 0.2387 | Val loss: 0.2264\n",
      "Epoch: 25/200 | Train loss: 0.2398 | Val loss: 0.2258\n",
      "Epoch: 26/200 | Train loss: 10.6518 | Val loss: 0.2256\n",
      "Epoch: 27/200 | Train loss: 0.2375 | Val loss: 0.2256\n",
      "Epoch: 28/200 | Train loss: 0.2374 | Val loss: 0.2256\n",
      "Epoch: 29/200 | Train loss: 0.2375 | Val loss: 0.2256\n",
      "Epoch: 30/200 | Train loss: 0.2375 | Val loss: 0.2256\n",
      "Epoch: 31/200 | Train loss: 0.2375 | Val loss: 0.2256\n",
      "Epoch: 32/200 | Train loss: 0.2375 | Val loss: 0.2256\n",
      "Epoch: 33/200 | Train loss: 0.2386 | Val loss: 0.2256\n",
      "Epoch: 34/200 | Train loss: 163534.7740 | Val loss: 0.6256\n",
      "Epoch: 35/200 | Train loss: 0.4591 | Val loss: 0.3175\n",
      "Epoch: 36/200 | Train loss: 0.2848 | Val loss: 0.2410\n",
      "Epoch: 37/200 | Train loss: 0.2452 | Val loss: 0.2273\n",
      "Epoch: 38/200 | Train loss: 0.2384 | Val loss: 0.2256\n",
      "Epoch: 39/200 | Train loss: 0.2375 | Val loss: 0.2256\n",
      "Epoch: 40/200 | Train loss: 0.2375 | Val loss: 0.2257\n",
      "Epoch: 41/200 | Train loss: 0.2375 | Val loss: 0.2256\n",
      "Epoch: 42/200 | Train loss: 0.2375 | Val loss: 0.2256\n",
      "Early stopping!\n",
      "Trained in 42 epochs with best val loss: 0.2256\n",
      "Test loss: 0.2255781252315079\n",
      "\n",
      "\n",
      "---Combination 27---\n",
      "-Hidden size: 256\n",
      "-Batch size: 32\n",
      "-Dropout: 0.2\n",
      "-Depth: 4\n",
      "-Learning rate: 0.001\n",
      "\n",
      "Epoch: 1/200 | Train loss: 0.5372 | Val loss: 0.0296\n",
      "Epoch: 2/200 | Train loss: 0.0965 | Val loss: 0.0194\n",
      "Epoch: 3/200 | Train loss: 0.0728 | Val loss: 0.0132\n",
      "Epoch: 4/200 | Train loss: 0.0597 | Val loss: 0.0325\n",
      "Epoch: 5/200 | Train loss: 0.0573 | Val loss: 0.0137\n",
      "Epoch: 6/200 | Train loss: 0.0536 | Val loss: 0.0140\n",
      "Epoch: 7/200 | Train loss: 0.0494 | Val loss: 0.0255\n",
      "Epoch: 8/200 | Train loss: 0.0490 | Val loss: 0.0093\n",
      "Epoch: 9/200 | Train loss: 0.0462 | Val loss: 0.0129\n",
      "Epoch: 10/200 | Train loss: 0.0425 | Val loss: 0.0112\n",
      "Epoch: 11/200 | Train loss: 0.0409 | Val loss: 0.0116\n",
      "Epoch: 12/200 | Train loss: 0.0394 | Val loss: 0.0154\n",
      "Epoch: 13/200 | Train loss: 0.0374 | Val loss: 0.0080\n",
      "Epoch: 14/200 | Train loss: 0.0364 | Val loss: 0.0115\n",
      "Epoch: 15/200 | Train loss: 0.0350 | Val loss: 0.0140\n",
      "Epoch: 16/200 | Train loss: 0.0341 | Val loss: 0.0092\n",
      "Epoch: 17/200 | Train loss: 0.0320 | Val loss: 0.0092\n",
      "Epoch: 18/200 | Train loss: 0.0314 | Val loss: 0.0149\n",
      "Epoch: 19/200 | Train loss: 0.0300 | Val loss: 0.0165\n",
      "Epoch: 20/200 | Train loss: 0.0280 | Val loss: 0.0121\n",
      "Epoch: 21/200 | Train loss: 0.0309 | Val loss: 0.0099\n",
      "Epoch: 22/200 | Train loss: 0.0293 | Val loss: 0.0197\n",
      "Epoch: 23/200 | Train loss: 0.0283 | Val loss: 0.0153\n",
      "Early stopping!\n",
      "Trained in 23 epochs with best val loss: 0.0080\n",
      "Test loss: 0.015507854856010797\n",
      "\n",
      "\n",
      "---Combination 28---\n",
      "-Hidden size: 256\n",
      "-Batch size: 32\n",
      "-Dropout: 0.2\n",
      "-Depth: 4\n",
      "-Learning rate: 0.1\n",
      "\n",
      "Epoch: 1/200 | Train loss: 1200447160.2539 | Val loss: 14.1779\n",
      "Epoch: 2/200 | Train loss: 14.5729 | Val loss: 14.1414\n",
      "Epoch: 3/200 | Train loss: 14.2055 | Val loss: 14.0902\n",
      "Epoch: 4/200 | Train loss: 14.1886 | Val loss: 14.0240\n",
      "Epoch: 5/200 | Train loss: 14.0560 | Val loss: 13.9422\n",
      "Epoch: 6/200 | Train loss: 14.3867 | Val loss: 13.8433\n",
      "Epoch: 7/200 | Train loss: 13.8551 | Val loss: 13.7250\n",
      "Epoch: 8/200 | Train loss: 13.7264 | Val loss: 13.5853\n",
      "Epoch: 9/200 | Train loss: 13.5735 | Val loss: 13.4217\n",
      "Epoch: 10/200 | Train loss: 21.5164 | Val loss: 13.2291\n",
      "Epoch: 11/200 | Train loss: 26.4604 | Val loss: 13.0060\n",
      "Epoch: 12/200 | Train loss: 12.9503 | Val loss: 12.7516\n",
      "Epoch: 13/200 | Train loss: 12.6770 | Val loss: 12.4595\n",
      "Epoch: 14/200 | Train loss: 12.3640 | Val loss: 12.1262\n",
      "Epoch: 15/200 | Train loss: 12.0074 | Val loss: 11.7473\n",
      "Epoch: 16/200 | Train loss: 11.6034 | Val loss: 11.3191\n",
      "Epoch: 17/200 | Train loss: 11.1484 | Val loss: 10.8385\n",
      "Epoch: 18/200 | Train loss: 10.6393 | Val loss: 10.3029\n",
      "Epoch: 19/200 | Train loss: 10.0743 | Val loss: 9.7108\n",
      "Epoch: 20/200 | Train loss: 9.4525 | Val loss: 9.0627\n",
      "Epoch: 21/200 | Train loss: 8.7762 | Val loss: 8.3604\n",
      "Epoch: 22/200 | Train loss: 8.0467 | Val loss: 7.6103\n",
      "Epoch: 23/200 | Train loss: 7.3153 | Val loss: 6.8212\n",
      "Epoch: 24/200 | Train loss: 6.4674 | Val loss: 6.0039\n",
      "Epoch: 25/200 | Train loss: 5.6409 | Val loss: 5.1764\n",
      "Epoch: 26/200 | Train loss: 4.8131 | Val loss: 4.3579\n",
      "Epoch: 27/200 | Train loss: 4.0049 | Val loss: 3.5702\n",
      "Epoch: 28/200 | Train loss: 3.2395 | Val loss: 2.8381\n",
      "Epoch: 29/200 | Train loss: 2.5398 | Val loss: 2.1818\n",
      "Epoch: 30/200 | Train loss: 1.9262 | Val loss: 1.6203\n",
      "Epoch: 31/200 | Train loss: 1.4133 | Val loss: 1.1641\n",
      "Epoch: 32/200 | Train loss: 1.0076 | Val loss: 0.8155\n",
      "Epoch: 33/200 | Train loss: 0.7073 | Val loss: 0.5681\n",
      "Epoch: 34/200 | Train loss: 0.5011 | Val loss: 0.4064\n",
      "Epoch: 35/200 | Train loss: 0.3716 | Val loss: 0.3105\n",
      "Epoch: 36/200 | Train loss: 0.2983 | Val loss: 0.2604\n",
      "Epoch: 37/200 | Train loss: 0.2616 | Val loss: 0.2376\n",
      "Epoch: 38/200 | Train loss: 0.2457 | Val loss: 0.2288\n",
      "Epoch: 39/200 | Train loss: 0.2398 | Val loss: 0.2262\n",
      "Epoch: 40/200 | Train loss: 0.2380 | Val loss: 0.2256\n",
      "Epoch: 41/200 | Train loss: 0.2375 | Val loss: 0.2256\n",
      "Epoch: 42/200 | Train loss: 0.2375 | Val loss: 0.2256\n",
      "Epoch: 43/200 | Train loss: 0.2375 | Val loss: 0.2256\n",
      "Epoch: 44/200 | Train loss: 0.2374 | Val loss: 0.2257\n",
      "Epoch: 45/200 | Train loss: 0.2375 | Val loss: 0.2257\n",
      "Epoch: 46/200 | Train loss: 0.2375 | Val loss: 0.2257\n",
      "Epoch: 47/200 | Train loss: 0.2375 | Val loss: 0.2256\n",
      "Epoch: 48/200 | Train loss: 0.2375 | Val loss: 0.2256\n",
      "Epoch: 49/200 | Train loss: 0.2374 | Val loss: 0.2256\n",
      "Epoch: 50/200 | Train loss: 0.2375 | Val loss: 0.2256\n",
      "Epoch: 51/200 | Train loss: 0.2374 | Val loss: 0.2258\n",
      "Epoch: 52/200 | Train loss: 0.2376 | Val loss: 0.2256\n",
      "Epoch: 53/200 | Train loss: 0.2679 | Val loss: 0.2256\n",
      "Epoch: 54/200 | Train loss: 0.2732 | Val loss: 0.2258\n",
      "Epoch: 55/200 | Train loss: 0.2376 | Val loss: 0.2259\n",
      "Epoch: 56/200 | Train loss: 0.2375 | Val loss: 0.2258\n",
      "Epoch: 57/200 | Train loss: 0.2375 | Val loss: 0.2258\n",
      "Epoch: 58/200 | Train loss: 15617.4969 | Val loss: 2.3355\n",
      "Epoch: 59/200 | Train loss: 0.9053 | Val loss: 0.3038\n",
      "Early stopping!\n",
      "Trained in 59 epochs with best val loss: 0.2256\n",
      "Test loss: 0.3090138381805973\n",
      "\n",
      "\n",
      "---Combination 29---\n",
      "-Hidden size: 256\n",
      "-Batch size: 32\n",
      "-Dropout: 0.2\n",
      "-Depth: 5\n",
      "-Learning rate: 0.001\n",
      "\n",
      "Epoch: 1/200 | Train loss: 0.4375 | Val loss: 0.0315\n",
      "Epoch: 2/200 | Train loss: 0.0880 | Val loss: 0.0161\n",
      "Epoch: 3/200 | Train loss: 0.0748 | Val loss: 0.0169\n",
      "Epoch: 4/200 | Train loss: 0.0613 | Val loss: 0.0111\n",
      "Epoch: 5/200 | Train loss: 0.0542 | Val loss: 0.0168\n",
      "Epoch: 6/200 | Train loss: 0.0549 | Val loss: 0.0484\n",
      "Epoch: 7/200 | Train loss: 0.0475 | Val loss: 0.0094\n",
      "Epoch: 8/200 | Train loss: 0.0467 | Val loss: 0.0109\n",
      "Epoch: 9/200 | Train loss: 0.0418 | Val loss: 0.0170\n",
      "Epoch: 10/200 | Train loss: 0.0427 | Val loss: 0.0180\n",
      "Epoch: 11/200 | Train loss: 0.0415 | Val loss: 0.0099\n",
      "Epoch: 12/200 | Train loss: 0.0377 | Val loss: 0.0176\n",
      "Epoch: 13/200 | Train loss: 0.0371 | Val loss: 0.0091\n",
      "Epoch: 14/200 | Train loss: 0.0351 | Val loss: 0.0116\n",
      "Epoch: 15/200 | Train loss: 0.0336 | Val loss: 0.0186\n",
      "Epoch: 16/200 | Train loss: 0.0337 | Val loss: 0.0238\n",
      "Epoch: 17/200 | Train loss: 0.0309 | Val loss: 0.0107\n",
      "Epoch: 18/200 | Train loss: 0.0320 | Val loss: 0.0210\n",
      "Epoch: 19/200 | Train loss: 0.0314 | Val loss: 0.0102\n",
      "Epoch: 20/200 | Train loss: 0.0291 | Val loss: 0.0120\n",
      "Epoch: 21/200 | Train loss: 0.0288 | Val loss: 0.0100\n",
      "Epoch: 22/200 | Train loss: 0.0289 | Val loss: 0.0107\n",
      "Epoch: 23/200 | Train loss: 0.0288 | Val loss: 0.0114\n",
      "Early stopping!\n",
      "Trained in 23 epochs with best val loss: 0.0091\n",
      "Test loss: 0.011027049813149632\n",
      "\n",
      "\n",
      "---Combination 30---\n",
      "-Hidden size: 256\n",
      "-Batch size: 32\n",
      "-Dropout: 0.2\n",
      "-Depth: 5\n",
      "-Learning rate: 0.1\n",
      "\n",
      "Epoch: 1/200 | Train loss: 32054302007.0054 | Val loss: 14.0230\n",
      "Epoch: 2/200 | Train loss: 171507631.2143 | Val loss: 13.8604\n",
      "Epoch: 3/200 | Train loss: 44.4916 | Val loss: 13.8508\n",
      "Epoch: 4/200 | Train loss: 23.4762 | Val loss: 13.8385\n",
      "Epoch: 5/200 | Train loss: 19.5464 | Val loss: 13.8232\n",
      "Epoch: 6/200 | Train loss: 157.4351 | Val loss: 13.8055\n",
      "Epoch: 7/200 | Train loss: 48177.5659 | Val loss: 13.7806\n",
      "Epoch: 8/200 | Train loss: 23.8045 | Val loss: 13.7542\n",
      "Epoch: 9/200 | Train loss: 585.5255 | Val loss: 13.7214\n",
      "Epoch: 10/200 | Train loss: 109.8014 | Val loss: 13.6808\n",
      "Epoch: 11/200 | Train loss: 28.3889 | Val loss: 13.6374\n",
      "Epoch: 12/200 | Train loss: 13.6815 | Val loss: 13.5863\n",
      "Epoch: 13/200 | Train loss: 13.8615 | Val loss: 13.5268\n",
      "Epoch: 14/200 | Train loss: 15.2884 | Val loss: 13.4577\n",
      "Epoch: 15/200 | Train loss: 13.5031 | Val loss: 13.3768\n",
      "Epoch: 16/200 | Train loss: 13.4001 | Val loss: 13.2828\n",
      "Epoch: 17/200 | Train loss: 13.2986 | Val loss: 13.1737\n",
      "Epoch: 18/200 | Train loss: 13.1807 | Val loss: 13.0472\n",
      "Epoch: 19/200 | Train loss: 13.0444 | Val loss: 12.9009\n",
      "Epoch: 20/200 | Train loss: 12.8868 | Val loss: 12.7321\n",
      "Epoch: 21/200 | Train loss: 17.4233 | Val loss: 12.5375\n",
      "Epoch: 22/200 | Train loss: 13.4649 | Val loss: 12.3140\n",
      "Epoch: 23/200 | Train loss: 12.2559 | Val loss: 12.0579\n",
      "Epoch: 24/200 | Train loss: 11.9815 | Val loss: 11.7657\n",
      "Epoch: 25/200 | Train loss: 25.6863 | Val loss: 11.4349\n",
      "Epoch: 26/200 | Train loss: 11.3157 | Val loss: 11.0586\n",
      "Epoch: 27/200 | Train loss: 19.6625 | Val loss: 10.6356\n",
      "Epoch: 28/200 | Train loss: 31.1462 | Val loss: 10.1541\n",
      "Epoch: 29/200 | Train loss: 10.5257 | Val loss: 9.6289\n",
      "Epoch: 30/200 | Train loss: 9.4041 | Val loss: 9.0486\n",
      "Epoch: 31/200 | Train loss: 13.2627 | Val loss: 8.4219\n",
      "Epoch: 32/200 | Train loss: 8.1425 | Val loss: 7.7394\n",
      "Epoch: 33/200 | Train loss: 8.1652 | Val loss: 7.0121\n",
      "Epoch: 34/200 | Train loss: 10.2917 | Val loss: 6.2563\n",
      "Epoch: 35/200 | Train loss: 5.9175 | Val loss: 5.4741\n",
      "Epoch: 36/200 | Train loss: 5.1289 | Val loss: 4.6874\n",
      "Epoch: 37/200 | Train loss: 18.5192 | Val loss: 3.9327\n",
      "Epoch: 38/200 | Train loss: 3.6024 | Val loss: 3.1949\n",
      "Epoch: 39/200 | Train loss: 2.8887 | Val loss: 2.5161\n",
      "Epoch: 40/200 | Train loss: 2.2442 | Val loss: 1.9159\n",
      "Epoch: 41/200 | Train loss: 1.6865 | Val loss: 1.4099\n",
      "Epoch: 42/200 | Train loss: 1.2280 | Val loss: 1.0066\n",
      "Epoch: 43/200 | Train loss: 0.8727 | Val loss: 0.7052\n",
      "Epoch: 44/200 | Train loss: 0.6153 | Val loss: 0.4960\n",
      "Epoch: 45/200 | Train loss: 0.4432 | Val loss: 0.3634\n",
      "Epoch: 46/200 | Train loss: 283.0307 | Val loss: 0.2597\n",
      "Epoch: 47/200 | Train loss: 2.0347 | Val loss: 0.2370\n",
      "Epoch: 48/200 | Train loss: 0.2457 | Val loss: 0.2291\n",
      "Epoch: 49/200 | Train loss: 0.2401 | Val loss: 0.2264\n",
      "Epoch: 50/200 | Train loss: 0.6972 | Val loss: 0.2258\n",
      "Epoch: 51/200 | Train loss: 0.2376 | Val loss: 0.2256\n",
      "Epoch: 52/200 | Train loss: 0.2375 | Val loss: 0.2256\n",
      "Epoch: 53/200 | Train loss: 0.2374 | Val loss: 0.2257\n",
      "Epoch: 54/200 | Train loss: 437.3501 | Val loss: 0.2460\n",
      "Epoch: 55/200 | Train loss: 0.2446 | Val loss: 0.2262\n",
      "Epoch: 56/200 | Train loss: 13.1500 | Val loss: 0.2274\n",
      "Epoch: 57/200 | Train loss: 0.2382 | Val loss: 0.2256\n",
      "Epoch: 58/200 | Train loss: 0.2375 | Val loss: 0.2256\n",
      "Epoch: 59/200 | Train loss: 0.2440 | Val loss: 0.2256\n",
      "Epoch: 60/200 | Train loss: 8.8805 | Val loss: 0.2257\n",
      "Epoch: 61/200 | Train loss: 0.2374 | Val loss: 0.2258\n",
      "Early stopping!\n",
      "Trained in 61 epochs with best val loss: 0.2256\n",
      "Test loss: 0.22559061050415039\n",
      "\n",
      "\n",
      "---Combination 31---\n",
      "-Hidden size: 256\n",
      "-Batch size: 32\n",
      "-Dropout: 0.3\n",
      "-Depth: 3\n",
      "-Learning rate: 0.001\n",
      "\n",
      "Epoch: 1/200 | Train loss: 0.5714 | Val loss: 0.0855\n",
      "Epoch: 2/200 | Train loss: 0.1354 | Val loss: 0.0217\n",
      "Epoch: 3/200 | Train loss: 0.0943 | Val loss: 0.0137\n",
      "Epoch: 4/200 | Train loss: 0.0854 | Val loss: 0.0350\n",
      "Epoch: 5/200 | Train loss: 0.0758 | Val loss: 0.0158\n",
      "Epoch: 6/200 | Train loss: 0.0709 | Val loss: 0.0127\n",
      "Epoch: 7/200 | Train loss: 0.0663 | Val loss: 0.0238\n",
      "Epoch: 8/200 | Train loss: 0.0622 | Val loss: 0.0112\n",
      "Epoch: 9/200 | Train loss: 0.0568 | Val loss: 0.0255\n",
      "Epoch: 10/200 | Train loss: 0.0580 | Val loss: 0.0168\n",
      "Epoch: 11/200 | Train loss: 0.0536 | Val loss: 0.0131\n",
      "Epoch: 12/200 | Train loss: 0.0541 | Val loss: 0.0212\n",
      "Epoch: 13/200 | Train loss: 0.0518 | Val loss: 0.0123\n",
      "Epoch: 14/200 | Train loss: 0.0483 | Val loss: 0.0242\n",
      "Epoch: 15/200 | Train loss: 0.0491 | Val loss: 0.0150\n",
      "Epoch: 16/200 | Train loss: 0.0479 | Val loss: 0.0171\n",
      "Epoch: 17/200 | Train loss: 0.0462 | Val loss: 0.0167\n",
      "Epoch: 18/200 | Train loss: 0.0437 | Val loss: 0.0194\n",
      "Early stopping!\n",
      "Trained in 18 epochs with best val loss: 0.0112\n",
      "Test loss: 0.019641529838891995\n",
      "\n",
      "\n",
      "---Combination 32---\n",
      "-Hidden size: 256\n",
      "-Batch size: 32\n",
      "-Dropout: 0.3\n",
      "-Depth: 3\n",
      "-Learning rate: 0.1\n",
      "\n",
      "Epoch: 1/200 | Train loss: 22395932.8965 | Val loss: 11.3974\n",
      "Epoch: 2/200 | Train loss: 5.8726 | Val loss: 1.5758\n",
      "Epoch: 3/200 | Train loss: 3.2601 | Val loss: 1.9459\n",
      "Epoch: 4/200 | Train loss: 10.0812 | Val loss: 2.2601\n",
      "Epoch: 5/200 | Train loss: 12.0466 | Val loss: 3.9686\n",
      "Epoch: 6/200 | Train loss: 4.6918 | Val loss: 3.9106\n",
      "Epoch: 7/200 | Train loss: 3.7751 | Val loss: 3.9336\n",
      "Epoch: 8/200 | Train loss: 3.5960 | Val loss: 3.3976\n",
      "Epoch: 9/200 | Train loss: 3.5281 | Val loss: 2.7460\n",
      "Epoch: 10/200 | Train loss: 3.4332 | Val loss: 2.5733\n",
      "Epoch: 11/200 | Train loss: 3.2062 | Val loss: 2.6956\n",
      "Epoch: 12/200 | Train loss: 3.1487 | Val loss: 2.4134\n",
      "Early stopping!\n",
      "Trained in 12 epochs with best val loss: 1.5758\n",
      "Test loss: 2.4408858465111773\n",
      "\n",
      "\n",
      "---Combination 33---\n",
      "-Hidden size: 256\n",
      "-Batch size: 32\n",
      "-Dropout: 0.3\n",
      "-Depth: 4\n",
      "-Learning rate: 0.001\n",
      "\n",
      "Epoch: 1/200 | Train loss: 0.5798 | Val loss: 0.0514\n",
      "Epoch: 2/200 | Train loss: 0.1255 | Val loss: 0.0609\n",
      "Epoch: 3/200 | Train loss: 0.0995 | Val loss: 0.0272\n",
      "Epoch: 4/200 | Train loss: 0.0879 | Val loss: 0.0269\n",
      "Epoch: 5/200 | Train loss: 0.0822 | Val loss: 0.0204\n",
      "Epoch: 6/200 | Train loss: 0.0794 | Val loss: 0.0272\n",
      "Epoch: 7/200 | Train loss: 0.0706 | Val loss: 0.0319\n",
      "Epoch: 8/200 | Train loss: 0.0666 | Val loss: 0.0136\n",
      "Epoch: 9/200 | Train loss: 0.0629 | Val loss: 0.0258\n",
      "Epoch: 10/200 | Train loss: 0.0623 | Val loss: 0.0177\n",
      "Epoch: 11/200 | Train loss: 0.0568 | Val loss: 0.0257\n",
      "Epoch: 12/200 | Train loss: 0.0539 | Val loss: 0.0150\n",
      "Epoch: 13/200 | Train loss: 0.0518 | Val loss: 0.0153\n",
      "Epoch: 14/200 | Train loss: 0.0499 | Val loss: 0.0125\n",
      "Epoch: 15/200 | Train loss: 0.0475 | Val loss: 0.0249\n",
      "Epoch: 16/200 | Train loss: 0.0455 | Val loss: 0.0138\n",
      "Epoch: 17/200 | Train loss: 0.0436 | Val loss: 0.0146\n",
      "Epoch: 18/200 | Train loss: 0.0444 | Val loss: 0.0139\n",
      "Epoch: 19/200 | Train loss: 0.0425 | Val loss: 0.0237\n",
      "Epoch: 20/200 | Train loss: 0.0410 | Val loss: 0.0208\n",
      "Epoch: 21/200 | Train loss: 0.0412 | Val loss: 0.0161\n",
      "Epoch: 22/200 | Train loss: 0.0388 | Val loss: 0.0094\n",
      "Epoch: 23/200 | Train loss: 0.0385 | Val loss: 0.0139\n",
      "Epoch: 24/200 | Train loss: 0.0369 | Val loss: 0.0235\n",
      "Epoch: 25/200 | Train loss: 0.0345 | Val loss: 0.0150\n",
      "Epoch: 26/200 | Train loss: 0.0354 | Val loss: 0.0186\n",
      "Epoch: 27/200 | Train loss: 0.0335 | Val loss: 0.0145\n",
      "Epoch: 28/200 | Train loss: 0.0330 | Val loss: 0.0135\n",
      "Epoch: 29/200 | Train loss: 0.0313 | Val loss: 0.0114\n",
      "Epoch: 30/200 | Train loss: 0.0307 | Val loss: 0.0114\n",
      "Epoch: 31/200 | Train loss: 0.0307 | Val loss: 0.0153\n",
      "Epoch: 32/200 | Train loss: 0.0292 | Val loss: 0.0192\n",
      "Early stopping!\n",
      "Trained in 32 epochs with best val loss: 0.0094\n",
      "Test loss: 0.01914826205027276\n",
      "\n",
      "\n",
      "---Combination 34---\n",
      "-Hidden size: 256\n",
      "-Batch size: 32\n",
      "-Dropout: 0.3\n",
      "-Depth: 4\n",
      "-Learning rate: 0.1\n",
      "\n",
      "Epoch: 1/200 | Train loss: 212726557.9613 | Val loss: 13.5693\n",
      "Epoch: 2/200 | Train loss: 10512.4195 | Val loss: 13.4596\n",
      "Epoch: 3/200 | Train loss: 22.5311 | Val loss: 13.3437\n",
      "Epoch: 4/200 | Train loss: 13.5812 | Val loss: 13.1936\n",
      "Epoch: 5/200 | Train loss: 26.7749 | Val loss: 13.0094\n",
      "Epoch: 6/200 | Train loss: 13.0823 | Val loss: 12.7874\n",
      "Epoch: 7/200 | Train loss: 559.2497 | Val loss: 12.5429\n",
      "Epoch: 8/200 | Train loss: 15.6951 | Val loss: 12.2373\n",
      "Epoch: 9/200 | Train loss: 16.1039 | Val loss: 11.8820\n",
      "Epoch: 10/200 | Train loss: 20.1154 | Val loss: 11.4794\n",
      "Epoch: 11/200 | Train loss: 11.5353 | Val loss: 11.0221\n",
      "Epoch: 12/200 | Train loss: 13.5699 | Val loss: 10.5061\n",
      "Epoch: 13/200 | Train loss: 10.3015 | Val loss: 9.9354\n",
      "Epoch: 14/200 | Train loss: 9.6872 | Val loss: 9.3072\n",
      "Epoch: 15/200 | Train loss: 13.0118 | Val loss: 8.6189\n",
      "Epoch: 16/200 | Train loss: 8.3134 | Val loss: 7.8832\n",
      "Epoch: 17/200 | Train loss: 7.6125 | Val loss: 7.1052\n",
      "Epoch: 18/200 | Train loss: 6.8387 | Val loss: 6.2952\n",
      "Epoch: 19/200 | Train loss: 5.9564 | Val loss: 5.4663\n",
      "Epoch: 20/200 | Train loss: 5.0994 | Val loss: 4.6395\n",
      "Epoch: 21/200 | Train loss: 4.3914 | Val loss: 3.8380\n",
      "Epoch: 22/200 | Train loss: 3.4974 | Val loss: 3.0817\n",
      "Epoch: 23/200 | Train loss: 2.7706 | Val loss: 2.3956\n",
      "Epoch: 24/200 | Train loss: 57.7147 | Val loss: 1.7703\n",
      "Epoch: 25/200 | Train loss: 1.5518 | Val loss: 1.2859\n",
      "Epoch: 26/200 | Train loss: 1.1160 | Val loss: 0.9076\n",
      "Epoch: 27/200 | Train loss: 3.6771 | Val loss: 0.6281\n",
      "Epoch: 28/200 | Train loss: 0.5510 | Val loss: 0.4450\n",
      "Epoch: 29/200 | Train loss: 0.4025 | Val loss: 0.3331\n",
      "Epoch: 30/200 | Train loss: 0.3155 | Val loss: 0.2717\n",
      "Epoch: 31/200 | Train loss: 1.6089 | Val loss: 0.2437\n",
      "Epoch: 32/200 | Train loss: 0.2501 | Val loss: 0.2311\n",
      "Epoch: 33/200 | Train loss: 0.8353 | Val loss: 0.2265\n",
      "Epoch: 34/200 | Train loss: 0.2390 | Val loss: 0.2257\n",
      "Epoch: 35/200 | Train loss: 0.2395 | Val loss: 0.2256\n",
      "Epoch: 36/200 | Train loss: 0.2660 | Val loss: 0.2256\n",
      "Epoch: 37/200 | Train loss: 0.4258 | Val loss: 0.2258\n",
      "Epoch: 38/200 | Train loss: 1.9078 | Val loss: 0.2257\n",
      "Epoch: 39/200 | Train loss: 0.2374 | Val loss: 0.2257\n",
      "Epoch: 40/200 | Train loss: 0.2374 | Val loss: 0.2257\n",
      "Epoch: 41/200 | Train loss: 0.2378 | Val loss: 0.2257\n",
      "Epoch: 42/200 | Train loss: 0.8667 | Val loss: 0.2256\n",
      "Epoch: 43/200 | Train loss: 0.2668 | Val loss: 0.2257\n",
      "Epoch: 44/200 | Train loss: 0.2392 | Val loss: 0.2257\n",
      "Epoch: 45/200 | Train loss: 0.2375 | Val loss: 0.2257\n",
      "Early stopping!\n",
      "Trained in 45 epochs with best val loss: 0.2256\n",
      "Test loss: 0.22556122187255084\n",
      "\n",
      "\n",
      "---Combination 35---\n",
      "-Hidden size: 256\n",
      "-Batch size: 32\n",
      "-Dropout: 0.3\n",
      "-Depth: 5\n",
      "-Learning rate: 0.001\n",
      "\n",
      "Epoch: 1/200 | Train loss: 0.6125 | Val loss: 0.0394\n",
      "Epoch: 2/200 | Train loss: 0.1332 | Val loss: 0.0235\n",
      "Epoch: 3/200 | Train loss: 0.1024 | Val loss: 0.0231\n",
      "Epoch: 4/200 | Train loss: 0.0863 | Val loss: 0.0215\n",
      "Epoch: 5/200 | Train loss: 0.0810 | Val loss: 0.0188\n",
      "Epoch: 6/200 | Train loss: 0.0702 | Val loss: 0.0165\n",
      "Epoch: 7/200 | Train loss: 0.0642 | Val loss: 0.0153\n",
      "Epoch: 8/200 | Train loss: 0.0619 | Val loss: 0.0206\n",
      "Epoch: 9/200 | Train loss: 0.0564 | Val loss: 0.0123\n",
      "Epoch: 10/200 | Train loss: 0.0540 | Val loss: 0.0121\n",
      "Epoch: 11/200 | Train loss: 0.0507 | Val loss: 0.0125\n",
      "Epoch: 12/200 | Train loss: 0.0486 | Val loss: 0.0126\n",
      "Epoch: 13/200 | Train loss: 0.0474 | Val loss: 0.0161\n",
      "Epoch: 14/200 | Train loss: 0.0455 | Val loss: 0.0107\n",
      "Epoch: 15/200 | Train loss: 0.0445 | Val loss: 0.0204\n",
      "Epoch: 16/200 | Train loss: 0.0436 | Val loss: 0.0217\n",
      "Epoch: 17/200 | Train loss: 0.0411 | Val loss: 0.0223\n",
      "Epoch: 18/200 | Train loss: 0.0409 | Val loss: 0.0122\n",
      "Epoch: 19/200 | Train loss: 0.0411 | Val loss: 0.0203\n",
      "Epoch: 20/200 | Train loss: 0.0389 | Val loss: 0.0221\n",
      "Epoch: 21/200 | Train loss: 0.0384 | Val loss: 0.0199\n",
      "Epoch: 22/200 | Train loss: 0.0373 | Val loss: 0.0299\n",
      "Epoch: 23/200 | Train loss: 0.0363 | Val loss: 0.0222\n",
      "Epoch: 24/200 | Train loss: 0.0358 | Val loss: 0.0239\n",
      "Early stopping!\n",
      "Trained in 24 epochs with best val loss: 0.0107\n",
      "Test loss: 0.02349858295334422\n",
      "\n",
      "\n",
      "---Combination 36---\n",
      "-Hidden size: 256\n",
      "-Batch size: 32\n",
      "-Dropout: 0.3\n",
      "-Depth: 5\n",
      "-Learning rate: 0.1\n",
      "\n",
      "Epoch: 1/200 | Train loss: 5135522665.3510 | Val loss: 0.3384\n",
      "Epoch: 2/200 | Train loss: 49697.9021 | Val loss: 0.4778\n",
      "Epoch: 3/200 | Train loss: 83052947.5446 | Val loss: 61.0281\n",
      "Epoch: 4/200 | Train loss: 624.6425 | Val loss: 21.7801\n",
      "Epoch: 5/200 | Train loss: 14608.0179 | Val loss: 2.0199\n",
      "Epoch: 6/200 | Train loss: 75.7329 | Val loss: 0.2499\n",
      "Epoch: 7/200 | Train loss: 3418.3354 | Val loss: 0.2385\n",
      "Epoch: 8/200 | Train loss: 24.0667 | Val loss: 0.3611\n",
      "Epoch: 9/200 | Train loss: 603.1357 | Val loss: 0.2286\n",
      "Epoch: 10/200 | Train loss: 6.7123 | Val loss: 0.2682\n",
      "Epoch: 11/200 | Train loss: 4.3134 | Val loss: 0.2852\n",
      "Epoch: 12/200 | Train loss: 3.0381 | Val loss: 0.2958\n",
      "Epoch: 13/200 | Train loss: 91.1763 | Val loss: 0.3140\n",
      "Epoch: 14/200 | Train loss: 1.9944 | Val loss: 0.2754\n",
      "Epoch: 15/200 | Train loss: 1.7613 | Val loss: 0.2754\n",
      "Epoch: 16/200 | Train loss: 653.1962 | Val loss: 0.2294\n",
      "Epoch: 17/200 | Train loss: 1.6278 | Val loss: 0.2693\n",
      "Epoch: 18/200 | Train loss: 1.3325 | Val loss: 0.2515\n",
      "Epoch: 19/200 | Train loss: 1.1873 | Val loss: 0.2815\n",
      "Early stopping!\n",
      "Trained in 19 epochs with best val loss: 0.2286\n",
      "Test loss: 0.2859598073406496\n",
      "\n",
      "\n",
      "---Combination 37---\n",
      "-Hidden size: 256\n",
      "-Batch size: 64\n",
      "-Dropout: 0.2\n",
      "-Depth: 3\n",
      "-Learning rate: 0.001\n",
      "\n",
      "Epoch: 1/200 | Train loss: 0.6882 | Val loss: 0.0543\n",
      "Epoch: 2/200 | Train loss: 0.1305 | Val loss: 0.0451\n",
      "Epoch: 3/200 | Train loss: 0.0893 | Val loss: 0.0770\n",
      "Epoch: 4/200 | Train loss: 0.0742 | Val loss: 0.0247\n",
      "Epoch: 5/200 | Train loss: 0.0605 | Val loss: 0.0373\n",
      "Epoch: 6/200 | Train loss: 0.0608 | Val loss: 0.0267\n",
      "Epoch: 7/200 | Train loss: 0.0549 | Val loss: 0.0178\n",
      "Epoch: 8/200 | Train loss: 0.0509 | Val loss: 0.0126\n",
      "Epoch: 9/200 | Train loss: 0.0479 | Val loss: 0.0129\n",
      "Epoch: 10/200 | Train loss: 0.0438 | Val loss: 0.0227\n",
      "Epoch: 11/200 | Train loss: 0.0445 | Val loss: 0.0101\n",
      "Epoch: 12/200 | Train loss: 0.0438 | Val loss: 0.0134\n",
      "Epoch: 13/200 | Train loss: 0.0417 | Val loss: 0.0182\n",
      "Epoch: 14/200 | Train loss: 0.0377 | Val loss: 0.0088\n",
      "Epoch: 15/200 | Train loss: 0.0394 | Val loss: 0.0117\n",
      "Epoch: 16/200 | Train loss: 0.0364 | Val loss: 0.0092\n",
      "Epoch: 17/200 | Train loss: 0.0372 | Val loss: 0.0205\n",
      "Epoch: 18/200 | Train loss: 0.0351 | Val loss: 0.0091\n",
      "Epoch: 19/200 | Train loss: 0.0346 | Val loss: 0.0085\n",
      "Epoch: 20/200 | Train loss: 0.0323 | Val loss: 0.0076\n",
      "Epoch: 21/200 | Train loss: 0.0331 | Val loss: 0.0100\n",
      "Epoch: 22/200 | Train loss: 0.0321 | Val loss: 0.0113\n",
      "Epoch: 23/200 | Train loss: 0.0318 | Val loss: 0.0077\n",
      "Epoch: 24/200 | Train loss: 0.0308 | Val loss: 0.0091\n",
      "Epoch: 25/200 | Train loss: 0.0313 | Val loss: 0.0186\n",
      "Epoch: 26/200 | Train loss: 0.0300 | Val loss: 0.0097\n",
      "Epoch: 27/200 | Train loss: 0.0312 | Val loss: 0.0074\n",
      "Epoch: 28/200 | Train loss: 0.0290 | Val loss: 0.0110\n",
      "Epoch: 29/200 | Train loss: 0.0299 | Val loss: 0.0081\n",
      "Epoch: 30/200 | Train loss: 0.0284 | Val loss: 0.0087\n",
      "Epoch: 31/200 | Train loss: 0.0289 | Val loss: 0.0091\n",
      "Epoch: 32/200 | Train loss: 0.0280 | Val loss: 0.0096\n",
      "Epoch: 33/200 | Train loss: 0.0278 | Val loss: 0.0102\n",
      "Epoch: 34/200 | Train loss: 0.0301 | Val loss: 0.0080\n",
      "Epoch: 35/200 | Train loss: 0.0276 | Val loss: 0.0079\n",
      "Epoch: 36/200 | Train loss: 0.0266 | Val loss: 0.0089\n",
      "Epoch: 37/200 | Train loss: 0.0262 | Val loss: 0.0087\n",
      "Early stopping!\n",
      "Trained in 37 epochs with best val loss: 0.0074\n",
      "Test loss: 0.00866983150118503\n",
      "\n",
      "\n",
      "---Combination 38---\n",
      "-Hidden size: 256\n",
      "-Batch size: 64\n",
      "-Dropout: 0.2\n",
      "-Depth: 3\n",
      "-Learning rate: 0.1\n",
      "\n",
      "Epoch: 1/200 | Train loss: 307227487.9424 | Val loss: 13.7869\n",
      "Epoch: 2/200 | Train loss: 96.3394 | Val loss: 13.7556\n",
      "Epoch: 3/200 | Train loss: 34590.5781 | Val loss: 13.6890\n",
      "Epoch: 4/200 | Train loss: 508.5684 | Val loss: 13.6334\n",
      "Epoch: 5/200 | Train loss: 13098.3662 | Val loss: 13.5601\n",
      "Epoch: 6/200 | Train loss: 11.5975 | Val loss: 13.4995\n",
      "Epoch: 7/200 | Train loss: 11.4843 | Val loss: 13.4308\n",
      "Epoch: 8/200 | Train loss: 11.5145 | Val loss: 13.3549\n",
      "Epoch: 9/200 | Train loss: 11.9550 | Val loss: 13.2705\n",
      "Epoch: 10/200 | Train loss: 33.5057 | Val loss: 13.1818\n",
      "Epoch: 11/200 | Train loss: 11.2223 | Val loss: 13.0787\n",
      "Epoch: 12/200 | Train loss: 10.9908 | Val loss: 12.9672\n",
      "Epoch: 13/200 | Train loss: 10.9070 | Val loss: 12.8463\n",
      "Epoch: 14/200 | Train loss: 10.7249 | Val loss: 12.7157\n",
      "Epoch: 15/200 | Train loss: 10.6326 | Val loss: 12.5745\n",
      "Epoch: 16/200 | Train loss: 10.5486 | Val loss: 12.4218\n",
      "Epoch: 17/200 | Train loss: 10.3403 | Val loss: 12.2585\n",
      "Epoch: 18/200 | Train loss: 10.2373 | Val loss: 12.0827\n",
      "Epoch: 19/200 | Train loss: 10.0121 | Val loss: 11.8955\n",
      "Epoch: 20/200 | Train loss: 18.8828 | Val loss: 11.7126\n",
      "Epoch: 21/200 | Train loss: 9.6358 | Val loss: 11.5027\n",
      "Epoch: 22/200 | Train loss: 8.7790 | Val loss: 11.2942\n",
      "Epoch: 23/200 | Train loss: 9.8606 | Val loss: 0.3427\n",
      "Epoch: 24/200 | Train loss: 3.2479 | Val loss: 0.3754\n",
      "Epoch: 25/200 | Train loss: 3.1360 | Val loss: 0.5089\n",
      "Epoch: 26/200 | Train loss: 3.1157 | Val loss: 0.4109\n",
      "Epoch: 27/200 | Train loss: 3.0131 | Val loss: 0.4413\n",
      "Epoch: 28/200 | Train loss: 2.9722 | Val loss: 0.3399\n",
      "Epoch: 29/200 | Train loss: 2.9074 | Val loss: 0.2600\n",
      "Epoch: 30/200 | Train loss: 2.8928 | Val loss: 0.3057\n",
      "Epoch: 31/200 | Train loss: 2.8549 | Val loss: 0.4382\n",
      "Epoch: 32/200 | Train loss: 2.8042 | Val loss: 0.2623\n",
      "Epoch: 33/200 | Train loss: 2.7792 | Val loss: 0.2989\n",
      "Epoch: 34/200 | Train loss: 2.7333 | Val loss: 0.4406\n",
      "Epoch: 35/200 | Train loss: 2.6762 | Val loss: 0.4184\n",
      "Epoch: 36/200 | Train loss: 2.4834 | Val loss: 0.2807\n",
      "Epoch: 37/200 | Train loss: 1.6460 | Val loss: 0.3585\n",
      "Epoch: 38/200 | Train loss: 1.5503 | Val loss: 0.3247\n",
      "Epoch: 39/200 | Train loss: 1.5883 | Val loss: 0.4843\n",
      "Early stopping!\n",
      "Trained in 39 epochs with best val loss: 0.2600\n",
      "Test loss: 0.4893862768359806\n",
      "\n",
      "\n",
      "---Combination 39---\n",
      "-Hidden size: 256\n",
      "-Batch size: 64\n",
      "-Dropout: 0.2\n",
      "-Depth: 4\n",
      "-Learning rate: 0.001\n",
      "\n",
      "Epoch: 1/200 | Train loss: 0.7148 | Val loss: 0.0566\n",
      "Epoch: 2/200 | Train loss: 0.1201 | Val loss: 0.0194\n",
      "Epoch: 3/200 | Train loss: 0.0817 | Val loss: 0.0137\n",
      "Epoch: 4/200 | Train loss: 0.0679 | Val loss: 0.0186\n",
      "Epoch: 5/200 | Train loss: 0.0560 | Val loss: 0.0109\n",
      "Epoch: 6/200 | Train loss: 0.0551 | Val loss: 0.0121\n",
      "Epoch: 7/200 | Train loss: 0.0521 | Val loss: 0.0098\n",
      "Epoch: 8/200 | Train loss: 0.0498 | Val loss: 0.0115\n",
      "Epoch: 9/200 | Train loss: 0.0475 | Val loss: 0.0150\n",
      "Epoch: 10/200 | Train loss: 0.0458 | Val loss: 0.0100\n",
      "Epoch: 11/200 | Train loss: 0.0445 | Val loss: 0.0282\n",
      "Epoch: 12/200 | Train loss: 0.0440 | Val loss: 0.0120\n",
      "Epoch: 13/200 | Train loss: 0.0412 | Val loss: 0.0086\n",
      "Epoch: 14/200 | Train loss: 0.0387 | Val loss: 0.0149\n",
      "Epoch: 15/200 | Train loss: 0.0419 | Val loss: 0.0126\n",
      "Epoch: 16/200 | Train loss: 0.0387 | Val loss: 0.0167\n",
      "Epoch: 17/200 | Train loss: 0.0371 | Val loss: 0.0118\n",
      "Epoch: 18/200 | Train loss: 0.0366 | Val loss: 0.0156\n",
      "Epoch: 19/200 | Train loss: 0.0341 | Val loss: 0.0253\n",
      "Epoch: 20/200 | Train loss: 0.0353 | Val loss: 0.0173\n",
      "Epoch: 21/200 | Train loss: 0.0349 | Val loss: 0.0226\n",
      "Epoch: 22/200 | Train loss: 0.0346 | Val loss: 0.0143\n",
      "Epoch: 23/200 | Train loss: 0.0312 | Val loss: 0.0146\n",
      "Early stopping!\n",
      "Trained in 23 epochs with best val loss: 0.0086\n",
      "Test loss: 0.014482594999498215\n",
      "\n",
      "\n",
      "---Combination 40---\n",
      "-Hidden size: 256\n",
      "-Batch size: 64\n",
      "-Dropout: 0.2\n",
      "-Depth: 4\n",
      "-Learning rate: 0.1\n",
      "\n",
      "Epoch: 1/200 | Train loss: 7938204211.0567 | Val loss: 0.7219\n",
      "Epoch: 2/200 | Train loss: 2.2257 | Val loss: 1.9573\n",
      "Epoch: 3/200 | Train loss: 1.6767 | Val loss: 0.9172\n",
      "Epoch: 4/200 | Train loss: 1.8315 | Val loss: 0.4249\n",
      "Epoch: 5/200 | Train loss: 1.6287 | Val loss: 0.8495\n",
      "Epoch: 6/200 | Train loss: 2.1034 | Val loss: 0.5456\n",
      "Epoch: 7/200 | Train loss: 1.6241 | Val loss: 1.2385\n",
      "Epoch: 8/200 | Train loss: 1.5137 | Val loss: 1.7038\n",
      "Epoch: 9/200 | Train loss: 1.4942 | Val loss: 0.8458\n",
      "Epoch: 10/200 | Train loss: 1.4010 | Val loss: 0.2967\n",
      "Epoch: 11/200 | Train loss: 1.5572 | Val loss: 1.5686\n",
      "Epoch: 12/200 | Train loss: 1.4842 | Val loss: 1.0570\n",
      "Epoch: 13/200 | Train loss: 1.4827 | Val loss: 0.9929\n",
      "Epoch: 14/200 | Train loss: 1.3604 | Val loss: 1.5360\n",
      "Epoch: 15/200 | Train loss: 1.4895 | Val loss: 1.0294\n",
      "Epoch: 16/200 | Train loss: 2.3103 | Val loss: 0.9905\n",
      "Epoch: 17/200 | Train loss: 1.5053 | Val loss: 1.3795\n",
      "Epoch: 18/200 | Train loss: 1.4918 | Val loss: 1.0553\n",
      "Epoch: 19/200 | Train loss: 1.3101 | Val loss: 4.2055\n",
      "Epoch: 20/200 | Train loss: 1.4984 | Val loss: 0.4049\n",
      "Early stopping!\n",
      "Trained in 20 epochs with best val loss: 0.2967\n",
      "Test loss: 0.41282067912212317\n",
      "\n",
      "\n",
      "---Combination 41---\n",
      "-Hidden size: 256\n",
      "-Batch size: 64\n",
      "-Dropout: 0.2\n",
      "-Depth: 5\n",
      "-Learning rate: 0.001\n",
      "\n",
      "Epoch: 1/200 | Train loss: 0.9436 | Val loss: 0.0651\n",
      "Epoch: 2/200 | Train loss: 0.1314 | Val loss: 0.0224\n",
      "Epoch: 3/200 | Train loss: 0.0867 | Val loss: 0.0242\n",
      "Epoch: 4/200 | Train loss: 0.0717 | Val loss: 0.0152\n",
      "Epoch: 5/200 | Train loss: 0.0641 | Val loss: 0.0267\n",
      "Epoch: 6/200 | Train loss: 0.0593 | Val loss: 0.0127\n",
      "Epoch: 7/200 | Train loss: 0.0558 | Val loss: 0.0112\n",
      "Epoch: 8/200 | Train loss: 0.0514 | Val loss: 0.0114\n",
      "Epoch: 9/200 | Train loss: 0.0497 | Val loss: 0.0318\n",
      "Epoch: 10/200 | Train loss: 0.0467 | Val loss: 0.0090\n",
      "Epoch: 11/200 | Train loss: 0.0453 | Val loss: 0.0098\n",
      "Epoch: 12/200 | Train loss: 0.0453 | Val loss: 0.0315\n",
      "Epoch: 13/200 | Train loss: 0.0441 | Val loss: 0.0113\n",
      "Epoch: 14/200 | Train loss: 0.0399 | Val loss: 0.0139\n",
      "Epoch: 15/200 | Train loss: 0.0387 | Val loss: 0.0198\n",
      "Epoch: 16/200 | Train loss: 0.0372 | Val loss: 0.0103\n",
      "Epoch: 17/200 | Train loss: 0.0353 | Val loss: 0.0204\n",
      "Epoch: 18/200 | Train loss: 0.0354 | Val loss: 0.0093\n",
      "Epoch: 19/200 | Train loss: 0.0323 | Val loss: 0.0089\n",
      "Epoch: 20/200 | Train loss: 0.0335 | Val loss: 0.0121\n",
      "Epoch: 21/200 | Train loss: 0.0329 | Val loss: 0.0149\n",
      "Epoch: 22/200 | Train loss: 0.0318 | Val loss: 0.0185\n",
      "Epoch: 23/200 | Train loss: 0.0326 | Val loss: 0.0097\n",
      "Epoch: 24/200 | Train loss: 0.0299 | Val loss: 0.0077\n",
      "Epoch: 25/200 | Train loss: 0.0284 | Val loss: 0.0154\n",
      "Epoch: 26/200 | Train loss: 0.0305 | Val loss: 0.0128\n",
      "Epoch: 27/200 | Train loss: 0.0288 | Val loss: 0.0093\n",
      "Epoch: 28/200 | Train loss: 0.0278 | Val loss: 0.0091\n",
      "Epoch: 29/200 | Train loss: 0.0272 | Val loss: 0.0197\n",
      "Epoch: 30/200 | Train loss: 0.0278 | Val loss: 0.0133\n",
      "Epoch: 31/200 | Train loss: 0.0273 | Val loss: 0.0105\n",
      "Epoch: 32/200 | Train loss: 0.0267 | Val loss: 0.0094\n",
      "Epoch: 33/200 | Train loss: 0.0252 | Val loss: 0.0115\n",
      "Epoch: 34/200 | Train loss: 0.0280 | Val loss: 0.0091\n",
      "Early stopping!\n",
      "Trained in 34 epochs with best val loss: 0.0077\n",
      "Test loss: 0.009171720596867196\n",
      "\n",
      "\n",
      "---Combination 42---\n",
      "-Hidden size: 256\n",
      "-Batch size: 64\n",
      "-Dropout: 0.2\n",
      "-Depth: 5\n",
      "-Learning rate: 0.1\n",
      "\n",
      "Epoch: 1/200 | Train loss: 314893904427.0789 | Val loss: 13.8050\n",
      "Epoch: 2/200 | Train loss: 1319100.4743 | Val loss: 0.2504\n",
      "Epoch: 3/200 | Train loss: 14299.6855 | Val loss: 13.7975\n",
      "Epoch: 4/200 | Train loss: 40777.4007 | Val loss: 13.7963\n",
      "Epoch: 5/200 | Train loss: 252.4218 | Val loss: 13.7952\n",
      "Epoch: 6/200 | Train loss: 51367.2831 | Val loss: 13.7949\n",
      "Epoch: 7/200 | Train loss: 1948.5842 | Val loss: 13.7929\n",
      "Epoch: 8/200 | Train loss: 80.0101 | Val loss: 13.7910\n",
      "Epoch: 9/200 | Train loss: 33181.2944 | Val loss: 13.7889\n",
      "Epoch: 10/200 | Train loss: 720445.3678 | Val loss: 13.7794\n",
      "Epoch: 11/200 | Train loss: 21.7574 | Val loss: 13.7764\n",
      "Epoch: 12/200 | Train loss: 15.0117 | Val loss: 13.7731\n",
      "Early stopping!\n",
      "Trained in 12 epochs with best val loss: 0.2504\n",
      "Test loss: 13.84122500765151\n",
      "\n",
      "\n",
      "---Combination 43---\n",
      "-Hidden size: 256\n",
      "-Batch size: 64\n",
      "-Dropout: 0.3\n",
      "-Depth: 3\n",
      "-Learning rate: 0.001\n",
      "\n",
      "Epoch: 1/200 | Train loss: 0.7842 | Val loss: 0.1224\n",
      "Epoch: 2/200 | Train loss: 0.1590 | Val loss: 0.2759\n",
      "Epoch: 3/200 | Train loss: 0.1240 | Val loss: 0.0193\n",
      "Epoch: 4/200 | Train loss: 0.0956 | Val loss: 0.0236\n",
      "Epoch: 5/200 | Train loss: 0.0827 | Val loss: 0.0119\n",
      "Epoch: 6/200 | Train loss: 0.0803 | Val loss: 0.0282\n",
      "Epoch: 7/200 | Train loss: 0.0751 | Val loss: 0.0161\n",
      "Epoch: 8/200 | Train loss: 0.0744 | Val loss: 0.0305\n",
      "Epoch: 9/200 | Train loss: 0.0702 | Val loss: 0.0109\n",
      "Epoch: 10/200 | Train loss: 0.0675 | Val loss: 0.0138\n",
      "Epoch: 11/200 | Train loss: 0.0622 | Val loss: 0.0169\n",
      "Epoch: 12/200 | Train loss: 0.0635 | Val loss: 0.0138\n",
      "Epoch: 13/200 | Train loss: 0.0583 | Val loss: 0.0252\n",
      "Epoch: 14/200 | Train loss: 0.0590 | Val loss: 0.0092\n",
      "Epoch: 15/200 | Train loss: 0.0560 | Val loss: 0.0174\n",
      "Epoch: 16/200 | Train loss: 0.0550 | Val loss: 0.0202\n",
      "Epoch: 17/200 | Train loss: 0.0528 | Val loss: 0.0154\n",
      "Epoch: 18/200 | Train loss: 0.0498 | Val loss: 0.0085\n",
      "Epoch: 19/200 | Train loss: 0.0502 | Val loss: 0.0168\n",
      "Epoch: 20/200 | Train loss: 0.0481 | Val loss: 0.0144\n",
      "Epoch: 21/200 | Train loss: 0.0492 | Val loss: 0.0113\n",
      "Epoch: 22/200 | Train loss: 0.0499 | Val loss: 0.0119\n",
      "Epoch: 23/200 | Train loss: 0.0469 | Val loss: 0.0232\n",
      "Epoch: 24/200 | Train loss: 0.0462 | Val loss: 0.0110\n",
      "Epoch: 25/200 | Train loss: 0.0450 | Val loss: 0.0228\n",
      "Epoch: 26/200 | Train loss: 0.0449 | Val loss: 0.0151\n",
      "Epoch: 27/200 | Train loss: 0.0423 | Val loss: 0.0159\n",
      "Epoch: 28/200 | Train loss: 0.0437 | Val loss: 0.0158\n",
      "Early stopping!\n",
      "Trained in 28 epochs with best val loss: 0.0085\n",
      "Test loss: 0.015888187349976404\n",
      "\n",
      "\n",
      "---Combination 44---\n",
      "-Hidden size: 256\n",
      "-Batch size: 64\n",
      "-Dropout: 0.3\n",
      "-Depth: 3\n",
      "-Learning rate: 0.1\n",
      "\n",
      "Epoch: 1/200 | Train loss: 111611322.0785 | Val loss: 1.2705\n",
      "Epoch: 2/200 | Train loss: 93591.2058 | Val loss: 0.7465\n",
      "Epoch: 3/200 | Train loss: 17.1994 | Val loss: 1.1243\n",
      "Epoch: 4/200 | Train loss: 9.8503 | Val loss: 0.8704\n",
      "Epoch: 5/200 | Train loss: 8.7437 | Val loss: 2.2319\n",
      "Epoch: 6/200 | Train loss: 4.8893 | Val loss: 1.6429\n",
      "Epoch: 7/200 | Train loss: 4.0600 | Val loss: 0.2561\n",
      "Epoch: 8/200 | Train loss: 7.4708 | Val loss: 0.6591\n",
      "Epoch: 9/200 | Train loss: 4.0154 | Val loss: 0.6860\n",
      "Epoch: 10/200 | Train loss: 3.2495 | Val loss: 0.7595\n",
      "Epoch: 11/200 | Train loss: 3.0882 | Val loss: 0.6280\n",
      "Epoch: 12/200 | Train loss: 5.9902 | Val loss: 0.9041\n",
      "Epoch: 13/200 | Train loss: 2.9725 | Val loss: 1.0941\n",
      "Epoch: 14/200 | Train loss: 2.8700 | Val loss: 0.8937\n",
      "Epoch: 15/200 | Train loss: 2.7999 | Val loss: 0.8452\n",
      "Epoch: 16/200 | Train loss: 2.8447 | Val loss: 1.6864\n",
      "Epoch: 17/200 | Train loss: 2.7714 | Val loss: 1.1581\n",
      "Early stopping!\n",
      "Trained in 17 epochs with best val loss: 0.2561\n",
      "Test loss: 1.1760740905568219\n",
      "\n",
      "\n",
      "---Combination 45---\n",
      "-Hidden size: 256\n",
      "-Batch size: 64\n",
      "-Dropout: 0.3\n",
      "-Depth: 4\n",
      "-Learning rate: 0.001\n",
      "\n",
      "Epoch: 1/200 | Train loss: 0.9164 | Val loss: 0.0521\n",
      "Epoch: 2/200 | Train loss: 0.1629 | Val loss: 0.0617\n",
      "Epoch: 3/200 | Train loss: 0.1101 | Val loss: 0.0561\n",
      "Epoch: 4/200 | Train loss: 0.0952 | Val loss: 0.0263\n",
      "Epoch: 5/200 | Train loss: 0.0828 | Val loss: 0.0272\n",
      "Epoch: 6/200 | Train loss: 0.0793 | Val loss: 0.0330\n",
      "Epoch: 7/200 | Train loss: 0.0724 | Val loss: 0.0181\n",
      "Epoch: 8/200 | Train loss: 0.0712 | Val loss: 0.0133\n",
      "Epoch: 9/200 | Train loss: 0.0706 | Val loss: 0.0118\n",
      "Epoch: 10/200 | Train loss: 0.0652 | Val loss: 0.0401\n",
      "Epoch: 11/200 | Train loss: 0.0644 | Val loss: 0.0136\n",
      "Epoch: 12/200 | Train loss: 0.0596 | Val loss: 0.0169\n",
      "Epoch: 13/200 | Train loss: 0.0583 | Val loss: 0.0150\n",
      "Epoch: 14/200 | Train loss: 0.0572 | Val loss: 0.0181\n",
      "Epoch: 15/200 | Train loss: 0.0545 | Val loss: 0.0221\n",
      "Epoch: 16/200 | Train loss: 0.0514 | Val loss: 0.0097\n",
      "Epoch: 17/200 | Train loss: 0.0497 | Val loss: 0.0182\n",
      "Epoch: 18/200 | Train loss: 0.0464 | Val loss: 0.0092\n",
      "Epoch: 19/200 | Train loss: 0.0510 | Val loss: 0.0109\n",
      "Epoch: 20/200 | Train loss: 0.0465 | Val loss: 0.0169\n",
      "Epoch: 21/200 | Train loss: 0.0474 | Val loss: 0.0161\n",
      "Epoch: 22/200 | Train loss: 0.0438 | Val loss: 0.0107\n",
      "Epoch: 23/200 | Train loss: 0.0440 | Val loss: 0.0105\n",
      "Epoch: 24/200 | Train loss: 0.0436 | Val loss: 0.0101\n",
      "Epoch: 25/200 | Train loss: 0.0419 | Val loss: 0.0100\n",
      "Epoch: 26/200 | Train loss: 0.0417 | Val loss: 0.0099\n",
      "Epoch: 27/200 | Train loss: 0.0412 | Val loss: 0.0089\n",
      "Epoch: 28/200 | Train loss: 0.0397 | Val loss: 0.0108\n",
      "Epoch: 29/200 | Train loss: 0.0404 | Val loss: 0.0102\n",
      "Epoch: 30/200 | Train loss: 0.0401 | Val loss: 0.0181\n",
      "Epoch: 31/200 | Train loss: 0.0406 | Val loss: 0.0181\n",
      "Epoch: 32/200 | Train loss: 0.0369 | Val loss: 0.0160\n",
      "Epoch: 33/200 | Train loss: 0.0378 | Val loss: 0.0169\n",
      "Epoch: 34/200 | Train loss: 0.0368 | Val loss: 0.0123\n",
      "Epoch: 35/200 | Train loss: 0.0369 | Val loss: 0.0116\n",
      "Epoch: 36/200 | Train loss: 0.0355 | Val loss: 0.0160\n",
      "Epoch: 37/200 | Train loss: 0.0351 | Val loss: 0.0091\n",
      "Early stopping!\n",
      "Trained in 37 epochs with best val loss: 0.0089\n",
      "Test loss: 0.008702344333995943\n",
      "\n",
      "\n",
      "---Combination 46---\n",
      "-Hidden size: 256\n",
      "-Batch size: 64\n",
      "-Dropout: 0.3\n",
      "-Depth: 4\n",
      "-Learning rate: 0.1\n",
      "\n",
      "Epoch: 1/200 | Train loss: 1441236586.1691 | Val loss: 13.7833\n",
      "Epoch: 2/200 | Train loss: 13.9447 | Val loss: 13.7682\n",
      "Epoch: 3/200 | Train loss: 13.8372 | Val loss: 13.7478\n",
      "Epoch: 4/200 | Train loss: 13.8056 | Val loss: 13.7227\n",
      "Epoch: 5/200 | Train loss: 13.7786 | Val loss: 13.6931\n",
      "Epoch: 6/200 | Train loss: 14.7372 | Val loss: 13.6588\n",
      "Epoch: 7/200 | Train loss: 51.8565 | Val loss: 13.6206\n",
      "Epoch: 8/200 | Train loss: 10672.0378 | Val loss: 13.5655\n",
      "Epoch: 9/200 | Train loss: 13.6114 | Val loss: 13.5172\n",
      "Epoch: 10/200 | Train loss: 2686.5213 | Val loss: 13.4584\n",
      "Epoch: 11/200 | Train loss: 13.4966 | Val loss: 13.3973\n",
      "Epoch: 12/200 | Train loss: 284.8641 | Val loss: 13.3302\n",
      "Epoch: 13/200 | Train loss: 13.3650 | Val loss: 13.2602\n",
      "Epoch: 14/200 | Train loss: 13.4419 | Val loss: 13.1842\n",
      "Epoch: 15/200 | Train loss: 13.2124 | Val loss: 13.1015\n",
      "Epoch: 16/200 | Train loss: 13.1262 | Val loss: 13.0120\n",
      "Epoch: 17/200 | Train loss: 13.0329 | Val loss: 12.9153\n",
      "Epoch: 18/200 | Train loss: 12.9322 | Val loss: 12.8108\n",
      "Epoch: 19/200 | Train loss: 12.8236 | Val loss: 12.6984\n",
      "Epoch: 20/200 | Train loss: 12.7067 | Val loss: 12.5775\n",
      "Epoch: 21/200 | Train loss: 12.5811 | Val loss: 12.4475\n",
      "Epoch: 22/200 | Train loss: 12.4462 | Val loss: 12.3080\n",
      "Epoch: 23/200 | Train loss: 12.3016 | Val loss: 12.1588\n",
      "Epoch: 24/200 | Train loss: 12.1469 | Val loss: 11.9991\n",
      "Epoch: 25/200 | Train loss: 237.8000 | Val loss: 11.8210\n",
      "Epoch: 26/200 | Train loss: 11.7974 | Val loss: 11.6391\n",
      "Epoch: 27/200 | Train loss: 11.6093 | Val loss: 11.4454\n",
      "Epoch: 28/200 | Train loss: 11.4092 | Val loss: 11.2396\n",
      "Epoch: 29/200 | Train loss: 11.1965 | Val loss: 11.0209\n",
      "Epoch: 30/200 | Train loss: 10.9708 | Val loss: 10.7892\n",
      "Epoch: 31/200 | Train loss: 10.7319 | Val loss: 10.5440\n",
      "Epoch: 32/200 | Train loss: 10.4795 | Val loss: 10.2852\n",
      "Epoch: 33/200 | Train loss: 10.2132 | Val loss: 10.0126\n",
      "Epoch: 34/200 | Train loss: 9.9330 | Val loss: 9.7261\n",
      "Epoch: 35/200 | Train loss: 9.6387 | Val loss: 9.4254\n",
      "Epoch: 36/200 | Train loss: 9.3304 | Val loss: 9.1108\n",
      "Epoch: 37/200 | Train loss: 9.0084 | Val loss: 8.7828\n",
      "Epoch: 38/200 | Train loss: 8.6729 | Val loss: 8.4417\n",
      "Epoch: 39/200 | Train loss: 8.3245 | Val loss: 8.0880\n",
      "Epoch: 40/200 | Train loss: 7.9637 | Val loss: 7.7223\n",
      "Epoch: 41/200 | Train loss: 7.5919 | Val loss: 7.3461\n",
      "Epoch: 42/200 | Train loss: 7.2098 | Val loss: 6.9607\n",
      "Epoch: 43/200 | Train loss: 6.8190 | Val loss: 6.5672\n",
      "Epoch: 44/200 | Train loss: 6.4210 | Val loss: 6.1673\n",
      "Epoch: 45/200 | Train loss: 6.0174 | Val loss: 5.7632\n",
      "Epoch: 46/200 | Train loss: 5.6107 | Val loss: 5.3572\n",
      "Epoch: 47/200 | Train loss: 5.2034 | Val loss: 4.9512\n",
      "Epoch: 48/200 | Train loss: 4.7972 | Val loss: 4.5485\n",
      "Epoch: 49/200 | Train loss: 4.3954 | Val loss: 4.1512\n",
      "Epoch: 50/200 | Train loss: 4.0006 | Val loss: 3.7626\n",
      "Epoch: 51/200 | Train loss: 3.6157 | Val loss: 3.3852\n",
      "Epoch: 52/200 | Train loss: 3.2436 | Val loss: 3.0224\n",
      "Epoch: 53/200 | Train loss: 2.8871 | Val loss: 2.6762\n",
      "Epoch: 54/200 | Train loss: 2.5490 | Val loss: 2.3502\n",
      "Epoch: 55/200 | Train loss: 2.2317 | Val loss: 2.0458\n",
      "Epoch: 56/200 | Train loss: 1.9374 | Val loss: 1.7653\n",
      "Epoch: 57/200 | Train loss: 1.6679 | Val loss: 1.5107\n",
      "Epoch: 58/200 | Train loss: 1.4244 | Val loss: 1.2821\n",
      "Epoch: 59/200 | Train loss: 1.2071 | Val loss: 1.0803\n",
      "Epoch: 60/200 | Train loss: 1.0169 | Val loss: 0.9049\n",
      "Epoch: 61/200 | Train loss: 0.8527 | Val loss: 0.7551\n",
      "Epoch: 62/200 | Train loss: 0.7141 | Val loss: 0.6301\n",
      "Epoch: 63/200 | Train loss: 0.5992 | Val loss: 0.5279\n",
      "Epoch: 64/200 | Train loss: 0.5057 | Val loss: 0.4459\n",
      "Epoch: 65/200 | Train loss: 0.4316 | Val loss: 0.3820\n",
      "Epoch: 66/200 | Train loss: 0.3743 | Val loss: 0.3333\n",
      "Epoch: 67/200 | Train loss: 0.3312 | Val loss: 0.2974\n",
      "Epoch: 68/200 | Train loss: 0.2996 | Val loss: 0.2718\n",
      "Epoch: 69/200 | Train loss: 0.2774 | Val loss: 0.2541\n",
      "Epoch: 70/200 | Train loss: 0.2621 | Val loss: 0.2425\n",
      "Epoch: 71/200 | Train loss: 0.2522 | Val loss: 0.2350\n",
      "Epoch: 72/200 | Train loss: 0.2458 | Val loss: 0.2305\n",
      "Epoch: 73/200 | Train loss: 0.2420 | Val loss: 0.2281\n",
      "Epoch: 74/200 | Train loss: 0.2398 | Val loss: 0.2266\n",
      "Epoch: 75/200 | Train loss: 0.2386 | Val loss: 0.2260\n",
      "Epoch: 76/200 | Train loss: 0.2380 | Val loss: 0.2257\n",
      "Epoch: 77/200 | Train loss: 0.2377 | Val loss: 0.2256\n",
      "Epoch: 78/200 | Train loss: 0.2375 | Val loss: 0.2256\n",
      "Epoch: 79/200 | Train loss: 0.2375 | Val loss: 0.2256\n",
      "Epoch: 80/200 | Train loss: 0.2374 | Val loss: 0.2256\n",
      "Epoch: 81/200 | Train loss: 0.2374 | Val loss: 0.2256\n",
      "Epoch: 82/200 | Train loss: 0.2374 | Val loss: 0.2256\n",
      "Epoch: 83/200 | Train loss: 0.2374 | Val loss: 0.2256\n",
      "Epoch: 84/200 | Train loss: 0.2374 | Val loss: 0.2256\n",
      "Epoch: 85/200 | Train loss: 0.2375 | Val loss: 0.2257\n",
      "Epoch: 86/200 | Train loss: 0.2374 | Val loss: 0.2257\n",
      "Epoch: 87/200 | Train loss: 0.2374 | Val loss: 0.2257\n",
      "Epoch: 88/200 | Train loss: 0.2375 | Val loss: 0.2256\n",
      "Early stopping!\n",
      "Trained in 88 epochs with best val loss: 0.2256\n",
      "Test loss: 0.22556530643200529\n",
      "\n",
      "\n",
      "---Combination 47---\n",
      "-Hidden size: 256\n",
      "-Batch size: 64\n",
      "-Dropout: 0.3\n",
      "-Depth: 5\n",
      "-Learning rate: 0.001\n",
      "\n",
      "Epoch: 1/200 | Train loss: 1.0072 | Val loss: 0.0842\n",
      "Epoch: 2/200 | Train loss: 0.1640 | Val loss: 0.0437\n",
      "Epoch: 3/200 | Train loss: 0.1172 | Val loss: 0.0508\n",
      "Epoch: 4/200 | Train loss: 0.0983 | Val loss: 0.0658\n",
      "Epoch: 5/200 | Train loss: 0.0903 | Val loss: 0.0316\n",
      "Epoch: 6/200 | Train loss: 0.0855 | Val loss: 0.0142\n",
      "Epoch: 7/200 | Train loss: 0.0755 | Val loss: 0.0195\n",
      "Epoch: 8/200 | Train loss: 0.0751 | Val loss: 0.0281\n",
      "Epoch: 9/200 | Train loss: 0.0734 | Val loss: 0.0107\n",
      "Epoch: 10/200 | Train loss: 0.0712 | Val loss: 0.0207\n",
      "Epoch: 11/200 | Train loss: 0.0652 | Val loss: 0.0189\n",
      "Epoch: 12/200 | Train loss: 0.0648 | Val loss: 0.0214\n",
      "Epoch: 13/200 | Train loss: 0.0634 | Val loss: 0.0135\n",
      "Epoch: 14/200 | Train loss: 0.0605 | Val loss: 0.0250\n",
      "Epoch: 15/200 | Train loss: 0.0588 | Val loss: 0.0187\n",
      "Epoch: 16/200 | Train loss: 0.0535 | Val loss: 0.0117\n",
      "Epoch: 17/200 | Train loss: 0.0538 | Val loss: 0.0150\n",
      "Epoch: 18/200 | Train loss: 0.0521 | Val loss: 0.0142\n",
      "Epoch: 19/200 | Train loss: 0.0483 | Val loss: 0.0118\n",
      "Early stopping!\n",
      "Trained in 19 epochs with best val loss: 0.0107\n",
      "Test loss: 0.011725299451770126\n",
      "\n",
      "\n",
      "---Combination 48---\n",
      "-Hidden size: 256\n",
      "-Batch size: 64\n",
      "-Dropout: 0.3\n",
      "-Depth: 5\n",
      "-Learning rate: 0.1\n",
      "\n",
      "Epoch: 1/200 | Train loss: 33837889918.3545 | Val loss: 14.1566\n",
      "Epoch: 2/200 | Train loss: 109.5097 | Val loss: 14.1544\n",
      "Epoch: 3/200 | Train loss: 13.6306 | Val loss: 14.1509\n",
      "Epoch: 4/200 | Train loss: 14.0687 | Val loss: 14.1467\n",
      "Epoch: 5/200 | Train loss: 5166.6091 | Val loss: 14.1407\n",
      "Epoch: 6/200 | Train loss: 173039.0531 | Val loss: 14.1267\n",
      "Epoch: 7/200 | Train loss: 13.7950 | Val loss: 14.1208\n",
      "Epoch: 8/200 | Train loss: 12.7448 | Val loss: 14.1136\n",
      "Epoch: 9/200 | Train loss: 12.5295 | Val loss: 14.1056\n",
      "Epoch: 10/200 | Train loss: 14.2771 | Val loss: 14.0967\n",
      "Epoch: 11/200 | Train loss: 12.3983 | Val loss: 14.0870\n",
      "Epoch: 12/200 | Train loss: 12.1917 | Val loss: 14.0762\n",
      "Epoch: 13/200 | Train loss: 12.0647 | Val loss: 14.0645\n",
      "Epoch: 14/200 | Train loss: 12.4241 | Val loss: 14.0518\n",
      "Epoch: 15/200 | Train loss: 11.8644 | Val loss: 14.0378\n",
      "Epoch: 16/200 | Train loss: 11.7177 | Val loss: 14.0228\n",
      "Epoch: 17/200 | Train loss: 11.7289 | Val loss: 14.0063\n",
      "Epoch: 18/200 | Train loss: 11.7160 | Val loss: 13.9884\n",
      "Epoch: 19/200 | Train loss: 11.6533 | Val loss: 13.9688\n",
      "Epoch: 20/200 | Train loss: 17.5424 | Val loss: 13.9481\n",
      "Epoch: 21/200 | Train loss: 10.9566 | Val loss: 13.9261\n",
      "Epoch: 22/200 | Train loss: 11.0980 | Val loss: 13.9018\n",
      "Epoch: 23/200 | Train loss: 10.9697 | Val loss: 13.8757\n",
      "Epoch: 24/200 | Train loss: 10.8614 | Val loss: 13.8476\n",
      "Epoch: 25/200 | Train loss: 10.7984 | Val loss: 13.8173\n",
      "Epoch: 26/200 | Train loss: 11.1258 | Val loss: 13.7844\n",
      "Epoch: 27/200 | Train loss: 10.5882 | Val loss: 13.7497\n",
      "Epoch: 28/200 | Train loss: 10.7489 | Val loss: 13.7112\n",
      "Epoch: 29/200 | Train loss: 53.8047 | Val loss: 13.6691\n",
      "Epoch: 30/200 | Train loss: 10.6049 | Val loss: 13.6246\n",
      "Epoch: 31/200 | Train loss: 10.3440 | Val loss: 13.5774\n",
      "Epoch: 32/200 | Train loss: 10.4565 | Val loss: 13.5257\n",
      "Epoch: 33/200 | Train loss: 10.3067 | Val loss: 13.4705\n",
      "Epoch: 34/200 | Train loss: 10.3272 | Val loss: 13.4107\n",
      "Epoch: 35/200 | Train loss: 10.8300 | Val loss: 13.3462\n",
      "Epoch: 36/200 | Train loss: 10.1016 | Val loss: 13.2772\n",
      "Epoch: 37/200 | Train loss: 9.9961 | Val loss: 13.2035\n",
      "Epoch: 38/200 | Train loss: 9.9257 | Val loss: 13.1242\n",
      "Epoch: 39/200 | Train loss: 9.9340 | Val loss: 13.0382\n",
      "Epoch: 40/200 | Train loss: 9.7927 | Val loss: 12.9467\n",
      "Epoch: 41/200 | Train loss: 9.7014 | Val loss: 12.8481\n",
      "Epoch: 42/200 | Train loss: 9.5380 | Val loss: 12.7432\n",
      "Epoch: 43/200 | Train loss: 9.4070 | Val loss: 12.6315\n",
      "Epoch: 44/200 | Train loss: 9.3055 | Val loss: 12.5119\n",
      "Epoch: 45/200 | Train loss: 9.5144 | Val loss: 12.3834\n",
      "Epoch: 46/200 | Train loss: 9.1990 | Val loss: 12.2448\n",
      "Epoch: 47/200 | Train loss: 9.0141 | Val loss: 12.0977\n",
      "Epoch: 48/200 | Train loss: 8.9025 | Val loss: 11.9399\n",
      "Epoch: 49/200 | Train loss: 8.6779 | Val loss: 11.7742\n",
      "Epoch: 50/200 | Train loss: 8.5585 | Val loss: 11.5968\n",
      "Epoch: 51/200 | Train loss: 8.4606 | Val loss: 11.4079\n",
      "Epoch: 52/200 | Train loss: 8.2463 | Val loss: 11.2082\n",
      "Epoch: 53/200 | Train loss: 8.1305 | Val loss: 10.9948\n",
      "Epoch: 54/200 | Train loss: 8.0280 | Val loss: 10.7683\n",
      "Epoch: 55/200 | Train loss: 7.7777 | Val loss: 10.5305\n",
      "Epoch: 56/200 | Train loss: 7.6590 | Val loss: 10.2771\n",
      "Epoch: 57/200 | Train loss: 7.5287 | Val loss: 10.0080\n",
      "Epoch: 58/200 | Train loss: 7.2303 | Val loss: 9.7294\n",
      "Epoch: 59/200 | Train loss: 7.0716 | Val loss: 9.4353\n",
      "Epoch: 60/200 | Train loss: 6.8149 | Val loss: 9.1276\n",
      "Epoch: 61/200 | Train loss: 6.5856 | Val loss: 8.8072\n",
      "Epoch: 62/200 | Train loss: 6.3282 | Val loss: 8.4760\n",
      "Epoch: 63/200 | Train loss: 6.0927 | Val loss: 8.1304\n",
      "Epoch: 64/200 | Train loss: 5.7804 | Val loss: 7.7772\n",
      "Epoch: 65/200 | Train loss: 5.6383 | Val loss: 7.4051\n",
      "Epoch: 66/200 | Train loss: 5.2877 | Val loss: 7.0280\n",
      "Epoch: 67/200 | Train loss: 4.9849 | Val loss: 6.6452\n",
      "Epoch: 68/200 | Train loss: 4.7452 | Val loss: 6.2515\n",
      "Epoch: 69/200 | Train loss: 4.4734 | Val loss: 5.8514\n",
      "Epoch: 70/200 | Train loss: 4.1708 | Val loss: 5.4498\n",
      "Epoch: 71/200 | Train loss: 3.8461 | Val loss: 5.0504\n",
      "Epoch: 72/200 | Train loss: 3.5835 | Val loss: 4.6523\n",
      "Epoch: 73/200 | Train loss: 3.3213 | Val loss: 4.2550\n",
      "Epoch: 74/200 | Train loss: 3.0315 | Val loss: 3.8644\n",
      "Epoch: 75/200 | Train loss: 2.7845 | Val loss: 3.4818\n",
      "Epoch: 76/200 | Train loss: 2.4534 | Val loss: 3.1204\n",
      "Epoch: 77/200 | Train loss: 2.2140 | Val loss: 2.7735\n",
      "Epoch: 78/200 | Train loss: 1.9895 | Val loss: 2.4396\n",
      "Epoch: 79/200 | Train loss: 1.7530 | Val loss: 2.1283\n",
      "Epoch: 80/200 | Train loss: 1.5311 | Val loss: 1.8417\n",
      "Epoch: 81/200 | Train loss: 1.3290 | Val loss: 1.5792\n",
      "Epoch: 82/200 | Train loss: 1.1535 | Val loss: 1.3421\n",
      "Epoch: 83/200 | Train loss: 0.9896 | Val loss: 1.1333\n",
      "Epoch: 84/200 | Train loss: 0.8301 | Val loss: 0.9515\n",
      "Epoch: 85/200 | Train loss: 0.7212 | Val loss: 0.7940\n",
      "Epoch: 86/200 | Train loss: 0.6248 | Val loss: 0.6594\n",
      "Epoch: 87/200 | Train loss: 0.5251 | Val loss: 0.5495\n",
      "Epoch: 88/200 | Train loss: 0.4497 | Val loss: 0.4626\n",
      "Epoch: 89/200 | Train loss: 0.3925 | Val loss: 0.3934\n",
      "Epoch: 90/200 | Train loss: 0.3427 | Val loss: 0.3422\n",
      "Epoch: 91/200 | Train loss: 0.3139 | Val loss: 0.3030\n",
      "Epoch: 92/200 | Train loss: 0.2887 | Val loss: 0.2748\n",
      "Epoch: 93/200 | Train loss: 0.2688 | Val loss: 0.2559\n",
      "Epoch: 94/200 | Train loss: 0.2579 | Val loss: 0.2432\n",
      "Epoch: 95/200 | Train loss: 0.2503 | Val loss: 0.2349\n",
      "Epoch: 96/200 | Train loss: 0.2448 | Val loss: 0.2300\n",
      "Epoch: 97/200 | Train loss: 0.2407 | Val loss: 0.2276\n",
      "Epoch: 98/200 | Train loss: 0.2392 | Val loss: 0.2263\n",
      "Epoch: 99/200 | Train loss: 0.2382 | Val loss: 0.2258\n",
      "Epoch: 100/200 | Train loss: 0.2377 | Val loss: 0.2256\n",
      "Epoch: 101/200 | Train loss: 0.2375 | Val loss: 0.2256\n",
      "Epoch: 102/200 | Train loss: 0.2375 | Val loss: 0.2256\n",
      "Epoch: 103/200 | Train loss: 0.2374 | Val loss: 0.2256\n",
      "Epoch: 104/200 | Train loss: 0.2374 | Val loss: 0.2256\n",
      "Epoch: 105/200 | Train loss: 0.2374 | Val loss: 0.2256\n",
      "Epoch: 106/200 | Train loss: 0.2374 | Val loss: 0.2256\n",
      "Epoch: 107/200 | Train loss: 0.2374 | Val loss: 0.2256\n",
      "Epoch: 108/200 | Train loss: 0.2375 | Val loss: 0.2257\n",
      "Epoch: 109/200 | Train loss: 0.2374 | Val loss: 0.2257\n",
      "Epoch: 110/200 | Train loss: 0.2375 | Val loss: 0.2256\n",
      "Epoch: 111/200 | Train loss: 0.2375 | Val loss: 0.2256\n",
      "Early stopping!\n",
      "Trained in 111 epochs with best val loss: 0.2256\n",
      "Test loss: 0.22556223467640255\n",
      "\n",
      "\n",
      "---Combination 49---\n",
      "-Hidden size: 512\n",
      "-Batch size: 32\n",
      "-Dropout: 0.2\n",
      "-Depth: 3\n",
      "-Learning rate: 0.001\n",
      "\n",
      "Epoch: 1/200 | Train loss: 0.3519 | Val loss: 0.0300\n",
      "Epoch: 2/200 | Train loss: 0.0770 | Val loss: 0.0163\n",
      "Epoch: 3/200 | Train loss: 0.0515 | Val loss: 0.0173\n",
      "Epoch: 4/200 | Train loss: 0.0431 | Val loss: 0.0198\n",
      "Epoch: 5/200 | Train loss: 0.0372 | Val loss: 0.0126\n",
      "Epoch: 6/200 | Train loss: 0.0351 | Val loss: 0.0132\n",
      "Epoch: 7/200 | Train loss: 0.0342 | Val loss: 0.0125\n",
      "Epoch: 8/200 | Train loss: 0.0313 | Val loss: 0.0102\n",
      "Epoch: 9/200 | Train loss: 0.0305 | Val loss: 0.0097\n",
      "Epoch: 10/200 | Train loss: 0.0284 | Val loss: 0.0120\n",
      "Epoch: 11/200 | Train loss: 0.0283 | Val loss: 0.0101\n",
      "Epoch: 12/200 | Train loss: 0.0275 | Val loss: 0.0117\n",
      "Epoch: 13/200 | Train loss: 0.0267 | Val loss: 0.0220\n",
      "Epoch: 14/200 | Train loss: 0.0268 | Val loss: 0.0111\n",
      "Epoch: 15/200 | Train loss: 0.0250 | Val loss: 0.0097\n",
      "Epoch: 16/200 | Train loss: 0.0250 | Val loss: 0.0139\n",
      "Epoch: 17/200 | Train loss: 0.0249 | Val loss: 0.0097\n",
      "Epoch: 18/200 | Train loss: 0.0239 | Val loss: 0.0099\n",
      "Epoch: 19/200 | Train loss: 0.0239 | Val loss: 0.0099\n",
      "Epoch: 20/200 | Train loss: 0.0226 | Val loss: 0.0104\n",
      "Epoch: 21/200 | Train loss: 0.0222 | Val loss: 0.0099\n",
      "Epoch: 22/200 | Train loss: 0.0233 | Val loss: 0.0089\n",
      "Epoch: 23/200 | Train loss: 0.0228 | Val loss: 0.0094\n",
      "Epoch: 24/200 | Train loss: 0.0212 | Val loss: 0.0079\n",
      "Epoch: 25/200 | Train loss: 0.0212 | Val loss: 0.0099\n",
      "Epoch: 26/200 | Train loss: 0.0205 | Val loss: 0.0117\n",
      "Epoch: 27/200 | Train loss: 0.0216 | Val loss: 0.0082\n",
      "Epoch: 28/200 | Train loss: 0.0203 | Val loss: 0.0105\n",
      "Epoch: 29/200 | Train loss: 0.0209 | Val loss: 0.0123\n",
      "Epoch: 30/200 | Train loss: 0.0196 | Val loss: 0.0079\n",
      "Epoch: 31/200 | Train loss: 0.0189 | Val loss: 0.0118\n",
      "Epoch: 32/200 | Train loss: 0.0177 | Val loss: 0.0105\n",
      "Epoch: 33/200 | Train loss: 0.0180 | Val loss: 0.0113\n",
      "Epoch: 34/200 | Train loss: 0.0179 | Val loss: 0.0108\n",
      "Epoch: 35/200 | Train loss: 0.0173 | Val loss: 0.0110\n",
      "Epoch: 36/200 | Train loss: 0.0176 | Val loss: 0.0089\n",
      "Epoch: 37/200 | Train loss: 0.0172 | Val loss: 0.0188\n",
      "Epoch: 38/200 | Train loss: 0.0167 | Val loss: 0.0099\n",
      "Epoch: 39/200 | Train loss: 0.0161 | Val loss: 0.0143\n",
      "Epoch: 40/200 | Train loss: 0.0161 | Val loss: 0.0085\n",
      "Early stopping!\n",
      "Trained in 40 epochs with best val loss: 0.0079\n",
      "Test loss: 0.008549167474974757\n",
      "\n",
      "\n",
      "---Combination 50---\n",
      "-Hidden size: 512\n",
      "-Batch size: 32\n",
      "-Dropout: 0.2\n",
      "-Depth: 3\n",
      "-Learning rate: 0.1\n",
      "\n",
      "Epoch: 1/200 | Train loss: 8928561969.0360 | Val loss: 14.0856\n",
      "Epoch: 2/200 | Train loss: 14.1986 | Val loss: 14.0730\n",
      "Epoch: 3/200 | Train loss: 14.0781 | Val loss: 14.0552\n",
      "Epoch: 4/200 | Train loss: 13.9813 | Val loss: 14.0322\n",
      "Epoch: 5/200 | Train loss: 20024769.0780 | Val loss: 13.5763\n",
      "Epoch: 6/200 | Train loss: 6431.8412 | Val loss: 13.5410\n",
      "Epoch: 7/200 | Train loss: 803.2060 | Val loss: 13.4972\n",
      "Epoch: 8/200 | Train loss: 68.5872 | Val loss: 13.4489\n",
      "Epoch: 9/200 | Train loss: 131.9124 | Val loss: 13.3902\n",
      "Epoch: 10/200 | Train loss: 118.2239 | Val loss: 13.3233\n",
      "Epoch: 11/200 | Train loss: 68.4874 | Val loss: 13.2450\n",
      "Epoch: 12/200 | Train loss: 48.9946 | Val loss: 13.1531\n",
      "Epoch: 13/200 | Train loss: 22.2533 | Val loss: 13.0455\n",
      "Epoch: 14/200 | Train loss: 43.0622 | Val loss: 12.9207\n",
      "Epoch: 15/200 | Train loss: 42.3318 | Val loss: 12.7752\n",
      "Epoch: 16/200 | Train loss: 20.8730 | Val loss: 12.6085\n",
      "Epoch: 17/200 | Train loss: 28.2147 | Val loss: 12.4189\n",
      "Epoch: 18/200 | Train loss: 15.2935 | Val loss: 12.1978\n",
      "Epoch: 19/200 | Train loss: 2003.4025 | Val loss: 11.9792\n",
      "Epoch: 20/200 | Train loss: 13.0343 | Val loss: 11.6875\n",
      "Epoch: 21/200 | Train loss: 11.9243 | Val loss: 11.3559\n",
      "Epoch: 22/200 | Train loss: 11.5130 | Val loss: 10.9806\n",
      "Epoch: 23/200 | Train loss: 10.9541 | Val loss: 10.5575\n",
      "Epoch: 24/200 | Train loss: 10.4366 | Val loss: 10.0848\n",
      "Epoch: 25/200 | Train loss: 10.1847 | Val loss: 9.5597\n",
      "Epoch: 26/200 | Train loss: 9.5063 | Val loss: 8.9808\n",
      "Epoch: 27/200 | Train loss: 8.7249 | Val loss: 8.3492\n",
      "Epoch: 28/200 | Train loss: 8.0635 | Val loss: 7.6683\n",
      "Epoch: 29/200 | Train loss: 7.3597 | Val loss: 6.9446\n",
      "Epoch: 30/200 | Train loss: 6.6955 | Val loss: 6.1875\n",
      "Epoch: 31/200 | Train loss: 6.1136 | Val loss: 5.4111\n",
      "Epoch: 32/200 | Train loss: 5.0614 | Val loss: 4.6300\n",
      "Epoch: 33/200 | Train loss: 4.3160 | Val loss: 3.8645\n",
      "Epoch: 34/200 | Train loss: 3.6846 | Val loss: 3.1373\n",
      "Epoch: 35/200 | Train loss: 2.9408 | Val loss: 2.4678\n",
      "Epoch: 36/200 | Train loss: 2.2940 | Val loss: 1.8769\n",
      "Epoch: 37/200 | Train loss: 2.5465 | Val loss: 1.3806\n",
      "Epoch: 38/200 | Train loss: 1.2152 | Val loss: 0.9848\n",
      "Epoch: 39/200 | Train loss: 5.2631 | Val loss: 0.6825\n",
      "Epoch: 40/200 | Train loss: 6.3654 | Val loss: 0.4874\n",
      "Epoch: 41/200 | Train loss: 0.7080 | Val loss: 0.3581\n",
      "Epoch: 42/200 | Train loss: 0.4443 | Val loss: 0.2857\n",
      "Epoch: 43/200 | Train loss: 0.7224 | Val loss: 0.2490\n",
      "Epoch: 44/200 | Train loss: 0.2536 | Val loss: 0.2331\n",
      "Epoch: 45/200 | Train loss: 0.2541 | Val loss: 0.2274\n",
      "Epoch: 46/200 | Train loss: 0.3166 | Val loss: 0.2259\n",
      "Epoch: 47/200 | Train loss: 0.2440 | Val loss: 0.2256\n",
      "Epoch: 48/200 | Train loss: 0.2375 | Val loss: 0.2256\n",
      "Epoch: 49/200 | Train loss: 0.2546 | Val loss: 0.2257\n",
      "Epoch: 50/200 | Train loss: 0.2705 | Val loss: 0.2256\n",
      "Epoch: 51/200 | Train loss: 0.2853 | Val loss: 0.2256\n",
      "Epoch: 52/200 | Train loss: 0.3039 | Val loss: 0.2257\n",
      "Epoch: 53/200 | Train loss: 0.2527 | Val loss: 0.2256\n",
      "Epoch: 54/200 | Train loss: 0.2868 | Val loss: 0.2256\n",
      "Epoch: 55/200 | Train loss: 0.2418 | Val loss: 0.2256\n",
      "Epoch: 56/200 | Train loss: 0.2726 | Val loss: 0.2257\n",
      "Epoch: 57/200 | Train loss: 0.2625 | Val loss: 0.2257\n",
      "Early stopping!\n",
      "Trained in 57 epochs with best val loss: 0.2256\n",
      "Test loss: 0.2255647251571434\n",
      "\n",
      "\n",
      "---Combination 51---\n",
      "-Hidden size: 512\n",
      "-Batch size: 32\n",
      "-Dropout: 0.2\n",
      "-Depth: 4\n",
      "-Learning rate: 0.001\n",
      "\n",
      "Epoch: 1/200 | Train loss: 0.5634 | Val loss: 0.0317\n",
      "Epoch: 2/200 | Train loss: 0.0804 | Val loss: 0.0373\n",
      "Epoch: 3/200 | Train loss: 0.0551 | Val loss: 0.0158\n",
      "Epoch: 4/200 | Train loss: 0.0467 | Val loss: 0.0133\n",
      "Epoch: 5/200 | Train loss: 0.0439 | Val loss: 0.0119\n",
      "Epoch: 6/200 | Train loss: 0.0376 | Val loss: 0.0331\n",
      "Epoch: 7/200 | Train loss: 0.0372 | Val loss: 0.0106\n",
      "Epoch: 8/200 | Train loss: 0.0331 | Val loss: 0.0113\n",
      "Epoch: 9/200 | Train loss: 0.0354 | Val loss: 0.0181\n",
      "Epoch: 10/200 | Train loss: 0.0315 | Val loss: 0.0115\n",
      "Epoch: 11/200 | Train loss: 0.0339 | Val loss: 0.0199\n",
      "Epoch: 12/200 | Train loss: 0.0285 | Val loss: 0.0112\n",
      "Epoch: 13/200 | Train loss: 0.0308 | Val loss: 0.0220\n",
      "Epoch: 14/200 | Train loss: 0.0259 | Val loss: 0.0180\n",
      "Epoch: 15/200 | Train loss: 0.0266 | Val loss: 0.0137\n",
      "Epoch: 16/200 | Train loss: 0.0258 | Val loss: 0.0116\n",
      "Epoch: 17/200 | Train loss: 0.0244 | Val loss: 0.0338\n",
      "Early stopping!\n",
      "Trained in 17 epochs with best val loss: 0.0106\n",
      "Test loss: 0.033902764244787935\n",
      "\n",
      "\n",
      "---Combination 52---\n",
      "-Hidden size: 512\n",
      "-Batch size: 32\n",
      "-Dropout: 0.2\n",
      "-Depth: 4\n",
      "-Learning rate: 0.1\n",
      "\n",
      "Epoch: 1/200 | Train loss: 822961434459.6552 | Val loss: 53.0866\n",
      "Epoch: 2/200 | Train loss: 625658.8007 | Val loss: 6598.3064\n",
      "Epoch: 3/200 | Train loss: 26374.8523 | Val loss: 0.2371\n",
      "Epoch: 4/200 | Train loss: 1505.2162 | Val loss: 10.4182\n",
      "Epoch: 5/200 | Train loss: 489.3581 | Val loss: 7.7794\n",
      "Epoch: 6/200 | Train loss: 273.7384 | Val loss: 12.4783\n",
      "Epoch: 7/200 | Train loss: 232.1509 | Val loss: 10.6329\n",
      "Epoch: 8/200 | Train loss: 272.0922 | Val loss: 4.7912\n",
      "Epoch: 9/200 | Train loss: 164.2804 | Val loss: 3.1823\n",
      "Epoch: 10/200 | Train loss: 103.2396 | Val loss: 1.7040\n",
      "Epoch: 11/200 | Train loss: 198.0481 | Val loss: 2.5569\n",
      "Epoch: 12/200 | Train loss: 228.8446 | Val loss: 0.2704\n",
      "Epoch: 13/200 | Train loss: 130.6736 | Val loss: 0.2872\n",
      "Early stopping!\n",
      "Trained in 13 epochs with best val loss: 0.2371\n",
      "Test loss: 0.2918592120858206\n",
      "\n",
      "\n",
      "---Combination 53---\n",
      "-Hidden size: 512\n",
      "-Batch size: 32\n",
      "-Dropout: 0.2\n",
      "-Depth: 5\n",
      "-Learning rate: 0.001\n",
      "\n",
      "Epoch: 1/200 | Train loss: 0.9230 | Val loss: 0.0330\n",
      "Epoch: 2/200 | Train loss: 0.0873 | Val loss: 0.0146\n",
      "Epoch: 3/200 | Train loss: 0.0619 | Val loss: 0.0118\n",
      "Epoch: 4/200 | Train loss: 0.0530 | Val loss: 0.0122\n",
      "Epoch: 5/200 | Train loss: 0.0455 | Val loss: 0.0120\n",
      "Epoch: 6/200 | Train loss: 0.0412 | Val loss: 0.0155\n",
      "Epoch: 7/200 | Train loss: 0.0391 | Val loss: 0.0098\n",
      "Epoch: 8/200 | Train loss: 0.0355 | Val loss: 0.0196\n",
      "Epoch: 9/200 | Train loss: 0.0327 | Val loss: 0.0092\n",
      "Epoch: 10/200 | Train loss: 0.0319 | Val loss: 0.0186\n",
      "Epoch: 11/200 | Train loss: 0.0294 | Val loss: 0.0200\n",
      "Epoch: 12/200 | Train loss: 0.0310 | Val loss: 0.0099\n",
      "Epoch: 13/200 | Train loss: 0.0272 | Val loss: 0.0094\n",
      "Epoch: 14/200 | Train loss: 0.0272 | Val loss: 0.0248\n",
      "Epoch: 15/200 | Train loss: 0.0259 | Val loss: 0.0088\n",
      "Epoch: 16/200 | Train loss: 0.0260 | Val loss: 0.0075\n",
      "Epoch: 17/200 | Train loss: 0.0247 | Val loss: 0.0095\n",
      "Epoch: 18/200 | Train loss: 0.0246 | Val loss: 0.0106\n",
      "Epoch: 19/200 | Train loss: 0.0238 | Val loss: 0.0236\n",
      "Epoch: 20/200 | Train loss: 0.0234 | Val loss: 0.0084\n",
      "Epoch: 21/200 | Train loss: 0.0247 | Val loss: 0.0175\n",
      "Epoch: 22/200 | Train loss: 0.0235 | Val loss: 0.0140\n",
      "Epoch: 23/200 | Train loss: 0.0220 | Val loss: 0.0080\n",
      "Epoch: 24/200 | Train loss: 0.0232 | Val loss: 0.0145\n",
      "Epoch: 25/200 | Train loss: 0.0221 | Val loss: 0.0150\n",
      "Epoch: 26/200 | Train loss: 0.0223 | Val loss: 0.0084\n",
      "Early stopping!\n",
      "Trained in 26 epochs with best val loss: 0.0075\n",
      "Test loss: 0.008658700541633627\n",
      "\n",
      "\n",
      "---Combination 54---\n",
      "-Hidden size: 512\n",
      "-Batch size: 32\n",
      "-Dropout: 0.2\n",
      "-Depth: 5\n",
      "-Learning rate: 0.1\n",
      "\n",
      "Epoch: 1/200 | Train loss: 170540624628314.9688 | Val loss: 13.8484\n",
      "Epoch: 2/200 | Train loss: 19848.1907 | Val loss: 13.8484\n",
      "Epoch: 3/200 | Train loss: 13.9183 | Val loss: 13.8482\n",
      "Epoch: 4/200 | Train loss: 13.9924 | Val loss: 13.8481\n",
      "Epoch: 5/200 | Train loss: 13.9180 | Val loss: 13.8479\n",
      "Epoch: 6/200 | Train loss: 184205.2683 | Val loss: 13.8479\n",
      "Epoch: 7/200 | Train loss: 311983.2750 | Val loss: 13.8471\n",
      "Epoch: 8/200 | Train loss: 6189.6982 | Val loss: 13.8467\n",
      "Epoch: 9/200 | Train loss: 14.1770 | Val loss: 13.8462\n",
      "Epoch: 10/200 | Train loss: 13.9160 | Val loss: 13.8457\n",
      "Epoch: 11/200 | Train loss: 13.9155 | Val loss: 13.8451\n",
      "Epoch: 12/200 | Train loss: 13.9148 | Val loss: 13.8444\n",
      "Epoch: 13/200 | Train loss: 13.9140 | Val loss: 13.8436\n",
      "Epoch: 14/200 | Train loss: 13.9131 | Val loss: 13.8426\n",
      "Epoch: 15/200 | Train loss: 595.7150 | Val loss: 13.8414\n",
      "Epoch: 16/200 | Train loss: 13.9108 | Val loss: 13.8401\n",
      "Epoch: 17/200 | Train loss: 13.9093 | Val loss: 13.8385\n",
      "Epoch: 18/200 | Train loss: 13.9077 | Val loss: 13.8367\n",
      "Epoch: 19/200 | Train loss: 13.9057 | Val loss: 13.8346\n",
      "Epoch: 20/200 | Train loss: 13.9034 | Val loss: 13.8321\n",
      "Epoch: 21/200 | Train loss: 13.9007 | Val loss: 13.8292\n",
      "Epoch: 22/200 | Train loss: 13.8975 | Val loss: 13.8258\n",
      "Epoch: 23/200 | Train loss: 13.8939 | Val loss: 13.8218\n",
      "Epoch: 24/200 | Train loss: 13.8896 | Val loss: 13.8172\n",
      "Epoch: 25/200 | Train loss: 13.8846 | Val loss: 13.8118\n",
      "Epoch: 26/200 | Train loss: 13.8787 | Val loss: 13.8055\n",
      "Epoch: 27/200 | Train loss: 13.8719 | Val loss: 13.7981\n",
      "Epoch: 28/200 | Train loss: 13.8639 | Val loss: 13.7895\n",
      "Epoch: 29/200 | Train loss: 13.8546 | Val loss: 13.7795\n",
      "Epoch: 30/200 | Train loss: 13.8437 | Val loss: 13.7678\n",
      "Epoch: 31/200 | Train loss: 13.8310 | Val loss: 13.7541\n",
      "Epoch: 32/200 | Train loss: 13.8161 | Val loss: 13.7381\n",
      "Epoch: 33/200 | Train loss: 13.7988 | Val loss: 13.7194\n",
      "Epoch: 34/200 | Train loss: 13.7786 | Val loss: 13.6977\n",
      "Epoch: 35/200 | Train loss: 13.7551 | Val loss: 13.6723\n",
      "Epoch: 36/200 | Train loss: 13.7276 | Val loss: 13.6428\n",
      "Epoch: 37/200 | Train loss: 13.6956 | Val loss: 13.6083\n",
      "Epoch: 38/200 | Train loss: 13.6583 | Val loss: 13.5681\n",
      "Epoch: 39/200 | Train loss: 13.6148 | Val loss: 13.5213\n",
      "Epoch: 40/200 | Train loss: 13.5642 | Val loss: 13.4669\n",
      "Epoch: 41/200 | Train loss: 13.5054 | Val loss: 13.4036\n",
      "Epoch: 42/200 | Train loss: 13.4369 | Val loss: 13.3300\n",
      "Epoch: 43/200 | Train loss: 13.3574 | Val loss: 13.2445\n",
      "Epoch: 44/200 | Train loss: 13.2651 | Val loss: 13.1454\n",
      "Epoch: 45/200 | Train loss: 13.1580 | Val loss: 13.0305\n",
      "Epoch: 46/200 | Train loss: 13.0341 | Val loss: 12.8975\n",
      "Epoch: 47/200 | Train loss: 12.8908 | Val loss: 12.7439\n",
      "Epoch: 48/200 | Train loss: 12.7254 | Val loss: 12.5669\n",
      "Epoch: 49/200 | Train loss: 12.5349 | Val loss: 12.3632\n",
      "Epoch: 50/200 | Train loss: 12.3160 | Val loss: 12.1294\n",
      "Epoch: 51/200 | Train loss: 12.0652 | Val loss: 11.8619\n",
      "Epoch: 52/200 | Train loss: 11.7788 | Val loss: 11.5570\n",
      "Epoch: 53/200 | Train loss: 11.4529 | Val loss: 11.2109\n",
      "Epoch: 54/200 | Train loss: 11.0838 | Val loss: 10.8197\n",
      "Epoch: 55/200 | Train loss: 10.6680 | Val loss: 10.3804\n",
      "Epoch: 56/200 | Train loss: 10.2023 | Val loss: 9.8900\n",
      "Epoch: 57/200 | Train loss: 9.6845 | Val loss: 9.3470\n",
      "Epoch: 58/200 | Train loss: 9.1134 | Val loss: 8.7505\n",
      "Epoch: 59/200 | Train loss: 8.4898 | Val loss: 8.1034\n",
      "Epoch: 60/200 | Train loss: 7.8164 | Val loss: 7.4084\n",
      "Epoch: 61/200 | Train loss: 7.0990 | Val loss: 6.6739\n",
      "Epoch: 62/200 | Train loss: 6.3462 | Val loss: 5.9097\n",
      "Epoch: 63/200 | Train loss: 5.5707 | Val loss: 5.1304\n",
      "Epoch: 64/200 | Train loss: 4.7883 | Val loss: 4.3538\n",
      "Epoch: 65/200 | Train loss: 4.0184 | Val loss: 3.6004\n",
      "Epoch: 66/200 | Train loss: 3.2823 | Val loss: 2.8916\n",
      "Epoch: 67/200 | Train loss: 2.6016 | Val loss: 2.2493\n",
      "Epoch: 68/200 | Train loss: 1.9964 | Val loss: 1.6913\n",
      "Epoch: 69/200 | Train loss: 1.4828 | Val loss: 1.2297\n",
      "Epoch: 70/200 | Train loss: 1.0693 | Val loss: 0.8710\n",
      "Epoch: 71/200 | Train loss: 0.7566 | Val loss: 0.6097\n",
      "Epoch: 72/200 | Train loss: 0.5368 | Val loss: 0.4348\n",
      "Epoch: 73/200 | Train loss: 0.3947 | Val loss: 0.3278\n",
      "Epoch: 74/200 | Train loss: 747761.8132 | Val loss: 4.5358\n",
      "Epoch: 75/200 | Train loss: 3.4500 | Val loss: 2.4467\n",
      "Epoch: 76/200 | Train loss: 1.8211 | Val loss: 1.2472\n",
      "Epoch: 77/200 | Train loss: 0.9303 | Val loss: 0.6359\n",
      "Epoch: 78/200 | Train loss: 0.5013 | Val loss: 0.3658\n",
      "Epoch: 79/200 | Train loss: 0.3226 | Val loss: 0.2648\n",
      "Epoch: 80/200 | Train loss: 0.2601 | Val loss: 0.2339\n",
      "Epoch: 81/200 | Train loss: 0.2423 | Val loss: 0.2268\n",
      "Epoch: 82/200 | Train loss: 0.2383 | Val loss: 0.2256\n",
      "Epoch: 83/200 | Train loss: 0.2376 | Val loss: 0.2256\n",
      "Epoch: 84/200 | Train loss: 0.2375 | Val loss: 0.2256\n",
      "Epoch: 85/200 | Train loss: 0.2375 | Val loss: 0.2256\n",
      "Epoch: 86/200 | Train loss: 0.2375 | Val loss: 0.2257\n",
      "Epoch: 87/200 | Train loss: 0.2375 | Val loss: 0.2257\n",
      "Epoch: 88/200 | Train loss: 0.2375 | Val loss: 0.2258\n"
     ]
    }
   ],
   "source": [
    "# Training the model\n",
    "best_model = None\n",
    "best_loss = np.inf\n",
    "best_config = None\n",
    "\n",
    "i = 0  #c'era hyperparams\n",
    "for hidden_size, batch_size, dropout_p, depth, learning_rate in product(hidden_size, batch_size, dropout_p, depth,\n",
    "                                                                        learning_rate):\n",
    "    i += 1\n",
    "\n",
    "    # Tensorboard\n",
    "    dir_name = f'hidden_size={hidden_size}_batch_size={batch_size}_dropout_p={dropout_p}_depth={depth}_learning_rate={learning_rate}'\n",
    "\n",
    "    if os.path.exists(path + dir_name):\n",
    "        print(\"Combination already trained, skipping...\")\n",
    "        continue\n",
    "\n",
    "    writer = SummaryWriter(path + dir_name)\n",
    "\n",
    "    print(f'\\n---Combination {i}---')\n",
    "    print(f'-Hidden size: {hidden_size}')\n",
    "    print(f'-Batch size: {batch_size}')\n",
    "    print(f'-Dropout: {dropout_p}')\n",
    "    print(f'-Depth: {depth}')\n",
    "    print(f'-Learning rate: {learning_rate}\\n')\n",
    "\n",
    "    model = NeuralNetwork(input_size, hidden_size, dropout_p, depth)\n",
    "\n",
    "    # Updating batch size\n",
    "    train_set = torch.utils.data.DataLoader(\n",
    "        torch.utils.data.TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train)), batch_size=batch_size,\n",
    "        shuffle=True)\n",
    "\n",
    "    train_model(model, num_epochs, train_set, val_set, learning_rate, writer)\n",
    "\n",
    "    # Testing the model\n",
    "    test_loss, y_true, y_pred = test_model(model, test_set)\n",
    "    print(f'Test loss: {test_loss}\\n')\n",
    "\n",
    "    config = {\n",
    "        'hidden_size': hidden_size,\n",
    "        'batch_size': batch_size,\n",
    "        'dropout_p': dropout_p,\n",
    "        'depth': depth,\n",
    "        'learning_rate': learning_rate,\n",
    "    }\n",
    "\n",
    "    writer.add_hparams(config, {'hparam/loss': test_loss})\n",
    "    writer.flush()\n",
    "\n",
    "    # Early stopping\n",
    "    if test_loss < best_loss:\n",
    "        best_loss = test_loss\n",
    "        best_model = copy.deepcopy(model)\n",
    "        best_config = config\n",
    "\n",
    "    writer.close()  # 1 ora in meno con pca"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-06-22T14:38:12.013258425Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Testing the model\n",
    "print(\"Best config: \", best_config)\n",
    "test_loss, y_true, y_pred = test_model(best_model, test_set)\n",
    "print(f'Best Test loss: {test_loss}')\n",
    "\n",
    "r2 = r2_score(y_true, y_pred)\n",
    "mse = mean_squared_error(y_true, y_pred)\n",
    "print(f'R2 score: {r2}')\n",
    "print(f'MSE: {mse}')\n",
    "\n",
    "open(results_path + 'neural_network', 'w').write(\n",
    "    f'Best config: {best_config}\\nBest Test loss: {test_loss}\\nR2 score: {r2}\\nMSE: {mse}\\n\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# Plotting the results\n",
    "plt.figure(figsize=(10, 10))\n",
    "sns.regplot(x=y_true, y=y_pred, scatter_kws={'alpha': 0.1})\n",
    "plt.xlabel('True ratings')\n",
    "plt.ylabel('Predicted ratings')\n",
    "plt.title('Predicted vs True ratings')\n",
    "plt.show()\n",
    "\n",
    "# Distribution of predicted ratings\n",
    "plt.figure(figsize=(10, 10))\n",
    "y_pred = np.array(y_pred)\n",
    "sns.kdeplot(y_true)\n",
    "sns.kdeplot(y_pred.flatten())\n",
    "plt.legend(['Predicted ratings', 'True ratings'])\n",
    "plt.xlabel('Predicted ratings')\n",
    "plt.title('Distribution of predicted ratings')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
